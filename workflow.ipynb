{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Foreshock Rate Estimates in Southern California Considering Sensitivity to Mainshock Selection, Earthquake Catalog and Foreshock Definition\n",
    "\n",
    "All the code required for the work for the above paper\n",
    "\n",
    "Uses the statseis module.\n",
    "  \n",
    "[![DOI](https://zenodo.org/badge/807570243.svg)](https://doi.org/10.5281/zenodo.14051948)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in packages and functions, load/pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import scipy\n",
    "import scipy.stats as stats  \n",
    "from scipy.stats import gamma, poisson\n",
    "import scipy.special as special\n",
    "from scipy.stats import ks_2samp\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.patches as patches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.markers import MarkerStyle\n",
    "from matplotlib.markers import JoinStyle\n",
    "from functools import reduce\n",
    "import string\n",
    "from collections import namedtuple\n",
    "import matplotlib.ticker as mticker\n",
    "import shutil\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as mcolors\n",
    "from obspy.imaging.beachball import beach\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'png'  # static\n",
    "pio.templates.default = 'simple_white'  # publication-friendly\n",
    "\n",
    "import sys\n",
    "# sys.path.append(f\"../../statseis/\")\n",
    "import statseis.utils as utils\n",
    "import statseis.statseis as statseis\n",
    "\n",
    "# -- Setup logger\n",
    "logger = logging.getLogger()\n",
    "c_handler = logging.StreamHandler(sys.stdout)  # avoid red colored cells by not logging to stderr\n",
    "logger.setLevel(logging.INFO)\n",
    "c_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "logger.handlers = [c_handler]\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rc('xtick',labelsize=20)\n",
    "plt.rc('ytick',labelsize=20)\n",
    "plt.rc('axes', labelsize=20)\n",
    "plt.rcParams.update({'axes.titlesize': 20})\n",
    "plt.rcParams[\"figure.autolayout\"] = True  # what does this do?\n",
    "\n",
    "date = str(dt.datetime.now().date().strftime(\"%y%m%d\"))\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "colours = sns.color_palette(\"colorblind\", 10)\n",
    "colour_names = ['dark blue', \n",
    "               'orange',\n",
    "               'green',\n",
    "               'red',\n",
    "               'dark pink',\n",
    "               'brown',\n",
    "               'light pink',\n",
    "               'grey',\n",
    "               'yellow',\n",
    "               'light blue']\n",
    "colour_dict = dict(zip(colour_names, colours))\n",
    "colours\n",
    "\n",
    "plot_colors = ['#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e','#e6ab02','#a6761d','#666666']\n",
    "plot_color_dict = dict(zip(['teal', 'orange', 'purple', 'pink', 'green', 'yellow', 'brown', 'grey'], plot_colors))\n",
    "\n",
    "alphabet = string.ascii_lowercase\n",
    "panel_labels = [letter + ')' for letter in alphabet]\n",
    "\n",
    "scale_eq_marker = (lambda x: 10 + np.exp(1.1*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data locally\n",
    "If already downloaded and processed (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_names =['QTM_9_5', 'QTM_12', 'SCSN']\n",
    "region = [-118.80, -115.40, 32.68, 36.20]\n",
    "\n",
    "start_date = dt.datetime(2008, 1, 1)\n",
    "end_date = dt.datetime(2018, 1, 1)\n",
    "catalogue_dict = {}\n",
    "for name in catalogue_names:\n",
    "    catalogue_file = pd.read_csv(f\"../../catalogues/reformatted/{name}_reformat.csv\")\n",
    "    utils.string_to_datetime_df(catalogue_file)\n",
    "    catalogue_file = catalogue_file.loc[(catalogue_file['DATETIME'] >= start_date) &\\\n",
    "                                        (catalogue_file['DATETIME'] < end_date)].copy()\n",
    "    # catalogue_file.to_csv(f\"../../catalogues/reformatted/{name}_reformat.csv\", index=False)\n",
    "\n",
    "    catalogue_dict.update({name:catalogue_file})\n",
    "\n",
    "mainshock_dict = {}\n",
    "for name, data in catalogue_dict.items():\n",
    "    mainshock_file = pd.read_csv(f'../data/{name}/mainshocks.csv')\n",
    "    utils.string_to_datetime_df(mainshock_file)\n",
    "    # mainshock_file.to_csv(f'../data/{name}/mainshocks.csv', index=False)\n",
    "    mainshock_dict[name] = mainshock_file\n",
    "\n",
    "QTM_9_5 = catalogue_dict['QTM_9_5'].copy()\n",
    "QTM_12 = catalogue_dict['QTM_12'].copy()\n",
    "SCSN = catalogue_dict['SCSN'].copy()\n",
    "\n",
    "QTM_9_5_mainshocks = mainshock_dict['QTM_9_5'].copy()\n",
    "QTM_12_mainshocks = mainshock_dict['QTM_12'].copy()\n",
    "SCSN_mainshocks = mainshock_dict['SCSN'].copy()\n",
    "\n",
    "mainshocks_in_all_QTM_9_5 = QTM_9_5_mainshocks.loc[(QTM_9_5_mainshocks['ID'].isin(SCSN_mainshocks['ID'])) &\\\n",
    "                                                        (QTM_9_5_mainshocks['ID'].isin(QTM_12_mainshocks['ID']))].copy()\n",
    "\n",
    "mainshocks_in_all_QTM_12 = QTM_12_mainshocks.loc[(QTM_12_mainshocks['ID'].isin(QTM_9_5_mainshocks['ID'])) &\\\n",
    "                                                        (QTM_12_mainshocks['ID'].isin(SCSN_mainshocks['ID']))].copy()\n",
    "\n",
    "mainshocks_in_all_SCSN = SCSN_mainshocks.loc[(SCSN_mainshocks['ID'].isin(QTM_9_5_mainshocks['ID'])) &\\\n",
    "                                                        (SCSN_mainshocks['ID'].isin(QTM_12_mainshocks['ID']))].copy()\n",
    "\n",
    "mainshocks_in_all_dict = {'QTM_9_5':mainshocks_in_all_QTM_9_5,\n",
    "                          'QTM_12':mainshocks_in_all_QTM_12,\n",
    "                          'SCSN':mainshocks_in_all_SCSN}\n",
    "\n",
    "QTM_9_5_only_mainshocks = QTM_9_5_mainshocks.loc[~(QTM_9_5_mainshocks['ID'].isin(SCSN_mainshocks['ID'])) &\\\n",
    "                                                 ~(QTM_9_5_mainshocks['ID'].isin(QTM_12_mainshocks['ID']))].copy()\n",
    "\n",
    "QTM_12_only_mainshocks = QTM_12_mainshocks.loc[~(QTM_12_mainshocks['ID'].isin(SCSN_mainshocks['ID'])) &\\\n",
    "                                               ~(QTM_12_mainshocks['ID'].isin(QTM_9_5_mainshocks['ID']))].copy  ()\n",
    "\n",
    "SCSN_only_mainshocks = SCSN_mainshocks.loc[~(SCSN_mainshocks['ID'].isin(QTM_9_5_mainshocks['ID'])) &\\\n",
    "                                           ~(SCSN_mainshocks['ID'].isin(QTM_12_mainshocks['ID']))].copy()\n",
    "\n",
    "QTM_only_mainshocks =  QTM_12_mainshocks.loc[(QTM_12_mainshocks['ID'].isin(QTM_9_5_mainshocks['ID'])) &\\\n",
    "                                             ~(QTM_12_mainshocks['ID'].isin(SCSN_mainshocks['ID']))].copy()\n",
    "\n",
    "merged_mainshocks_catalog = pd.merge(pd.merge(mainshocks_in_all_SCSN, mainshocks_in_all_QTM_12, on=['ID', 'Selection'], how='inner', suffixes=('', '_QTM_12')), mainshocks_in_all_QTM_9_5, on=['ID', 'Selection'], how='inner', suffixes=('', '_QTM_9_5'))\n",
    "# merged_mainshocks_catalog = pd.read_csv('../data/mainshocks/merged_mainshock_catalog.csv')\n",
    "# merged_mainshocks_catalog.to_csv('../data/mainshocks/merged_mainshock_catalog.csv', index=False)\n",
    "# utils.string_to_datetime_df(merged_mainshocks_catalog)\n",
    "\n",
    "DDET_mainshocks = merged_mainshocks_catalog.loc[merged_mainshocks_catalog['Selection']=='Both'].copy()\n",
    "\n",
    "station_df = pd.read_csv('../../catalogues/reformatted/stations_SCEDC.csv')\n",
    "for col in ['ONDATE', 'OFFDATE']:\n",
    "    station_df[col] = station_df[col].apply(lambda x: parser.parse(x))\n",
    "station_no_dup = station_df.drop_duplicates(subset='STA')\n",
    "\n",
    "good_mainshocks = pd.read_csv('../outputs/good_mainshocks.csv')\n",
    "utils.string_to_datetime_df(good_mainshocks)\n",
    "\n",
    "# Combined_DDET = pd.read_csv('../outputs/combined_DDET.csv')\n",
    "# utils.string_to_datetime_df(Combined_DDET)\n",
    "# DDET_mainshocks = Combined_DDET.copy()\n",
    "# good_mainshocks = DDET_mainshocks.loc[DDET_mainshocks['ID'].isin(good_mainshocks['ID'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDET_mainshocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and preprocess data (skip if loaded locally)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QTM catalogue\n",
    "https://scedc.caltech.edu/data/qtm-catalog.html\n",
    "\n",
    "2008 - 2017\n",
    "\n",
    "898,597 earthquakes (high detection threshold)\n",
    "\n",
    "1,811,362 earthquakes (low detection threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTM_high_detection_threshold = pd.read_csv('../../catalogues/QTM/qtm_final_12dev.hypo.txt', delim_whitespace=True)\n",
    "# QTM_high_detection_threshold\n",
    "\n",
    "catalogue = QTM_high_detection_threshold.copy()\n",
    "\n",
    "catalogue['DATETIME'] = pd.to_datetime(catalogue[['DAY', 'MONTH', 'YEAR', 'HOUR', 'MINUTE', 'SECOND']],\n",
    "                                       format = '%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "catalogue_reformatted = utils.reformat_catalogue(catalogue[['EVENTID', 'MAGNITUDE', 'DATETIME', 'DEPTH', 'LONGITUDE', \n",
    "                                                      'LATITUDE']])\n",
    "\n",
    "catalogue_reformatted.to_csv('../../catalogues/reformatted/QTM_12_reformat.csv', index = False)\n",
    "\n",
    "catalogue_reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTM_low_detection_threshold = pd.read_csv('../../catalogues/QTM/qtm_final_9.5dev.hypo.txt', delim_whitespace=True)\n",
    "catalogue = QTM_low_detection_threshold.copy()\n",
    "\n",
    "catalogue['DATETIME'] = pd.to_datetime(catalogue[['DAY', 'MONTH', 'YEAR', 'HOUR', 'MINUTE', 'SECOND']],\n",
    "                                       format = '%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "catalogue_reformatted = utils.reformat_catalogue(catalogue[['EVENTID', 'MAGNITUDE', 'DATETIME', 'DEPTH', 'LONGITUDE', \n",
    "                                                      'LATITUDE']])\n",
    "\n",
    "catalogue_reformatted.to_csv('../../catalogues/reformatted/QTM_9_5_reformat.csv', index = False)\n",
    "\n",
    "catalogue_reformatted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCSN catalogue\n",
    "\n",
    "~179,000 earthquakes between 2008 and 2017.\n",
    "\n",
    "http://service.scedc.caltech.edu/eq-catalogs/date_mag_loc.php/\n",
    "\n",
    "<!-- ![title](notebook_images/SCSN_search_params.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCSN = pd.read_csv('../../catalogues/SCSN/SCSN.txt', delim_whitespace=True, skiprows=2, skipfooter=2, engine='python')\n",
    "\n",
    "SCSN['DATETIME'] = pd.to_datetime(SCSN['#YYY/MM/DD'] + ' ' + SCSN['HH:mm:SS.ss'], infer_datetime_format=True, format = '%Y/%m/%d %H:%M:%S.%f')\n",
    "\n",
    "SCSN.drop(['#YYY/MM/DD', 'HH:mm:SS.ss', 'ET', 'GT', 'M', 'Q', 'NPH', 'NGRM'], axis=1, inplace=True)\n",
    "\n",
    "SCSN = SCSN[['EVID', 'MAG', 'DATETIME', 'DEPTH', 'LON', 'LAT']].copy()\n",
    "\n",
    "SCSN.columns = ['ID', 'MAGNITUDE', 'DATETIME', 'DEPTH', 'LON', 'LAT']\n",
    "\n",
    "catalogue = SCSN.copy()\n",
    "\n",
    "catalogue.to_csv('../../catalogues/reformatted/SCSN_reformat.csv', index = False)\n",
    "\n",
    "catalogue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYS_catalog_2011\n",
    "https://scedc.caltech.edu/data/alt-2011-dd-hauksson-yang-shearer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MIN', 'SEC', 'ID', 'LAT', 'LON', 'DEPTH', 'MAGNITUDE',\n",
    "#             'N_PHASES', 'AZIMUTHAL_GAP', 'NEAREST_STATION', 'H_ERROR', 'V_ERROR', 'RMS_RESIDUAL', \n",
    "#             'LOCAL_EVENT', 'MAG_TYPE', 'SOLUTION', 'BOX']\n",
    "columns = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND', 'ID', 'latR', 'lonR', 'depR', 'MAGNITUDE',\n",
    "           'qID', 'cID', 'nbranch', 'qnpair', 'qndiffP', 'qndiffS', 'rmsP', 'rmsS', 'eh', 'ez', 'et', 'latC', 'lonC', 'depC',\n",
    "           'LOCAL_EVENT', 'MAG_TYPE', 'SOLUTION', 'BOX']\n",
    "# HYS = pd.read_csv(f\"../catalogues/HYS/sc_1981_2022q1_1d_3d_gc_soda.gc.txt\", delim_whitespace=True, header=None, on_bad_lines='skip')\n",
    "HYS = pd.read_csv(f\"../../catalogues/HYS/sc_1981_2019_1d_3d_gc_soda_noqb_v0.gc.txt\", delim_whitespace=True, header=None, on_bad_lines='skip')\n",
    "HYS.columns = columns\n",
    "catalogue = HYS.copy()\n",
    "catalogue['DATETIME'] = pd.to_datetime(catalogue[['DAY', 'MONTH', 'YEAR', 'HOUR', 'MINUTE', 'SECOND']],\n",
    "                                       format = '%d-%m-%Y %H:%M:%S')\n",
    "catalogue.drop(['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND', 'qID', 'cID', 'nbranch', 'qnpair', 'qndiffP', 'qndiffS', 'rmsP', 'rmsS', 'eh', 'ez', 'et', 'latC', 'lonC', 'depC',\n",
    "           'LOCAL_EVENT', 'MAG_TYPE', 'SOLUTION', 'BOX'], axis=1, inplace=True)\n",
    "catalogue = catalogue[['ID', 'MAGNITUDE', 'DATETIME', 'lonR', 'latR', 'depR']]\n",
    "catalogue.columns = ['ID', 'MAGNITUDE', 'DATETIME', 'LON', 'LAT', 'DEPTH']\n",
    "catalogue.to_csv(f\"../../catalogues/reformatted/HYS_reformat.csv\", index=False)\n",
    "catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal mechanism catalog for Southern California\n",
    "Cheng et al., 2023  \n",
    "https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2022JB025975  \n",
    "https://data.mendeley.com/datasets/9s54cy253d/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv('../../catalogues/CNN_SoCal/focmecall_SC_11_09_2022', delim_whitespace=True, header=None)\n",
    "columns = ['year',\n",
    "            'month',\n",
    "            'day',\n",
    "            'hour',\n",
    "            'minute',\n",
    "            'second',\n",
    "            'ID',\n",
    "            'LAT',\n",
    "            'LON',\n",
    "            'DEPTH',\n",
    "            'MAGNITUDE',\n",
    "            'strike',\n",
    "            'dip',\n",
    "            'rake',\n",
    "            '1st nodal plane uncertainty',\n",
    "            '2nd nodal plane uncertainty',\n",
    "            'number of polarities',\n",
    "            'polarity misfit',\n",
    "            'number of S/P amplitude ratio',\n",
    "            'average log10(S/P) amplitude ratio misfit',\n",
    "            'focal mechanism quality',\n",
    "            'probability of solution close to real solution',\n",
    "            'azimuth gap',\n",
    "            'take-off angle gap',\n",
    "            'polarity misfit',\n",
    "            'station distribution ratio (STDR)']\n",
    "catalog.columns = columns\n",
    "catalog['DATETIME'] = pd.to_datetime(catalog[['day', 'month', 'year', 'hour', 'minute', 'second']],\n",
    "                                       format = '%d-%m-%Y %H:%M:%S')\n",
    "catalog = catalog[['ID', 'MAGNITUDE', 'DATETIME', 'LAT', 'LON', 'DEPTH', 'strike', 'dip', 'rake']]\n",
    "catalog.to_csv('../../catalogues/reformatted/CNN_SoCal_reformat.csv', index=False)\n",
    "catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting focal mechanisms from CNN_SoCal catalog\n",
    "How to separate by mechanism? GCMT says \"Use tension and null axis plunge to search by mechanism. For example, thrust faults have large plunge (>45) of tension axis, strike-slip faults have large plunge of null axis, and normal faults have small (<45) for both tension and null axes.\"  \n",
    "\n",
    "Is this azimuth gap and take-off angle gap?  \n",
    "\n",
    "Ignore for now and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_events = catalog.loc[catalog['MAGNITUDE']>=7].copy()\n",
    "largest_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = ccrs.PlateCarree()\n",
    "\n",
    "fig = plt.figure(dpi=300)\n",
    "ax = fig.add_subplot(111, projection=projection)\n",
    "extent = [-121.953, -114.026, 31.86317, 37.22333]\n",
    "ax.set_extent(extent)\n",
    "\n",
    "ax.coastlines()\n",
    "ax.gridlines()\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "largest_events['NAME'] = ['Landers', 'Hector-Mine', 'El-Mayor Cucupah', 'Ridgecrest']\n",
    "for event in largest_events.itertuples():\n",
    "    x, y = projection.transform_point(x=event.LON, y=event.LAT,\n",
    "                                    src_crs=ccrs.Geodetic())\n",
    "    # focmecs = [0.136, -0.591, 0.455, -0.396, 0.046, -0.615]\n",
    "    focmecs = [event.strike, event.dip, event.rake]\n",
    "\n",
    "    b = beach(focmecs, xy=(x, y), width=0.4, linewidth=1, alpha=0.85)\n",
    "    b.set_zorder(10)\n",
    "    ax.add_collection(b)\n",
    "    ax.text(x-0.1, y+0.2, event.NAME, fontsize=8, ha='right', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station data\n",
    "https://service.scedc.caltech.edu/station/weblist.php  \n",
    "Had to read into excel and then resave as a csv  \n",
    "\n",
    "- I need stations that are\n",
    "\t- turned on before catalog start date\n",
    "\t- turned off after catalog start date\n",
    "\t- 3 component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = pd.read_csv('../../catalogues/SCEC/stations_imported.csv')\n",
    "for col in ['ONDATE', 'OFFDATE']:\n",
    "    station_df[col] = station_df[col].apply(lambda x: dt.datetime.strptime(x, '%d/%m/%Y'))\n",
    "station_df = station_df.loc[(station_df['ONDATE'] < dt.datetime(2008,1,1)) &\\\n",
    "                            (station_df['OFFDATE'] >= dt.datetime(2019,1,1))].copy()\n",
    "station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "not_three_component = []\n",
    "for STA in station_df['STA']:\n",
    "    x = station_df.loc[station_df['STA']==STA]\n",
    "    counts.append(len(x))\n",
    "    if len(x)<3:\n",
    "        not_three_component.append(STA)\n",
    "plt.hist(counts, bins=utils.get_bins(counts, nearest=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.loc[station_df['STA'].isin(not_three_component)].sort_values(by='STA').head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = station_df.loc[~station_df['STA'].isin(not_three_component)].copy()\n",
    "station_df.reset_index(inplace=True, drop=True)\n",
    "station_df.to_csv('../../catalogues/reformatted/stations_SCEDC.csv', index=False)\n",
    "station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Visualising data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table: earthquake catalog data\n",
    "Note: There is a M$\\ge$ 4 earthquake in the SCSN, that is M<4 in the QTM (or the other way round, confirm later).  \n",
    "But if you come back to this code and you don't know why some numbers don't add up, that is one reason why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = [-118.80, -115.40, 32.68, 36.20]\n",
    "start_date = dt.datetime(2008, 1, 1)\n",
    "end_date = dt.datetime(2018, 1, 1)\n",
    "\n",
    "catalogue_dict = {}\n",
    "for name in catalogue_names:\n",
    "\n",
    "    catalogue_file = pd.read_csv(f\"../../catalogues/reformatted/{name}_reformat.csv\")\n",
    "    utils.string_to_datetime_df(catalogue_file)\n",
    "    catalogue_file = catalogue_file.loc[(catalogue_file['DATETIME'] >= start_date) &\\\n",
    "                                        (catalogue_file['DATETIME'] < end_date)].copy()\n",
    "\n",
    "    \n",
    "    catalogue_dict.update({name:catalogue_file})\n",
    "\n",
    "    catalogue_restrict = utils.restrict_catalogue_geographically(catalogue_file, region=region)\n",
    "    catalogue_dict.update({name+'_restrict':catalogue_restrict})\n",
    "\n",
    "combined_cat = pd.merge(pd.merge(SCSN, QTM_12, how='inner', on='ID', suffixes=('', '_QTM_12')), QTM_9_5, how='inner', on='ID', suffixes=('', '_QTM_9_5'))\n",
    "combined_SCSN_restricted = utils.restrict_catalogue_geographically(combined_cat, region=region)\n",
    "combined_QTM_restricted = utils.restrict_catalogue_geographically(QTM_12.loc[QTM_12['ID'].isin(combined_cat['ID'])].copy(), region=region)\n",
    "catalogue_dict.update({'Combined':combined_cat})\n",
    "catalogue_dict.update({'combined_SCSN_restricted':combined_SCSN_restricted})\n",
    "catalogue_dict.update({'combined_QTM_restricted':combined_QTM_restricted})\n",
    "    \n",
    "catalogue_table = []\n",
    "for name, data in catalogue_dict.items():\n",
    "    print(name)\n",
    "    data['DATETIME'] = utils.string_to_datetime(data['DATETIME'])\n",
    "    Mc_maxc = round(statseis.Mc_by_maximum_curvature(data['MAGNITUDE'], correction=0.2), 2)\n",
    "    Mc_mbs = round(statseis.statseis.get_mbs(np.array(data['MAGNITUDE']), mbin=0.1)[0], 2)\n",
    "    b_value_maxc = round(statseis.b_val_max_likelihood(np.array(data['MAGNITUDE']), mc=Mc_maxc),2)\n",
    "    b_value_mbs = round(statseis.b_val_max_likelihood(np.array(data['MAGNITUDE']), mc=Mc_mbs),2)\n",
    "    N = len(data)\n",
    "    N_candidate_mainshocks = len(data.loc[(data['MAGNITUDE']>=4) & (data['DATETIME']>=dt.datetime(2009,1,1))])\n",
    "    start_date = min(data['DATETIME'])\n",
    "    end_date = max(data['DATETIME'])\n",
    "    catalogue_stats_dict = {'Name':name,\n",
    "                            'Total_Earthquakes':N,\n",
    "                            'N_Mw_4+':N_candidate_mainshocks,\n",
    "                            'Mc_maxc':Mc_maxc,\n",
    "                            'Mc_mbs':Mc_mbs,\n",
    "                            'b_value_maxc':b_value_maxc,\n",
    "                            'b_value_mbs':b_value_mbs,\n",
    "                            'start_date':start_date,\n",
    "                            'end_date':end_date\n",
    "                           }\n",
    "    catalogue_table.append(catalogue_stats_dict)\n",
    "catalogue_table = pd.DataFrame.from_dict(catalogue_table)\n",
    "catalogue_table.sort_values(by='N_Mw_4+', inplace=True, ascending=False)\n",
    "# catalogue_table.to_csv('../outputs/tables/catalogue_table.csv', index=False)\n",
    "catalogue_dict = {key: catalogue_dict[key] for key in catalogue_names}\n",
    "catalogue_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure: Catalog time series, map, and FMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_dict = {key: catalogue_dict[key] for key in catalogue_names}\n",
    "\n",
    "size = 1\n",
    "scale=4\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title(f\"a)\", loc='left', fontsize=20)\n",
    "\n",
    "ax1.set_xlabel(f\"Year\")\n",
    "ax1.set_ylabel(f\"Magnitude\")\n",
    "\n",
    "ax2 = fig.add_subplot(223, projection=ccrs.PlateCarree())\n",
    "ax2.set_title(f\"b)\", loc='left', fontsize=20)\n",
    "\n",
    "ax2.set_extent([-121.953, -114.026, 31.86317, 37.22333], crs=ccrs.PlateCarree())\n",
    "ax2.add_feature(cfeature.COASTLINE)\n",
    "ax2.add_feature(cfeature.LAND, edgecolor='black')\n",
    "ax2.add_feature(cfeature.OCEAN, edgecolor='none')\n",
    "gl = ax2.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, zorder=0)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xlabel_style = {'size': 15}\n",
    "gl.ylabel_style = {'size': 15}\n",
    "gl.xlines = False\n",
    "gl.ylines = False\n",
    "gl.ylocator = mticker.FixedLocator(range(32,38,2))\n",
    "gl.xlocator = mticker.FixedLocator(np.arange(-120,-114,4))\n",
    "\n",
    "rectangle_coords = [-118.8, -115.4, 32.68, 36.2]\n",
    "rectangle = Rectangle(\n",
    "    (rectangle_coords[0], rectangle_coords[2]),\n",
    "    rectangle_coords[1] - rectangle_coords[0],\n",
    "    rectangle_coords[3] - rectangle_coords[2],\n",
    "    linewidth=1, edgecolor='red', facecolor='none', transform=ccrs.PlateCarree()\n",
    ")\n",
    "ax2.add_patch(rectangle)\n",
    "\n",
    "ax3 = fig.add_subplot(224)\n",
    "ax3.set_title(f\"c)\", loc='left', fontsize=20)\n",
    "ax3.set_xlabel('Magnitude')\n",
    "ax3.set_ylabel('Frequency')\n",
    "\n",
    "i = 0\n",
    "Mc_list = [0.3, 0.3, 1.7]\n",
    "ref_list = ['Ross et al., 2019', 'Ross et al., 2019', 'Hutton et al., 2010']\n",
    "for name, catalogue in catalogue_dict.items():\n",
    "    z = np.exp(catalogue['MAGNITUDE'])/5\n",
    "    ax1.scatter(catalogue['DATETIME'], catalogue['MAGNITUDE'], alpha=0.1, color=colours[i], s=z)\n",
    "    ax2.scatter(catalogue['LON'], catalogue['LAT'], alpha=0.1, transform=ccrs.PlateCarree(), s=z,  color=colours[i])\n",
    "\n",
    "    a = np.log10(len(catalogue))\n",
    "    bins = np.array(range(math.floor(catalogue['MAGNITUDE'].min())*10, math.ceil(catalogue['MAGNITUDE'].max())*10,1))/10\n",
    "    values, base = np.histogram(catalogue['MAGNITUDE'], bins=bins)\n",
    "    cumulative = np.cumsum(values)\n",
    "    mark = utils.find_nearest(base, Mc_list[i])\n",
    "    mark_index = np.where(base == mark)\n",
    "\n",
    "    ax3.axvline(Mc_list[i], \n",
    "                color=colours[i], \n",
    "                alpha=0.9, linestyle='--',zorder=0)\n",
    "    \n",
    "    ax3.plot(base[:-1], len(catalogue)-cumulative,\n",
    "             color=colours[i], \n",
    "             alpha=0.6\n",
    "             )\n",
    "\n",
    "    ax3.set_yscale(\"log\")\n",
    "    ax3.scatter(Mc_list[i], len(catalogue)-cumulative[mark_index], \n",
    "                color=colours[i],\n",
    "                marker=\"D\", zorder=5, \n",
    "             label=f\"{name}, Mc: {Mc_list[i]} \\n ({ref_list[i]})\")\n",
    "    i+=1\n",
    "\n",
    "ax3.legend(loc='upper right', fontsize=10)\n",
    "ax3.set_xticks(np.arange(-2,9,2))\n",
    "\n",
    "ax1.axhline(4, color='grey', linestyle='--', alpha=0.8)\n",
    "rectangle_coords = [dt.datetime(2008,1,1), dt.datetime(2009,1,1), -3, 8]\n",
    "rectangle = Rectangle(\n",
    "    (rectangle_coords[0], rectangle_coords[2]),\n",
    "    rectangle_coords[1] - rectangle_coords[0],\n",
    "    rectangle_coords[3] - rectangle_coords[2],\n",
    "    linewidth=1, ec='grey', fc='grey', alpha=0.2, zorder=0\n",
    "    )\n",
    "ax1.add_patch(rectangle)\n",
    "\n",
    "# plt.savefig('../outputs/figures/catalogues_multiplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catalog level Mc estimation comparison - Maxc, MBS, Lilliefors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_Mlils = [3.22, 3.24, 3.22]\n",
    "fig = plt.figure()\n",
    "ax3 = fig.add_subplot(111)\n",
    "i=0\n",
    "for key, catalogue in catalogue_dict.items():\n",
    "    Mbass = statseis.get_mbs(catalogue['MAGNITUDE'], mbin=0.1)[0]\n",
    "    Maxc =  statseis.get_maxc(catalogue['MAGNITUDE'], mbin=0.1)\n",
    "    Mlil = catalog_Mlils[i]\n",
    "    # MLil = lill.estimate_Mc_expon_test()\n",
    "\n",
    "    Mc_dict = {'Mbass':Mbass, \n",
    "               'Maxc': Maxc,\n",
    "               'Mlil':Mlil\n",
    "               }\n",
    "    print(key, Mbass, Maxc)\n",
    "\n",
    "    a = np.log10(len(catalogue))\n",
    "    bins = np.array(range(math.floor(catalogue['MAGNITUDE'].min())*10, math.ceil(catalogue['MAGNITUDE'].max())*10,1))/10\n",
    "    values, base = np.histogram(catalogue['MAGNITUDE'], bins=bins)\n",
    "    cumulative = np.cumsum(values)\n",
    "    # mark = utils.find_nearest(base, Mbass['Mc'])\n",
    "    # mark_index = np.where(base == mark)\n",
    "\n",
    "    ax3.plot(base[:-1], len(catalogue)-cumulative, color=plot_colors[i], linewidth=1.5, alpha=0.6)\n",
    "\n",
    "    for method, Mc in Mc_dict.items():\n",
    "        ax3.axvline(Mc, linewidth=1.5, color=plot_colors[i], alpha=0.9, linestyle='--',zorder=0, label=f'{key} - {method} = {Mc}')\n",
    "\n",
    "        # ax3.scatter(Mbass, len(catalogue)-cumulative[mark_index], color=plot_colors[i], label=fr\"Mbass: {Mbass}\",\n",
    "        #             marker=\"D\", zorder=5, s=size_dict['legend_markersize']*scale*scale)\n",
    "    i+=1\n",
    "ax3.set_yscale(\"log\")\n",
    "ax3.legend()\n",
    "ax3.set_xlabel('Magnitude')\n",
    "ax3.set_ylabel('N')\n",
    "\n",
    "# plt.savefig('../outputs/rough/catalog_Mc_comp.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Select and analyse mainshocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select mainshocks and create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_dict = {key: catalogue_dict[key] for key in catalogue_names}\n",
    "\n",
    "mainshock_table = []\n",
    "mainshock_dict = {}\n",
    "for name, data in catalogue_dict.items():\n",
    "    mainshock_file = statseis.select_mainshocks(data, station_file=station_no_dup)\n",
    "    mainshock_file = mainshock_file.loc[mainshock_file['DATETIME']>=dt.datetime(2009,1,1)].copy()\n",
    "    statseis.get_Mcs_400(mainshock_file, earthquake_catalogue=data, catalogue_name=name, min_n=275, max_r=50)\n",
    "    Path(f'../data/{name}/').mkdir(parents=True, exist_ok=True)\n",
    "    mainshock_file.to_csv(f'../data/{name}/mainshocks.csv', index=False)\n",
    "    both_selected = len(mainshock_file.loc[mainshock_file['Selection']=='Both'])\n",
    "    MDET_selected = len(mainshock_file.loc[mainshock_file['Selection']=='MDET']) + both_selected\n",
    "    FET_selected = len(mainshock_file.loc[mainshock_file['Selection']=='FET']) + both_selected\n",
    "\n",
    "    N_Mw_4 =  len(mainshock_file)\n",
    "    median_Mc = np.median(mainshock_file['Mc'])\n",
    "    mean_Mc = np.mean(mainshock_file['Mc'])\n",
    "    mainshock_table.append({'Catalogue':name,\n",
    "                            'N_Mw_4_plus':N_Mw_4,\n",
    "                            'MDET_selected':MDET_selected,\n",
    "                            'FET_selected':FET_selected,\n",
    "                            'Both_selected':both_selected,\n",
    "                            'MDET_perc':round(100*MDET_selected/N_Mw_4),\n",
    "                            'FET_perc':round(100*FET_selected/N_Mw_4),\n",
    "                            'Both_perc':round(100*both_selected/N_Mw_4),\n",
    "                            'median_Mc':median_Mc,\n",
    "                            'mean_Mc':mean_Mc\n",
    "                            })\n",
    "    mainshock_dict[name] = mainshock_file\n",
    "\n",
    "SCSN_mainshocks = mainshock_dict['SCSN']\n",
    "QTM_12_mainshocks = mainshock_dict['QTM_12']\n",
    "QTM_9_5_mainshocks = mainshock_dict['QTM_9_5']\n",
    "\n",
    "mainshock_dict['Combined'] = pd.merge(pd.merge(SCSN_mainshocks, QTM_12_mainshocks, on='ID', how='inner', suffixes=('', '_QTM_12')), QTM_9_5_mainshocks, how='inner', on='ID', suffixes=('_SCSN', '_QTM_9_5'))\n",
    "Combined_mainshocks = mainshock_dict['Combined'].copy()\n",
    "\n",
    "mainshock_table = pd.DataFrame.from_dict(mainshock_table)\n",
    "mainshock_table.to_csv('../outputs/tables/mainshock_table.csv', index=False)\n",
    "mainshock_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table: Combined catalog selections\n",
    "Messy, streamline later  \n",
    "Don't panic, there are 85 DDET mainshocks in one row as although there are 83 with the same selection across the SCSN and QTM catalogs, there are 85 in each (two separate ones change in each catalog).  \n",
    "Mainshock ID: 14998780 is M3.96 in the SCSN, and M4.42 in the QTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cat = pd.merge(pd.merge(SCSN, QTM_12, how='inner', on='ID', suffixes=('', '_QTM_12')), QTM_9_5, how='inner', on='ID', suffixes=('', '_QTM_9_5'))\n",
    "combined_cat_restricted = utils.restrict_catalogue_geographically(combined_cat, region=region)\n",
    "# catalogue_dict.update({'Combined':combined_cat})\n",
    "# catalogue_dict.update({'Combined_restricted':combined_cat_restricted})\n",
    "\n",
    "combined_mainshock_dict = {}\n",
    "combined_mainshock_table = []\n",
    "for cat in catalogue_dict.keys():\n",
    "    mfile = mainshock_dict[cat].copy()\n",
    "    mainshock_file =  mfile.loc[mfile['ID'].isin(combined_cat['ID'])].copy()\n",
    "    both_selected = len(mainshock_file.loc[mainshock_file[f'Selection']=='Both'])\n",
    "    MDET_selected = len(mainshock_file.loc[mainshock_file[f'Selection']=='MDET']) + both_selected\n",
    "    FET_selected = len(mainshock_file.loc[mainshock_file[f'Selection']=='FET']) + both_selected\n",
    "    N_Mw_4 =  len(mainshock_file)\n",
    "    median_Mc = np.median(mainshock_file[f'Mc'])\n",
    "    mean_Mc = np.mean(mainshock_file[f'Mc'])\n",
    "    combined_mainshock_table.append({'Catalogue':f'{cat}_combined',\n",
    "                            'N_Mw_4_plus':N_Mw_4,\n",
    "                            'MDET_selected':MDET_selected,\n",
    "                            'FET_selected':FET_selected,\n",
    "                            'Both_selected':both_selected,\n",
    "                            'MDET_perc':round(100*MDET_selected/N_Mw_4),\n",
    "                            'FET_perc':round(100*FET_selected/N_Mw_4),\n",
    "                            'Both_perc':round(100*both_selected/N_Mw_4),\n",
    "                            'median_Mc':median_Mc,\n",
    "                            'mean_Mc':mean_Mc\n",
    "                            })\n",
    "    combined_mainshock_dict[cat] = mainshock_file\n",
    "combined_mainshock_table = pd.DataFrame.from_dict(combined_mainshock_table)\n",
    "combined_mainshock_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mshocks_merged_on_ID_and_state = pd.merge(combined_mainshock_dict['SCSN'], combined_mainshock_dict['QTM_12'], how='inner', on=['ID', 'Selection'], suffixes=('', '_QTM_12'))\n",
    "QTM_12_merged_mshocks = QTM_12_mainshocks.loc[QTM_12_mainshocks['ID'].isin(mshocks_merged_on_ID_and_state['ID'])]\n",
    "SCSN_merged_mshocks = SCSN_mainshocks.loc[SCSN_mainshocks['ID'].isin(mshocks_merged_on_ID_and_state['ID'])]\n",
    "SCSN_merged_mshocks_restrict = utils.restrict_catalogue_geographically(SCSN_merged_mshocks, region=region)\n",
    "QTM_12_merged_mshocks_restrict = utils.restrict_catalogue_geographically(QTM_12_merged_mshocks, region=region)\n",
    "print(len(SCSN_merged_mshocks_restrict), len(QTM_12_merged_mshocks_restrict))\n",
    "QTM_12_merged_mshocks_restrict.loc[~QTM_12_merged_mshocks_restrict['ID'].isin(SCSN_merged_mshocks_restrict['ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCSN_merged_mshocks_restrict.loc[~SCSN_merged_mshocks_restrict['ID'].isin(QTM_12_merged_mshocks_restrict['ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statseis.mainshock_selections_counts(mshocks_merged_on_ID_and_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statseis.mainshock_selections_counts(QTM_12_merged_mshocks_restrict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statseis.mainshock_selections_counts(SCSN_merged_mshocks_restrict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = combined_mainshock_dict['SCSN'].copy()\n",
    "y = combined_mainshock_dict['QTM_12'].copy()\n",
    "\n",
    "x2 = x.loc[~x['ID'].isin(mshocks_merged_on_ID_and_state['ID'])].copy()\n",
    "y2 = y.loc[~y['ID'].isin(mshocks_merged_on_ID_and_state['ID'])].copy()\n",
    "\n",
    "differing_selections = pd.merge(x2, y2, on='ID', how='inner', suffixes=('_SCSN', '_QTM_12'))\n",
    "len(differing_selections)\n",
    "differing_selections[['Selection_SCSN', 'Selection_QTM_12']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = [-118.80, -115.40, 32.68, 36.20]\n",
    "for key, item in mainshock_dict.items():\n",
    "    if key!='Combined':\n",
    "        print(key)\n",
    "        r = utils.restrict_catalogue_geographically(item, region=region)\n",
    "        print(len(r))\n",
    "        for selection in ['Both', 'FET', 'MDET']:\n",
    "            s = r.loc[r['Selection']==selection].copy()\n",
    "            print(selection, len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Mc and b-values around mainshocks\n",
    "\n",
    "We estimate $M_{c}$ using the maximum curvature method (MAXC), the b-value stability method (MBS).  \n",
    "We choose a 10 km radius around mainshock epicenters, and set a threshold of 275 events.  \n",
    "We expand the radius by 5 km, up to 50 km, until the threshold is reached.\n",
    "\n",
    "We choose 275 events as our threshold as this is around the value where the b-values using our $M_{c}$ estimates drop off.  \n",
    "We believe this to be a reasonable threshold based on the performance of the $M_{c}$ by b-value stability method (MBS):  \n",
    "(http://www.corssa.org/export/sites/corssa/.galleries/articles-pdf/Mignan-Woessner-2012-CORSSA-Magnitude-of-completeness.pdf).  \n",
    "\n",
    "Some mainshocks occur in regions with too few events to make a robust estimate of $M_{c}$.  \n",
    "  \n",
    "Some mainshocks have <10 events within 10 km in the past year. We look at these mainshocks too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Mc in SCSN to QTM\n",
    "Mc by b-value stability (MBS) method.  \n",
    "Do the catalogs have the same Mc around mainshocks, or is there a significant difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainshock_file = good_mainshocks.copy() #DDET_mainshocks\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "i,j = 2,2\n",
    "\n",
    "ax = fig.add_subplot(i,j,1)\n",
    "ax.hist(mainshock_file['Mc'], alpha=0.5, label='SCSN', bins=np.arange(-0.2, 2.5, 0.2))\n",
    "ax.hist(mainshock_file['Mc_QTM_12'], alpha=0.5,  label='QTM 12', bins=np.arange(-0.2, 2.5, 0.2))\n",
    "ax.set_xlabel(f'$M_c$')\n",
    "ax.set_ylabel(f'N')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(i,j,2)\n",
    "ax.scatter(mainshock_file['Mc'], mainshock_file['Mc_QTM_12'], ec='white', linewidth=0.5)\n",
    "ax.set_xlabel(f'SCSN $M_c$')\n",
    "ax.set_ylabel(f'QTM 12 $M_c$')\n",
    "ax.plot(range(0,3), range(0,3), label='1:1')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(i,j,3)\n",
    "ax.hist(mainshock_file['n_local_cat_1yr'], alpha=0.5, label='SCSN', bins=np.arange(0, 3500, 100))\n",
    "ax.hist(mainshock_file['n_local_cat_1yr_QTM_12'], alpha=0.5,  label='QTM 12', bins=np.arange(0, 3500, 100))\n",
    "ax.set_xlabel('local catalog n')\n",
    "ax.set_ylabel(f'N')\n",
    "ax.legend()\n",
    "ax.set_xlim(0,2500)\n",
    "\n",
    "ax = fig.add_subplot(i,j,4)\n",
    "ax.scatter(mainshock_file['n_local_cat_1yr'], mainshock_file['n_local_cat_1yr_QTM_12'], ec='white', linewidth=0.5)\n",
    "ax.set_xlabel(f'SCSN N')\n",
    "ax.set_ylabel(f'QTM 12 N')\n",
    "ax.plot(range(0,1000), range(0,1000), label='1:1')\n",
    "ax.legend()\n",
    "ax.set_ylim(0,2500)\n",
    "\n",
    "# plt.savefig('../outputs/rough/Mc_catalog_comp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mc in SCSN vs QTM, and MBS vs MaxC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 37301704#37265488\n",
    "name = 'SCSN'\n",
    "catalog = catalogue_dict[name].copy()\n",
    "mainshock = statseis.iterable_mainshock(ID, SCSN_mainshocks)\n",
    "local_cat_SCSN = statseis.create_local_catalogue(mainshock=mainshock, earthquake_catalogue=catalog, catalogue_name=name, radius_km=50)\n",
    "\n",
    "name = 'QTM_12'\n",
    "catalog = catalogue_dict[name].copy()\n",
    "mainshock = statseis.iterable_mainshock(ID, QTM_12_mainshocks)\n",
    "local_cat_QTM_12 = statseis.create_local_catalogue(mainshock=mainshock, earthquake_catalogue=catalog, catalogue_name=name, radius_km=50)\n",
    "\n",
    "len(local_cat_SCSN), len(local_cat_QTM_12)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for cat in [local_cat_SCSN, local_cat_QTM_12]:\n",
    "    magnitudes = cat['MAGNITUDE']\n",
    "    magnitudes = np.array(magnitudes)\n",
    "    bins = np.arange(math.floor(magnitudes.min()), math.ceil(magnitudes.max()), 0.1)\n",
    "    values, base = np.histogram(magnitudes, bins=bins)\n",
    "    cumulative = np.cumsum(values)\n",
    "    ax.step(base[:-1], len(magnitudes)-cumulative, color='black')\n",
    "    Mc, this_fmd, b, b_avg, shibolt_unc = statseis.get_mbs(mag=magnitudes, mbin=0.1)\n",
    "    a, b_value, _a, _b = statseis.b_est(mag=np.array(cat['MAGNITUDE']), mbin=0.1, mc=Mc)\n",
    "    ax.axvline(x=Mc, linestyle='--', label=r'$M_{c}$: ' + str(round(Mc,1)), color=plot_colors[0])\n",
    "\n",
    "    N = [10**(a-b_value*M) for M in this_fmd]\n",
    "    ax.plot(this_fmd, N, label=f'b: {round(b_value,2)}',  color=plot_colors[1])\n",
    "\n",
    "    n_above_Mc = len(magnitudes[magnitudes>Mc])\n",
    "    ratio_above_Mc = round(100*n_above_Mc/len(magnitudes))\n",
    "    ax.plot([], [], label=f'{len(magnitudes)} events', marker=None, linestyle='')\n",
    "    ax.plot([], [], label=f'{n_above_Mc} above $M_c$ ({ratio_above_Mc}%)', marker=None, linestyle='')\n",
    "\n",
    "ax.set_xlabel('Magnitude')\n",
    "ax.set_ylabel('N')\n",
    "# axs[Mc_plot].legend()\n",
    "ax.set_yscale('log')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(DDET_mainshocks['Mc'] - DDET_mainshocks['Mc_QTM_12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "b = DDET_mainshocks['Mc'] - DDET_mainshocks['Mc_QTM_12']\n",
    "sorted_data = np.sort(b)\n",
    "# pdf = b/sum(b)\n",
    "# cdf = np.cumsum(pdf)\n",
    "cdf = np.arange(1, len(sorted_data) + 1)/len(b)\n",
    "\n",
    "ax=fig.add_subplot(111)\n",
    "ax.step(sorted_data, cdf)\n",
    "percentiles = np.percentile(sorted_data, [25, 50, 75])\n",
    "cdf_percentiles = np.array([0.25, 0.5, 0.75])\n",
    "# for percentile, cdf_percentile in zip(percentiles, cdf_percentiles):\n",
    "\n",
    "#     plt.vlines(percentile, ymin=0, ymax=cdf_percentile, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "#     # Annotate the percentiles\n",
    "#     plt.text(percentile, cdf_percentile, f'{percentile:.2f}', ha='center', va='top', color='black', fontsize=20)\n",
    "plt.yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "plt.xlabel(f'$\\delta$ $M_c$ (SCSN $M_c$ - QTM 12 $M_c$)')\n",
    "plt.ylabel('CDF')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainshock_dict = {}\n",
    "for name, data in catalogue_dict.items():\n",
    "    mainshock_file = statseis.select_mainshocks(data)\n",
    "    mainshock_file = mainshock_file.loc[mainshock_file['DATETIME']>=dt.datetime(2009,1,1)].copy()\n",
    "    statseis.get_Mcs_400(mainshock_file, earthquake_catalogue=data, catalogue_name=name, min_n=275, max_r=50)\n",
    "    both_selected = len(mainshock_file.loc[mainshock_file['Selection']=='Both'])\n",
    "    MDET_selected = len(mainshock_file.loc[mainshock_file['Selection']=='MDET']) + both_selected\n",
    "    FET_selected = len(mainshock_file.loc[mainshock_file['Selection']=='FET']) + both_selected\n",
    "    \n",
    "    mainshock_dict[name] = mainshock_file\n",
    "\n",
    "SCSN_mainshocks = mainshock_dict['SCSN']\n",
    "QTM_12_mainshocks = mainshock_dict['QTM_12']\n",
    "QTM_9_5_mainshocks = mainshock_dict['QTM_9_5']\n",
    "\n",
    "mainshock_dict['Combined'] = pd.merge(pd.merge(SCSN_mainshocks, QTM_12_mainshocks, on='ID', how='inner', suffixes=('', '_QTM_12')), QTM_9_5_mainshocks, how='inner', on='ID', suffixes=('', '_QTM_9_5'))\n",
    "Combined_mainshocks = mainshock_dict['Combined'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_DDET_mainshocks = Combined_mainshocks.loc[Combined_mainshocks['ID'].isin(good_mainshocks['ID'])].copy()\n",
    "Combined_DDET_mainshocks.reset_index(inplace=True, drop=True)\n",
    "Combined_DDET_mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_DDET_mainshocks.loc[Combined_DDET_mainshocks['Mc']<Combined_DDET_mainshocks['Mc_QTM_12']][['Mc', 'Mc_QTM_12', 'Mc_QTM_9_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_DDET_mainshocks.loc[Combined_DDET_mainshocks['Mc']<Combined_DDET_mainshocks['Mc_QTM_9_5']][['Mc', 'Mc_QTM_12', 'Mc_QTM_9_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_DDET_mainshocks.loc[Combined_DDET_mainshocks['Maxc_50']<Combined_DDET_mainshocks['Maxc_50_QTM_12']][['ID', 'Maxc_50', 'Maxc_50_QTM_12', 'Maxc_50_QTM_9_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_DDET_mainshocks.loc[Combined_DDET_mainshocks['Maxc_50']<Combined_DDET_mainshocks['Maxc_50_QTM_9_5']][['ID', 'Maxc_50', 'Maxc_50_QTM_12', 'Maxc_50_QTM_9_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "b = Combined_DDET_mainshocks['Mc'] - Combined_DDET_mainshocks['Mc_QTM_12']\n",
    "sorted_data = np.sort(b)\n",
    "# pdf = b/sum(b)\n",
    "# cdf = np.cumsum(pdf)\n",
    "cdf = np.arange(1, len(sorted_data) + 1)/len(b)\n",
    "\n",
    "ax=fig.add_subplot(111)\n",
    "ax.step(sorted_data, cdf)\n",
    "percentiles = np.percentile(sorted_data, [25, 50, 75])\n",
    "cdf_percentiles = np.array([0.25, 0.5, 0.75])\n",
    "# for percentile, cdf_percentile in zip(percentiles, cdf_percentiles):\n",
    "\n",
    "#     plt.vlines(percentile, ymin=0, ymax=cdf_percentile, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "#     # Annotate the percentiles\n",
    "#     plt.text(percentile, cdf_percentile, f'{percentile:.2f}', ha='center', va='top', color='black', fontsize=20)\n",
    "plt.yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "plt.xlabel(f'$\\delta$ $M_c$ (SCSN $M_c$ - QTM 12 $M_c$)')\n",
    "plt.ylabel('CDF')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,8))\n",
    "i, j = 6,1\n",
    "ax = fig.add_subplot(i,j,1)\n",
    "nearest=0.1\n",
    "x = DDET_mainshocks['Maxc']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=nearest), alpha=0.5, label='Maxc_10')\n",
    "x = DDET_mainshocks['Mbass']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=nearest), alpha=0.5, label='Mbass_10')\n",
    "ax.legend()\n",
    "ax.set_xlim(0,4.2)\n",
    "\n",
    "ax = fig.add_subplot(i,j,2)\n",
    "x = DDET_mainshocks['Maxc_50']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=nearest), alpha=0.5, label='Maxc_50')\n",
    "x = DDET_mainshocks['Mbass_50']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=nearest), alpha=0.5, label='Mbass_50')\n",
    "ax.legend()\n",
    "ax.set_xlim(0,4.2)\n",
    "\n",
    "ax = fig.add_subplot(i,j,3)\n",
    "x = DDET_mainshocks['Maxc_50'] - DDET_mainshocks['Maxc']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=0.05), alpha=0.5, label='Maxc_50 - Maxc')\n",
    "x = DDET_mainshocks['Mbass_50'] - DDET_mainshocks['Mbass']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=0.05), alpha=0.5, label='Mbass_50 - Mbass')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(i,j,4)\n",
    "x = DDET_mainshocks['Maxc_50'] - DDET_mainshocks['Mbass_50']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=0.05), alpha=0.5, label='Maxc_50 - Mbass_50')\n",
    "x = DDET_mainshocks['Maxc'] - DDET_mainshocks['Mbass']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=0.05), alpha=0.5, label='Maxc - Mbass')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(i,j,5)\n",
    "x = DDET_mainshocks['b_Mbass']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=nearest), alpha=0.5, label='b_Mbass')\n",
    "x = DDET_mainshocks['b_maxc']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=nearest), alpha=0.5, label='b_maxc')\n",
    "ax.legend()\n",
    "ax.set_xlim(0,4.2)\n",
    "\n",
    "ax = fig.add_subplot(i,j,6)\n",
    "x = DDET_mainshocks['b_Mbass_50']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=nearest), alpha=0.5, label='b_Mbass_50')\n",
    "x = DDET_mainshocks['b_Maxc_50']\n",
    "ax.hist(x, bins=utils.get_bins(x, nearest=nearest), alpha=0.5, label='b_Maxc_50')\n",
    "ax.legend()\n",
    "ax.set_xlim(0,4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_value_dict = {'b_Mbass':{}, 'b_maxc':{}, 'b_Mbass_50':{}, 'b_Maxc_50':{}}\n",
    "for key, item in b_value_dict.items():\n",
    "    b = DDET_mainshocks[key]\n",
    "    item.update({'mean':np.mean(b),\n",
    "                'median':np.nanmedian(b),\n",
    "                'std':np.std(b),\n",
    "                'var':np.var(b),\n",
    "                'se':np.std(b)/np.sqrt(len(b)),\n",
    "                '5_per':np.percentile(b, 5),\n",
    "                '95_per':np.percentile(b, 95)\n",
    "                })\n",
    "\n",
    "b_df = pd.DataFrame.from_dict(b_value_dict)\n",
    "b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in b_df:\n",
    "    print(col, len(DDET_mainshocks.loc[(DDET_mainshocks[col]<=b_value_dict[col]['mean']-b_value_dict[col]['std']) |\\\n",
    "                    (DDET_mainshocks[col]>=b_value_dict[col]['mean']+b_value_dict[col]['std'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,r = 2,2\n",
    "fig = plt.figure(figsize=(8,6), dpi=300)\n",
    "ax = fig.add_subplot(c,r,1)\n",
    "sns.histplot(DDET_mainshocks['radii_50'], ax=ax)\n",
    "\n",
    "ax = fig.add_subplot(c,r,2)\n",
    "ax.scatter(DDET_mainshocks['radii_50'], DDET_mainshocks['n_for_Mc_50'])\n",
    "ax.axhline(275, color='grey', linestyle='--')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('n_for_Mc')\n",
    "ax.set_xlabel('radius')\n",
    "\n",
    "ax = fig.add_subplot(c,r,3)\n",
    "sns.histplot(DDET_mainshocks['Mbass'])\n",
    "\n",
    "ax = fig.add_subplot(c,r,4)\n",
    "sns.histplot(DDET_mainshocks['n_for_Mc_50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting FMDs for one mainshock and all mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainshock = statseis.iterable_mainshock(ID=14477000, mainshock_file=DDET_mainshocks)\n",
    "catalog = catalogue_dict['QTM_12'].copy()\n",
    "local_cat = statseis.create_local_catalogue(mainshock, earthquake_catalogue=catalog, catalogue_name=name, radius_km=100)\n",
    "statseis.plot_local_cat(mainshock=mainshock, local_cat=local_cat, catalogue_name=name, Mc_cut=False, stations=station_df, earthquake_catalogue=catalog,\n",
    "                min_days=math.ceil(local_cat['DAYS_TO_MAINSHOCK'].max()), max_days=0,\n",
    "                radius_km=mainshock.radii_50, box_halfwidth_km=100, aftershock_days=math.floor(local_cat['DAYS_TO_MAINSHOCK'].min()))\n",
    "print(mainshock.n_for_Mc_50)\n",
    "statseis.plot_fmd(local_cat, save_path=f'../outputs/rough/{mainshock.ID}.png', ID=mainshock.ID, radius=mainshock.radii_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'SCSN'\n",
    "statseis.plot_FMD_mainshock_subset(mshock_file=DDET_mainshocks,  outfile_name='DDET_mainshocks', name=name, catalog=catalogue_dict[name])\n",
    "for plot_type in ['data_plots', 'model_plots']:\n",
    "    statseis.move_plots(mshock_file=DDET_mainshocks, out_folder_name='DDET_mainshocks', catalog=name, plot_type=plot_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mainshocks with <275 events within 50 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDET_mainshocks.loc[DDET_mainshocks.isnull().any(axis=1)]\n",
    "\n",
    "bad_mainshocks = DDET_mainshocks.loc[(DDET_mainshocks['n_for_Mc_50']<275)].copy()\n",
    "bad_mainshocks.to_csv('../outputs/bad_mainshocks.csv', index=False)\n",
    "# bad_mainshocks.sort_values(by='ID')[['ID', 'MAGNITUDE', 'n_local_cat_1yr', 'n_local_cat', 'n_for_Mc_50', 'Mbass', 'Mbass_50', 'b_Mbass', 'b_Mbass_50', 'Maxc_50', 'b_Maxc_50', 'radii_50', 'km_to_STA', 'LAT',  'STA_4_km']]\n",
    "\n",
    "good_mainshocks = DDET_mainshocks.loc[(DDET_mainshocks['n_for_Mc_50']>=275)].copy()\n",
    "good_mainshocks.to_csv('../outputs/good_mainshocks.csv', index=False)\n",
    "# good_mainshocks.sort_values(by='STA_4_km')[['ID', 'MAGNITUDE', 'n_local_cat_1yr', 'n_local_cat', 'n_for_Mc_50', 'Mbass', 'Mbass_50', 'b_Mbass', 'b_Mbass_50', 'Maxc_50', 'b_Maxc_50', 'radii_50', 'km_to_STA', 'STA_4_km']]\n",
    "\n",
    "name='SCSN'\n",
    "statseis.plot_FMD_mainshock_subset(mshock_file=bad_mainshocks, name = name, outfile_name = 'bad_mainshocks', catalog=catalogue_dict[name])\n",
    "for plot_type in ['data_plots', 'model_plots']:\n",
    "    statseis.move_plots(mshock_file=bad_mainshocks, catalog = 'SCSN', plot_type = plot_type, out_folder_name='bad_mainshocks')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "utils.basic_cartopy_map(ax)\n",
    "\n",
    "ax.scatter(DDET_mainshocks['LON'], DDET_mainshocks['LAT'], label='All', fc=plot_color_dict['teal'], ec='white', linewidth=0.25, s=200, zorder=100)\n",
    "ax.scatter(bad_mainshocks['LON'], bad_mainshocks['LAT'], label='<250 events', fc=plot_color_dict['orange'], ec='white', linewidth=0.25, s=200, zorder=101)\n",
    "ax.scatter(station_no_dup['LON'], station_no_dup['LAT'],  marker='^', fc=plot_color_dict['pink'], linewidth=0.25, ec='white', #fc='None',\n",
    "            label='STA', zorder=102)\n",
    "extent = utils.get_catalogue_extent(DDET_mainshocks, buffer=0.5)\n",
    "ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "ax.legend()\n",
    "\n",
    "# plt.savefig('../outputs/rough/bad_mainshocks_map.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mainshocks with less than 10 events 1 year prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_events = DDET_mainshocks.loc[(DDET_mainshocks['n_local_cat_1yr']<10) & ~(DDET_mainshocks['ID'].isin(bad_mainshocks['ID']))].copy()\n",
    "# l10_events = DDET_mainshocks.loc[(DDET_mainshocks['n_local_cat_1yr']<10)].copy()\n",
    "print(len(l10_events), len(DDET_mainshocks.loc[~DDET_mainshocks['ID'].isin(l10_events['ID']) & ~(DDET_mainshocks['ID'].isin(bad_mainshocks['ID']))]))\n",
    "# l10_events.sort_values(by='n_local_cat_1yr')[['ID', 'n_local_cat_1yr', 'n_local_cat', 'n_for_Mc_50', 'Mbass', 'Mbass_50', 'b_Mbass', 'b_Mbass_50', 'Maxc_50', 'b_Maxc_50', 'radii_50', 'km_to_STA', 'LAT']]\n",
    "\n",
    "name='SCSN'\n",
    "statseis.plot_FMD_mainshock_subset(mshock_file=l10_events,  outfile_name='l10_events', name=name, catalog=catalogue_dict[name])\n",
    "for plot_type in ['data_plots', 'model_plots']:\n",
    "    statseis.move_plots(mshock_file=l10_events, catalog = 'SCSN', plot_type = plot_type, out_folder_name='l10_events')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "\n",
    "utils.basic_cartopy_map(ax)\n",
    "\n",
    "ax.scatter(DDET_mainshocks['LON'], DDET_mainshocks['LAT'], label='All', fc=plot_color_dict['teal'], ec='white', linewidth=0.25, s=200, zorder=100)\n",
    "ax.scatter(l10_events['LON'], l10_events['LAT'], label='<10 events 1yr prior', fc=plot_color_dict['orange'], ec='white', linewidth=0.25, s=200, zorder=101)\n",
    "ax.scatter(bad_mainshocks['LON'], bad_mainshocks['LAT'], label='<250 events total', fc=plot_color_dict['purple'], ec='white', linewidth=0.25, s=200, zorder=101)\n",
    "ax.scatter(station_no_dup['LON'], station_no_dup['LAT'],  marker='^', fc=plot_color_dict['pink'], linewidth=0.25, ec='white', #fc='None',\n",
    "            label='STA', zorder=102)\n",
    "extent = utils.get_catalogue_extent(DDET_mainshocks, buffer=0.5)\n",
    "ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "ax.legend()\n",
    "\n",
    "# plt.savefig('../outputs/rough/l10_1yr_prior_map.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th station effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDET_mainshocks.loc[DDET_mainshocks['n_local_cat']<250].sort_values(by='n_local_cat')[['ID', 'n_local_cat', 'n_for_Mc_50', 'Mbass', 'Mbass_50', 'b_Mbass', 'b_Mbass_50', 'Maxc_50', 'b_Maxc_50', 'radii_50', 'km_to_STA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,8))\n",
    "i, j = 2,2\n",
    "ax = fig.add_subplot(i,j,1)\n",
    "ax.scatter(DDET_mainshocks['STA_4_km'], DDET_mainshocks['Mbass_50'])\n",
    "\n",
    "ax = fig.add_subplot(i,j,2)\n",
    "ax.scatter(DDET_mainshocks['STA_4_km'], DDET_mainshocks['Maxc_50'])\n",
    "\n",
    "ax = fig.add_subplot(i,j,3)\n",
    "ax.scatter(DDET_mainshocks['STA_4_km'], DDET_mainshocks['Mbass'])\n",
    "\n",
    "ax = fig.add_subplot(i,j,4)\n",
    "ax.scatter(DDET_mainshocks['STA_4_km'], DDET_mainshocks['Maxc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Mc for an expanding radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_mainshocks = pd.read_csv('../outputs/bad_mainshocks.csv')\n",
    "utils.string_to_datetime_df(bad_mainshocks)\n",
    "bad_mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc_expanding_r = statseis.get_Mc_expanding_r(bad_mainshocks, SCSN, 'SCSN', max_r=100)\n",
    "Mc_expanding_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "i, j = 3,2\n",
    "ax = fig.add_subplot(i,j,1)\n",
    "ax2 = fig.add_subplot(i,j,2)\n",
    "ax3 = fig.add_subplot(i,j,3)\n",
    "ax4 = fig.add_subplot(i,j,4)\n",
    "ax5 = fig.add_subplot(i,j,5)\n",
    "ax6 = fig.add_subplot(i,j,6)\n",
    "\n",
    "for mainshock in Mc_expanding_r:\n",
    "    df = mainshock['df'].copy()\n",
    "    ax.plot(df['radii'], df['Mbass'])\n",
    "    ax2.plot(df['radii'], df['Maxc'])\n",
    "    ax3.plot(df['radii'], df['b_Mbass'])\n",
    "    ax4.plot(df['radii'], df['n_local_cat'])\n",
    "    ax5.plot(df['n_local_cat'], df['Mbass'])\n",
    "    ax6.plot(df['n_local_cat'], df['Maxc'])\n",
    "\n",
    "\n",
    "for axes in fig.get_axes():\n",
    "    axes.set_xlabel('radius')\n",
    "\n",
    "ax.set_ylabel(f'$M_c$ Mbass')\n",
    "ax2.set_ylabel(f'Mc maxc')\n",
    "ax3.set_ylabel(f'b-value')\n",
    "ax4.set_ylabel(f'n')\n",
    "\n",
    "ax5.set_ylabel(f'$M_c$ Mbass')\n",
    "ax5.set_xlabel('n')\n",
    "ax5.set_xscale('log')\n",
    "\n",
    "ax6.set_ylabel(f'$M_c$ Maxc')\n",
    "ax6.set_xlabel('n')\n",
    "ax6.set_xscale('log')\n",
    "\n",
    "# plt.savefig('../outputs/rough/Mc_by_radius.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure: Differences between FET and MDET mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = string.ascii_lowercase\n",
    "rows, cols = 2, 2\n",
    "n_plots = rows*cols\n",
    "labels = [letter + ')' for letter in alphabet[:n_plots]]\n",
    "QTM_12_mainshocks = mainshock_dict['QTM_12'].copy()\n",
    "# QTM_12_mainshocks = mainshocks_in_all.copy()\n",
    "El_Mayor = QTM_12_mainshocks.loc[QTM_12_mainshocks['MAGNITUDE']>7].iloc[0]\n",
    "\n",
    "El_Mayor_data = QTM_12_mainshocks.loc[(QTM_12_mainshocks['DATETIME'] > pd.Timestamp(2010,1,1)) &\\\n",
    "                                      (QTM_12_mainshocks['DATETIME'] < pd.Timestamp(2011,4,4))].copy()\n",
    "El_Mayor_data = utils.restrict_catalogue_geographically(El_Mayor_data, region=[-116,-115, 32, 33])\n",
    "\n",
    "El_Mayor_data_dict = {'All':El_Mayor_data}\n",
    "QTM_12_mainshock_dict = {'All':QTM_12_mainshocks}\n",
    "options = ['Neither', 'Both', 'FET', 'MDET']\n",
    "for option in options:\n",
    "    EM_data = El_Mayor_data.loc[El_Mayor_data['Selection']==option].copy()\n",
    "    QTM_12_data = QTM_12_mainshocks.loc[QTM_12_mainshocks['Selection']==option].copy()\n",
    "    print(f\"{option}: {len(QTM_12_data)}\")\n",
    "    El_Mayor_data_dict.update({option:EM_data})\n",
    "    QTM_12_mainshock_dict.update({option:QTM_12_data})\n",
    "\n",
    "box_alpha = 0.2\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs = fig.add_gridspec(2,2)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0], projection=ccrs.PlateCarree())\n",
    "# ax1.set_extent([-121.953, -114.026, 31.86317, 37.22333], crs=ccrs.PlateCarree())\n",
    "ax2 = fig.add_subplot(gs[0, 1], projection=ccrs.PlateCarree())\n",
    "# ax2.set_extent([-116,-115, 32, 33], crs=ccrs.PlateCarree())\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "i = 0\n",
    "QTM_legend_labels = []\n",
    "EM_legend_labels = []\n",
    "for option in options:\n",
    "    EM_data = El_Mayor_data_dict[option]\n",
    "    QTM_12_data = QTM_12_mainshock_dict[option]\n",
    "\n",
    "    if option=='Neither':\n",
    "        colour='grey'\n",
    "        EM_size, QTM_size = [10]*2\n",
    "        i-=1\n",
    "    else:\n",
    "        colour=colours[i]\n",
    "        QTM_size = 2*QTM_12_data['MAGNITUDE']**2\n",
    "        EM_size = 2*EM_data['MAGNITUDE']**2\n",
    "\n",
    "    QTM_legend_labels.append(f\"{option}: {len(QTM_12_data)}\")\n",
    "    EM_legend_labels.append(f\"{option}: {len(EM_data)}\")\n",
    "    ax1.scatter(QTM_12_data['LON'], QTM_12_data['LAT'], s=QTM_size, label=f\"{option}: {len(QTM_12_data)}\", transform=ccrs.PlateCarree(),\n",
    "           color=colour, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "    ax2.scatter(EM_data['LON'], EM_data['LAT'], s=EM_size, label=f\"{option}: {len(EM_data)}\", transform=ccrs.PlateCarree(),\n",
    "           color=colour, edgecolors='white', linewidth=0.5)\n",
    "    radius_km = 10\n",
    "    new_LON, new_LAT = utils.add_distance_to_position_pyproj(El_Mayor['LON'], El_Mayor['LAT'], radius_km, 0)\n",
    "    radius_degrees = new_LON - El_Mayor['LON']\n",
    "    circle_10km = patches.Circle((El_Mayor['LON'], El_Mayor['LAT']), radius=radius_degrees, facecolor='none', edgecolor='red', alpha=0.2)\n",
    "    ax2.add_patch(circle_10km)\n",
    "    QTM_years = utils.datetime_to_decimal_year(QTM_12_data['DATETIME'])\n",
    "    ax3.scatter(QTM_years, QTM_12_data['MAGNITUDE'], s=QTM_size, label=f\"{option}: {len(QTM_12_data)}\",\n",
    "           color=colour, edgecolors='white', linewidth=0.5)\n",
    "    EM_years = utils.datetime_to_decimal_year(EM_data['DATETIME'])\n",
    "    ax4.scatter(EM_years, EM_data['MAGNITUDE'], s=EM_size, label=f\"{option}: {len(EM_data)}\",\n",
    "           color=colour, edgecolors='white', linewidth=0.5)\n",
    "    i+=1\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "    ax.add_feature(cfeature.OCEAN, edgecolor='none')\n",
    "    gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, zorder=0)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xlines = False\n",
    "    gl.ylines = False\n",
    "    gl.xlabel_style = {'size': 15}\n",
    "    gl.ylabel_style = {'size': 15}\n",
    "    # ax.legend()\n",
    "\n",
    "for ax in [ax3, ax4]:\n",
    "    ax.set_ylabel('Magnitude')\n",
    "    ax.set_xlabel('Year')\n",
    "    # ax.legend()\n",
    "\n",
    "i=0\n",
    "for ax in [ax1, ax2, ax3, ax4]:\n",
    "    ax.set_title(labels[i], loc='left')\n",
    "    i+=1\n",
    "\n",
    "ax3.xaxis.set_ticks(range(2008, 2019,4))\n",
    "ax3.set_xticklabels(range(2008, 2019,4))\n",
    "ax4.xaxis.set_ticks(range(2010, 2012,1))\n",
    "ax4.set_xticklabels(range(2010, 2012,1))\n",
    "# ax1.legend(loc='upper center', bbox_to_anchor=(0.55, 1.3), ncol=2, fontsize=12)\n",
    "# ax2.legend(loc='upper center', bbox_to_anchor=(0.55, 1.3), ncol=2, fontsize=12)\n",
    "# ax2.legend(loc='upper right', ncol=1, fontsize=12)\n",
    "\n",
    "rectangle_coords = [-116,-115, 32, 33]\n",
    "rectangle = Rectangle(\n",
    "    (rectangle_coords[0], rectangle_coords[2]),\n",
    "    rectangle_coords[1] - rectangle_coords[0],\n",
    "    rectangle_coords[3] - rectangle_coords[2],\n",
    "    linewidth=1, edgecolor='red', facecolor='none', transform=ccrs.PlateCarree()\n",
    ")\n",
    "ax1.add_patch(rectangle)\n",
    "\n",
    "QTM_legend = [Line2D([], [], marker='o', markersize=5, label=QTM_legend_labels[0], color='grey', linestyle='None'),\n",
    "              Line2D([], [], color=colours[0], marker='o', markersize=10, label=QTM_legend_labels[1], linestyle='None'),\n",
    "              Line2D([], [], color=colours[1], marker='o', markersize=10, label=QTM_legend_labels[2], linestyle='None'),\n",
    "              Line2D([], [], color=colours[2], marker='o', markersize=10, label=QTM_legend_labels[3], linestyle='None'),\n",
    "                ]\n",
    "EM_legend = [Line2D([], [], marker='o', markersize=5, label=EM_legend_labels[0], color='grey', linestyle='None'),\n",
    "              Line2D([], [], color=colours[0], marker='o', markersize=10, label=EM_legend_labels[1], linestyle='None'),\n",
    "              Line2D([], [], color=colours[1], marker='o', markersize=10, label=EM_legend_labels[2], linestyle='None'),\n",
    "              Line2D([], [], color=colours[2], marker='o', markersize=10, label=EM_legend_labels[3], linestyle='None'),\n",
    "                ]\n",
    "\n",
    "ax1.legend(handles=QTM_legend, loc='upper center', bbox_to_anchor=(0.55, 1.3), ncol=2, fontsize=12)\n",
    "ax2.legend(handles=EM_legend, loc='upper center', bbox_to_anchor=(0.55, 1.3), ncol=2, fontsize=11)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"../outputs/figures/mainshock_selection_comp_inc_El_Mayor_v2.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure: mainshock source parameters between catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "columns = 3\n",
    "n_plots = rows*columns\n",
    "alphabet = string.ascii_lowercase\n",
    "labels = [letter + ')' for letter in alphabet[:n_plots]]\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "\n",
    "s=30\n",
    "with plt.rc_context({'axes.titlesize': s,\n",
    "                     'axes.labelsize': s, \n",
    "                     'xtick.labelsize':s,\n",
    "                       'ytick.labelsize':s,\n",
    "                       'lines.markersize':15}):\n",
    "    i=0\n",
    "    ax_map = fig.add_subplot(rows, columns, i+1)\n",
    "    ax_map.set_ylabel('LAT')\n",
    "    ax_map.set_xlabel('LON')\n",
    "    ax_map.set_title(labels[i], loc='left', fontsize=s)\n",
    "\n",
    "    i+=1\n",
    "    ax_loc_diff = fig.add_subplot(rows, columns, i+1)\n",
    "    DDET_mainshocks['QTM_SCSN_loc_diff'] = statseis.calculate_distance_pyproj_vectorized(DDET_mainshocks['LON'], \n",
    "                                                                                                DDET_mainshocks['LAT'],\n",
    "                                                                                                DDET_mainshocks['LON_QTM_12'],\n",
    "                                                                                                DDET_mainshocks['LAT_QTM_12'])\n",
    "    ax_loc_diff.hist(DDET_mainshocks['QTM_SCSN_loc_diff'], bins=utils.get_bins(DDET_mainshocks['QTM_SCSN_loc_diff'], nearest=1))\n",
    "    ax_loc_diff.set_ylabel('N')\n",
    "    ax_loc_diff.set_xlabel(r'$\\delta$ location (km)')\n",
    "    ax_loc_diff.set_title(labels[i], loc='left', fontsize=s)\n",
    "\n",
    "    i+=1\n",
    "    ax_loc_diff_mag = fig.add_subplot(rows, columns, i+1)\n",
    "    # x_min, x_max, y_min, y_max = utils.get_catalogue_extent(DDET_mainshocks)\n",
    "    # x_centre, y_centre = (x_min+x_max)/2, (y_min+y_max)/2\n",
    "    # DDET_mainshocks['km_to_centre'] = statseis.calculate_distance_pyproj_vectorized(x_centre, y_centre, DDET_mainshocks['LON_QTM_12'], DDET_mainshocks['LAT_QTM_12'])\n",
    "    # ax_loc_diff_mag.scatter(DDET_mainshocks['km_to_centre'], DDET_mainshocks['QTM_SCSN_loc_diff'], edgecolors='white', alpha=0.5)\n",
    "    ax_loc_diff_mag.scatter(DDET_mainshocks['STA_4_km'], DDET_mainshocks['QTM_SCSN_loc_diff'], edgecolors='white', alpha=0.5)\n",
    "    # ax_loc_diff_mag.set_xlabel('km to centre')\n",
    "    ax_loc_diff_mag.set_xlabel(r'km to 4$^{th}$ STA')\n",
    "    ax_loc_diff_mag.set_ylabel(r'$\\delta$ location (km)')\n",
    "    ax_loc_diff_mag.set_title(labels[i], loc='left', fontsize=s)\n",
    "\n",
    "    i+=1\n",
    "    ax_time_series = fig.add_subplot(rows, columns, i+1)\n",
    "    ax_time_series.set_ylabel(r'$M_w$')\n",
    "    ax_time_series.set_xlabel('Year')\n",
    "    ax_time_series.set_title(labels[i], loc='left', fontsize=s)\n",
    "\n",
    "    i+=1\n",
    "    ax_fmd = fig.add_subplot(rows, columns, i+1)\n",
    "    ax_fmd.set_ylabel('N')\n",
    "    ax_fmd.set_xlabel('Mainshock $M_w$')\n",
    "    ax_fmd.set_title(labels[i], loc='left', fontsize=s)\n",
    "\n",
    "    i+=1\n",
    "    ax_mag_diff = fig.add_subplot(rows, columns, i+1)\n",
    "    mag_diff = DDET_mainshocks['MAGNITUDE'].values - DDET_mainshocks['MAGNITUDE_QTM_12'].values\n",
    "    ax_mag_diff.hist(mag_diff, bins=utils.get_bins(mag_diff, nearest=0.01))\n",
    "    ax_mag_diff.set_ylabel('N')\n",
    "    ax_mag_diff.set_xlabel(r'$\\delta$ $M_w$')\n",
    "    ax_mag_diff.set_title(labels[i], loc='left', fontsize=s)\n",
    "\n",
    "    i+=1\n",
    "    ax_mag_depth = fig.add_subplot(rows, columns, i+1)\n",
    "    ax_mag_depth.set_ylabel('Depth (km)')\n",
    "    ax_mag_depth.set_xlabel('MAGNITUDE')\n",
    "    ax_mag_depth.set_title(labels[i], loc='left', fontsize=s)\n",
    "    ax_mag_depth.axhline(y=0, color='grey', linestyle='--')\n",
    "\n",
    "    i+=1\n",
    "    ax_depths = fig.add_subplot(rows, columns, i+1)\n",
    "    ax_depths.set_ylabel('N')\n",
    "    ax_depths.set_xlabel('Depth')\n",
    "    ax_depths.set_title(labels[i], loc='left', fontsize=s)\n",
    "    ax_depths.axvline(x=0, color='grey', linestyle='--')\n",
    "\n",
    "    i+=1\n",
    "    ax_depth_diff = fig.add_subplot(rows, columns, i+1)\n",
    "    depth_diff = DDET_mainshocks['DEPTH'].values - DDET_mainshocks['DEPTH_QTM_12'].values\n",
    "    ax_depth_diff.hist(depth_diff, bins=utils.get_bins(depth_diff, nearest=1))\n",
    "    ax_depth_diff.set_ylabel('N')\n",
    "    ax_depth_diff.set_xlabel(r'$\\delta$ Depth (km)')\n",
    "    ax_depth_diff.set_title(labels[i], loc='left', fontsize=s)\n",
    "\n",
    "    i+=1\n",
    "    ax_mag_cat_len = fig.add_subplot(rows, columns, i+1)\n",
    "    ax_mag_cat_len.set_ylabel('1yr prior events (N)')\n",
    "    ax_mag_cat_len.set_xlabel(r'$M_w$')\n",
    "    ax_mag_cat_len.set_title(labels[i], loc='left', fontsize=s)\n",
    "\n",
    "    i+=1\n",
    "    ax_cat_len = fig.add_subplot(rows, columns, i+1)\n",
    "    ax_cat_len.set_ylabel('N')\n",
    "    ax_cat_len.set_xlabel('1yr prior events (N)')\n",
    "    ax_cat_len.set_title(labels[i], loc='left', fontsize=s)\n",
    "\n",
    "    i+=1\n",
    "    ax_cat_len_diff = fig.add_subplot(rows, columns, i+1)\n",
    "    # cat_len_diff = QTM_9_5_mainshocks_in_all['1yr_local_catalogue'].values - SCSN_mainshocks_in_all['1yr_local_catalogue'].values\n",
    "    # ax_cat_len_diff.set_xlabel(fr'1 year cat ($\\delta$ N)')\n",
    "    cat_len_diff = 100*(DDET_mainshocks['n_local_cat_1yr_QTM_12'].values - DDET_mainshocks['n_local_cat_1yr'].values)/DDET_mainshocks['n_local_cat_1yr'].values\n",
    "    ax_cat_len_diff.set_xlabel(fr'1yr prior events ($\\delta$ N %)')\n",
    "    ax_cat_len_diff.hist(cat_len_diff, bins=utils.get_bins(cat_len_diff, nearest=100))\n",
    "    ax_cat_len_diff.set_ylabel('N')\n",
    "    ax_cat_len_diff.set_title(labels[i], loc='left', fontsize=s)\n",
    "    ax_cat_len_diff.axvline(x=0, color='grey', linestyle='--')\n",
    "\n",
    "    for key, s in {'SCSN':'', 'QTM':'_QTM_12'}.items():\n",
    "        mainshock_file = DDET_mainshocks.copy()\n",
    "        selected_mainshocks = mainshock_file.copy()\n",
    "        \n",
    "        # aftershocks = mainshock_file.loc[mainshock_file[f'Selection{s}']=='Neither'].copy() \n",
    "\n",
    "        years= utils.datetime_to_decimal_year(selected_mainshocks[f'DATETIME{s}'])\n",
    "        # ax.stem(years, selected_mainshocks['MAGNITUDE'], basefmt='k-', linefmt='grey', markerfmt='r')\n",
    "        ax_map.scatter(selected_mainshocks[f'LON{s}'], selected_mainshocks[f'LAT{s}'], edgecolors='white', alpha=0.5, label=f'{key}')\n",
    "        ax_time_series.scatter(years, selected_mainshocks[f'MAGNITUDE{s}'], edgecolors='white', alpha=0.5, label=f'{key}')\n",
    "        # years = utils.datetime_to_decimal_year(aftershocks[f'DATETIME{s}'])\n",
    "        # ax_time_series.scatter(years, aftershocks[f'MAGNITUDE{s}'], color='grey', alpha=0.5, label='Aftershocks', s=10, zorder=0)\n",
    "        ax_time_series.set_xticks(range(2009,2021,3))\n",
    "        ax_time_series.set_xticklabels(range(2009,2021,3))\n",
    "        ax_time_series.legend()\n",
    "\n",
    "        ax_fmd.hist(selected_mainshocks[f'MAGNITUDE{s}'], bins=utils.get_bins(selected_mainshocks[f'MAGNITUDE{s}'], nearest=0.1), label=f'{key}', alpha=0.5)\n",
    "\n",
    "        ax_depths.hist(selected_mainshocks[f'DEPTH{s}'], bins=utils.get_bins(selected_mainshocks[f'DEPTH{s}'], nearest=2), label=f'{key}', alpha=0.5)\n",
    "\n",
    "        ax_cat_len.hist(selected_mainshocks[f'n_local_cat_1yr{s}'], bins=utils.get_bins(selected_mainshocks[f'n_local_cat_1yr{s}'], nearest=50), label=f'{key}', alpha=0.5)\n",
    "        ax_cat_len.set_xscale('log')\n",
    "\n",
    "        ax_mag_cat_len.scatter(selected_mainshocks[f'MAGNITUDE{s}'], selected_mainshocks[f'n_local_cat_1yr{s}'], label=f'{key}', alpha=0.5)\n",
    "        ax_mag_cat_len.set_yscale('log')\n",
    "\n",
    "        ax_mag_depth.scatter(selected_mainshocks[f'MAGNITUDE{s}'], selected_mainshocks[f'DEPTH{s}'], alpha=0.5, edgecolors='white', label=f'{key}')\n",
    "\n",
    "        all_axes = fig.get_axes()\n",
    "        # for ax in all_axes:\n",
    "        for ax in [ax_map, ax_time_series, ax_fmd, ax_mag_depth, ax_depths, ax_cat_len, ax_mag_cat_len, ax_mag_depth]:\n",
    "            ax.legend()\n",
    "\n",
    "    ax_mag_depth.set_ylim(ax_mag_depth.get_ylim()[::-1])\n",
    "# plt.savefig(f\"../outputs/figures/mainshock_catalogue_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'mag_diff':mag_diff, 'depth_diff':depth_diff, 'cat_len_diff':cat_len_diff, 'loc_diff':DDET_mainshocks['QTM_SCSN_loc_diff'].values}\n",
    "\n",
    "mainshock_difference_results = []\n",
    "for key, metric in metrics.items():\n",
    "    min, max, median, mean = utils.min_max_median_mean(metric)\n",
    "    mainshock_difference_results.append({'metric':key,\n",
    "                                         'min':min,\n",
    "                                          'max':max,\n",
    "                                           'median':median,\n",
    "                                            'mean':mean})\n",
    "    \n",
    "mainshock_difference_results = pd.DataFrame.from_dict(mainshock_difference_results)\n",
    "mainshock_difference_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_depth_IDs = selected_mainshocks.loc[selected_mainshocks['DEPTH']<=0, 'ID']\n",
    "\n",
    "for key, catalogue in catalogue_dict.items():\n",
    "\n",
    "    print(f\"{key}: {catalogue.loc[catalogue['ID'].isin(negative_depth_IDs), 'DEPTH'].values}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the mainshock selection exclusion thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R0 = 20 # km\n",
    "cr = 5 # km\n",
    "T0 = 50 # days\n",
    "ct = 25 # days\n",
    "\n",
    "C1 = 15\n",
    "C2 = 3.7e-5\n",
    "mu = 3.3e10\n",
    "a_mo = 3\n",
    "b_mo = 6.09\n",
    "a_mw = 1.67\n",
    "b_mw = 4.17\n",
    "\n",
    "scaling_relation_results = [] \n",
    "\n",
    "magnitudes = np.array([4, 5, 6, 6.5, 7, 7.2])\n",
    "\n",
    "WC_SRL = 10**((magnitudes - 5.08)/1.16)\n",
    "\n",
    "WC_sSRL = 10**((magnitudes - 4.38)/1.49)\n",
    "\n",
    "# california_rupture_length = 0.01 * 10 **(0.5*magnitudes) # alternate relation from Wells and Coppersmith (1994) specifically for S.Cali\n",
    "\n",
    "Marsan_RL = 2*0.008*10**(0.5*magnitudes)\n",
    "Marsan_RL_b = 2*0.05*10**(0.5*magnitudes)\n",
    "\n",
    "\n",
    "# Leonard_subsurface_rupture_length = 10**(np.log10(magnitude_to_moment(magnitudes)) - (3/2)*np.log10(C1) - np.log10(C2*mu))*(2/5)/1000**3 # didn't work\n",
    "\n",
    "Leonard_sSRL = 10**((magnitudes - b_mw)/a_mw)\n",
    "\n",
    "R_WC_SRL = (R0 + cr * WC_SRL) #*360/40000 # get R in degrees\n",
    "R_WC_sSRL = (R0 + cr * WC_sSRL)\n",
    "R_Marsan_RL = (R0 + cr * Marsan_RL)\n",
    "R_Leonard_sSRL = (R0 + cr * Leonard_sSRL)\n",
    "T = T0 + ct * (magnitudes - 4)\n",
    "    \n",
    "scaling_relation_results = pd.DataFrame({'magnitude':magnitudes,\n",
    "                                        'WC_SRL_km':WC_SRL,\n",
    "                                        'WC_sSRL_km':WC_sSRL,\n",
    "                                        # 'california_rupture_length_km':california_rupture_length,\n",
    "                                        'Leonard_sSRL_km':Leonard_sSRL,\n",
    "                                        'Marsan_RL_km':Marsan_RL,\n",
    "                                        # 'rupture_length_marsan_a_km':RL_marsan_b,\n",
    "                                        'R_WC_SRL':R_WC_SRL,\n",
    "                                        'R_WC_sSRL':R_WC_sSRL,\n",
    "                                        'R_Marsan_RL':R_Marsan_RL,\n",
    "                                        'R_Leonard_sSRL':R_Leonard_sSRL,\n",
    "                                        'T_days':T\n",
    "                                        })\n",
    "\n",
    "scaling_relation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "El_Mayor = QTM_12_mainshocks.loc[QTM_12_mainshocks['MAGNITUDE']>7].iloc[0]\n",
    "\n",
    "start_date, end_date = pd.Timestamp(2009,10,4), pd.Timestamp(2011,4,4)\n",
    "all_sizes_EM = QTM_12.loc[(QTM_12['DATETIME'] > start_date) &\\\n",
    "                                      (QTM_12['DATETIME'] < end_date)].copy()\n",
    "all_sizes_EM['DAYS_POST_MAINSHOCK'] = (El_Mayor['DATETIME'] - all_sizes_EM['DATETIME']).apply(lambda d: (d.total_seconds()/(24*3600)))*-1\n",
    "all_sizes_EM['DISTANCE_TO_MAINSHOCK'] = statseis.calculate_distance_pyproj_vectorized(El_Mayor['LON'], El_Mayor['LAT'], all_sizes_EM['LON'],  all_sizes_EM['LAT'])\n",
    "all_sizes_EM_pre = all_sizes_EM.loc[(all_sizes_EM['DAYS_POST_MAINSHOCK']<0)].copy()\n",
    "all_sizes_EM = all_sizes_EM.loc[(all_sizes_EM['DAYS_POST_MAINSHOCK']>0)].copy()\n",
    "\n",
    "\n",
    "El_Mayor_data = QTM_12_mainshocks.loc[(QTM_12_mainshocks['DATETIME'] > start_date) &\\\n",
    "                                      (QTM_12_mainshocks['DATETIME'] < end_date)].copy()\n",
    "# El_Mayor_data = utils.restrict_catalogue_geographically(El_Mayor_data, region=[-116,-115, 32, 33])\n",
    "\n",
    "El_Mayor_data['DAYS_POST_MAINSHOCK'] = (El_Mayor['DATETIME'] - El_Mayor_data['DATETIME']).apply(lambda d: (d.total_seconds()/(24*3600)))*-1\n",
    "\n",
    "El_Mayor_data['DISTANCE_TO_MAINSHOCK'] = statseis.calculate_distance_pyproj_vectorized(El_Mayor['LON'], El_Mayor['LAT'], El_Mayor_data['LON'],  El_Mayor_data['LAT'])\n",
    "El_Mayor_data_pre = El_Mayor_data.loc[(El_Mayor_data['DAYS_POST_MAINSHOCK']<0)].copy()\n",
    "El_Mayor_data = El_Mayor_data.loc[(El_Mayor_data['DAYS_POST_MAINSHOCK']>0)].copy()\n",
    "El_Mayor_data = El_Mayor_data.loc[El_Mayor_data['ID'].isin(merged_mainshocks_catalog['ID'])].copy()\n",
    "\n",
    "scale_eq_marker = (lambda x: 10 + np.exp(1.1*x))\n",
    "neither_relation = (lambda x: scale_eq_marker(x)/2)\n",
    "selection_dict = {'Both':{'zorder':11, 'color':plot_color_dict['teal'], 'size':scale_eq_marker, 'linewidth':0.5},\n",
    "                    'FET':{'zorder':12, 'color':plot_color_dict['orange'], 'size':scale_eq_marker, 'linewidth':0.5},\n",
    "                    'MDET':{'zorder':13, 'color':plot_color_dict['purple'], 'size':scale_eq_marker, 'linewidth':0.5},\n",
    "                    'Neither':{'zorder':10, 'color':plot_color_dict['yellow'], 'size':scale_eq_marker, 'linewidth':0.5}\n",
    "                    }\n",
    "\n",
    "scale = 1\n",
    "size_dict = {'figsize_x': 8*scale, \n",
    "             'figsize_y': 7*scale,\n",
    "             'textsize': 20*scale,\n",
    "             'linewidth':3*scale,\n",
    "             'edge_width':1/scale,\n",
    "             'legend_markersize':10*scale,\n",
    "             'point_alpha':0.9,\n",
    "             'ec':'white',\n",
    "             'fault_lw':2/scale,\n",
    "             'fault_fc':'black',\n",
    "             'label_pos':(-122.05, 31.65)\n",
    "             }\n",
    "\n",
    "fig = plt.figure(figsize=(6,8))\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('a)', loc='left')\n",
    "\n",
    "x = np.array(scaling_relation_results['T_days'])[::-1]\n",
    "y = np.array(scaling_relation_results['R_WC_SRL'])[::-1]\n",
    "labels = np.array(scaling_relation_results['magnitude'])[::-1]\n",
    "# plt.scatter(x, y, color=colours[0:len(x)])\n",
    "line_colors = ['#7570b3','#e7298a','#66a61e','#e6ab02','#a6761d','#666666']\n",
    "n_lines = range(len(x)) \n",
    "for i in n_lines:\n",
    "    line_color = line_colors[i]\n",
    "    if i ==0:\n",
    "        label = f\"MDET - {labels[i]}\"\n",
    "    else:\n",
    "        label = labels[i]\n",
    "    ax1.plot([x[i], x[i]], [0, y[i]], color=line_color,#color=colours[i], \n",
    "             linestyle='-', label=label)  \n",
    "    ax1.plot([0, x[i]], [y[i], y[i]], color=line_color,#color=colours[i], \n",
    "             linestyle='-')\n",
    "    # plt.text(x[i], y[i], labels[i], fontsize=12, ha='center', va='bottom')\n",
    "    # plt.fill_between([0, x[i]], [y[i], y[i]], color=colours[i], alpha=1)\n",
    "    # plt.fill_between([x[i], x[i]], [0, y[i]], color=colours[i], alpha=1)\n",
    "\n",
    "# plt.scatter(365, 10, color='black', label='FET')\n",
    "ax1.plot([365, 365], [0, 10], color=plot_color_dict['orange'], linestyle='-', label='FET')  \n",
    "ax1.plot([0, 365], [10, 10], color=plot_color_dict['orange'], linestyle='-')\n",
    "# plt.text(365, 15, 'FET', fontsize=12, ha='center', va='bottom')\n",
    "# plt.fill_between([0, 365], [10, 10], color='black', alpha=1)\n",
    "# plt.fill_between([365, 365], [0, 10], color='black', alpha=1)\n",
    "\n",
    "ax1.set_xlabel('T (days)')\n",
    "ax1.set_ylabel('R (km)')\n",
    "ax1.margins(x=0, y=0)\n",
    "ax1.set_xlim(0,380)\n",
    "ax1.set_ylim(0,420)\n",
    "ax1.legend(fontsize=15, bbox_to_anchor=(1.02, 1.03), loc='upper right')\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.set_title('b)', loc='left')\n",
    "x = np.array(scaling_relation_results['T_days'])[::-1]\n",
    "y = np.array(scaling_relation_results['R_WC_SRL'])[::-1]\n",
    "labels = np.array(scaling_relation_results['magnitude'])[::-1]\n",
    "# plt.scatter(x, y, color=colours[0:len(x)])\n",
    "\n",
    "# for i in range(len(x)):\n",
    "#     plt.plot([x[i], x[i]], [0, y[i]], color=colours[i], linestyle='-', label=labels[i])  \n",
    "#     plt.plot([0, x[i]], [y[i], y[i]], color=colours[i], linestyle='-')\n",
    "#     # plt.text(x[i], y[i], labels[i], fontsize=12, ha='center', va='bottom')\n",
    "    # plt.fill_between([0, x[i]], [y[i], y[i]], color=colours[i], alpha=1)\n",
    "    # plt.fill_between([x[i], x[i]], [0, y[i]], color=colours[i], alpha=1)\n",
    "\n",
    "# plt.scatter(365, 10, color='black', label='FET')\n",
    "ax2.scatter(0,0, label='El Mayor-Cucupah', s=0.5*np.exp(7.2), marker='*', color='black', zorder=100)\n",
    "ax2.plot([365, 365], [0, 10], color=plot_color_dict['orange'], linestyle='-', label='FET excludes')  \n",
    "ax2.plot([0, 365], [10, 10], color=plot_color_dict['orange'], linestyle='-')\n",
    "ax2.plot([130, 130], [0, 356], color=plot_color_dict['purple'], linestyle='-', label='MDET excludes')  \n",
    "ax2.plot([0, 130], [356, 356], color=plot_color_dict['purple'], linestyle='-')\n",
    "# plt.text(365, 15, 'FET', fontsize=12, ha='center', va='bottom')\n",
    "ax2.fill_between([0, 365], [10, 10], color=plot_color_dict['orange'], alpha=0.2)\n",
    "ax2.fill_between([0, 130], [356, 356], color=plot_color_dict['purple'], alpha=0.2)\n",
    "\n",
    "i = 0\n",
    "EM_legend_labels = []\n",
    "for option, params in selection_dict.items():\n",
    "    color=params['color']\n",
    "    EM_data = El_Mayor_data.loc[El_Mayor_data['Selection']==option].sort_values(by='MAGNITUDE', ascending=False)\n",
    "    EM_data_pre = El_Mayor_data_pre.loc[El_Mayor_data_pre['Selection']==option].sort_values(by='MAGNITUDE', ascending=False)\n",
    "\n",
    "    print(option, len(EM_data))\n",
    "    EM_size = params['size'](EM_data['MAGNITUDE'])\n",
    "    EM_pre_size = params['size'](EM_data_pre['MAGNITUDE'])\n",
    "\n",
    "    EM_legend_labels.append(f\"{option}: {len(EM_data)}\")\n",
    "    ax2.scatter(EM_data['DAYS_POST_MAINSHOCK'], EM_data['DISTANCE_TO_MAINSHOCK'], s=EM_size, label=f\"{option}\", zorder=params['zorder'],\n",
    "           color=color, ec='white', linewidth=0.5)\n",
    "    ax2.scatter(EM_data_pre['DAYS_POST_MAINSHOCK'], EM_data_pre['DISTANCE_TO_MAINSHOCK'], s=EM_pre_size, #label=f\"{option} selected\",\n",
    "           color=color, edgecolors='white', linewidth=0.5, zorder=params['zorder'],)\n",
    "    i+=1\n",
    "# plt.scatter(El_Mayor_data['DAYS_POST_MAINSHOCK'], El_Mayor_data['DISTANCE_TO_MAINSHOCK'])\n",
    "\n",
    "ax2.scatter(all_sizes_EM['DAYS_POST_MAINSHOCK'], all_sizes_EM['DISTANCE_TO_MAINSHOCK'], s=0.125*np.exp(all_sizes_EM['MAGNITUDE']), zorder=0, color='grey', alpha=0.1, label='all seismicity')\n",
    "ax2.scatter(all_sizes_EM_pre['DAYS_POST_MAINSHOCK'], all_sizes_EM_pre['DISTANCE_TO_MAINSHOCK'], s=0.125*np.exp(all_sizes_EM_pre['MAGNITUDE']), zorder=0, color='grey', alpha=0.1)\n",
    "\n",
    "ax2.set_xlabel('T (days)')\n",
    "ax2.set_ylabel('R (km)')\n",
    "ax2.margins(x=0, y=0)\n",
    "# plt.xlim(-20,380)\n",
    "# plt.ylim(-20,500)\n",
    "ax2.set_xlim(-110,380)\n",
    "ax2.set_ylim(-30,550)\n",
    "# ax2.legend(loc='upper center', bbox_to_anchor=(0.375,1.45), ncols=1)\n",
    "\n",
    "legend_handles = [Line2D([], [], color='black', marker='*', markersize=size_dict['legend_markersize']*2, label='El Mayor-Cucupah', linestyle='None'),\n",
    "                  Line2D([], [], color='black', marker='None', markersize=size_dict['legend_markersize']*2, label='', linestyle='None'),\n",
    "                  Line2D([], [], color=plot_color_dict['teal'], marker='o', markersize=size_dict['legend_markersize'], label='DDET', linestyle='None'),\n",
    "                  Line2D([], [], color=plot_color_dict['orange'], marker='o', markersize=size_dict['legend_markersize'], label='FET only', linestyle='None'),\n",
    "                  Line2D([], [], color=plot_color_dict['purple'], marker='o', markersize=size_dict['legend_markersize'], label='MDET only', linestyle='None'),\n",
    "                  Line2D([], [], color=plot_color_dict['yellow'], marker='o', markersize=size_dict['legend_markersize'], label='None', linestyle='None'),\n",
    "                #   Line2D([], [], color=plot_color_dict['grey'], marker='o', markersize=size_dict['legend_markersize']/3, label='All seismicity', linestyle='None')\n",
    "                  ]\n",
    "\n",
    "ax2.legend(handles=legend_handles, loc='upper right', ncol=1, framealpha=0.5, fontsize=15, bbox_to_anchor=(1.02, 1.03),\n",
    "           handletextpad=0.5, borderpad=0.5, labelspacing=0.5, handlelength=0.5, columnspacing=0.75).set_zorder(1000)\n",
    "\n",
    "ax2.annotate('Selected by:', (150, 425), size=15, zorder=2000)\n",
    "# ax2.legend()\n",
    "plt.tight_layout()\n",
    "# fig.subplots_adjust(hspace=-1)\n",
    "# plt.savefig('../outputs/figures/threshold_multiplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "i = 0\n",
    "EM_legend_labels = []\n",
    "\n",
    "i = 0\n",
    "EM_legend_labels = []\n",
    "for option, params in selection_dict.items():\n",
    "    color=params['color']\n",
    "    EM_data = El_Mayor_data.loc[El_Mayor_data['Selection']==option].sort_values(by='MAGNITUDE', ascending=False)\n",
    "    EM_data_pre = El_Mayor_data_pre.loc[El_Mayor_data_pre['Selection']==option].sort_values(by='MAGNITUDE', ascending=False)\n",
    "\n",
    "    print(option, len(EM_data))\n",
    "    EM_size = params['size'](EM_data['MAGNITUDE'])\n",
    "    EM_pre_size = params['size'](EM_data_pre['MAGNITUDE'])\n",
    "\n",
    "    EM_legend_labels.append(f\"{option}: {len(EM_data)}\")\n",
    "    ax.scatter(EM_data['LON'], EM_data['LAT'], s=EM_size, label=f\"{option}\", zorder=params['zorder'],\n",
    "           color=color, ec='white', linewidth=0.5)\n",
    "    ax.scatter(EM_data_pre['LON'], EM_data_pre['LAT'], s=EM_pre_size,\n",
    "           color=color, edgecolors='white', linewidth=0.5, zorder=params['zorder'])\n",
    "    i+=1\n",
    "\n",
    "ax.scatter(all_sizes_EM['LON'], all_sizes_EM['LAT'], s=0.125*np.exp(all_sizes_EM['MAGNITUDE']), zorder=0, fc='none', ec='grey', alpha=0.1, label='1 year post', transform=ccrs.PlateCarree())\n",
    "ax.scatter(all_sizes_EM_pre['LON'], all_sizes_EM_pre['LAT'], s=0.125*np.exp(all_sizes_EM_pre['MAGNITUDE']), zorder=0, color='grey', alpha=0.1, label='6 months pre', transform=ccrs.PlateCarree())\n",
    "ax.scatter(El_Mayor['LON'], El_Mayor['LAT'], s=1*np.exp(7.2), marker='*', color='red', zorder=0, alpha=0.2)\n",
    "ax.legend()\n",
    "\n",
    "gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, zorder=0)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xlines = False\n",
    "gl.ylines = False\n",
    "gl.xlabel_style = {'size': 15}\n",
    "gl.ylabel_style = {'size': 15}\n",
    "# plt.savefig('../outputs/rough/El_Mayor_pre_and_post_map.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of thresholds for different rupture length scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(scaling_relation_results['magnitude'], \n",
    "         scaling_relation_results['WC_SRL_km'],\n",
    "        label='WC_SRL')\n",
    "ax.plot(scaling_relation_results['magnitude'], \n",
    "         scaling_relation_results['WC_sSRL_km'],\n",
    "        label='WC_sSRL')\n",
    "ax.plot(scaling_relation_results['magnitude'], \n",
    "         scaling_relation_results['Marsan_RL_km'],\n",
    "        label='Marsan_RL')\n",
    "ax.plot(scaling_relation_results['magnitude'], \n",
    "         scaling_relation_results['Leonard_sSRL_km'],\n",
    "        label='Leonard_sSRL')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Magnitude',fontsize=20)\n",
    "ax.set_ylabel('L (km)',fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(scaling_relation_results['magnitude'], \n",
    "         scaling_relation_results['R_WC_SRL'],\n",
    "        label='R_WC_SRL')\n",
    "ax.plot(scaling_relation_results['magnitude'], \n",
    "         scaling_relation_results['R_WC_sSRL'],\n",
    "        label='R_WC_sSRL')\n",
    "ax.plot(scaling_relation_results['magnitude'], \n",
    "         scaling_relation_results['R_Marsan_RL'],\n",
    "        label='R_Marsan_RL')\n",
    "ax.plot(scaling_relation_results['magnitude'], \n",
    "         scaling_relation_results['R_Leonard_sSRL'],\n",
    "        label='R_Leonard_sSRL')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Magnitude',fontsize=20)\n",
    "ax.set_ylabel('R (km)',fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('../outputs/rough/RL_comp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scaling_relation_results['magnitude'],\n",
    "         scaling_relation_results['T_days'])\n",
    "plt.xlabel('Magnitude',fontsize=20)\n",
    "plt.ylabel('Exclusion time (days)',fontsize=20)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../outputs/rough/TR_time_exclusion_scaling.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding El Mayor-Cucupah slip models\n",
    "Wei et al. (2011): http://equake-rc.info/SRCMOD/searchmodels/viewmodel/s2010ELMAYO01WEIx/  \n",
    "Mendoza et al. (2013): http://equake-rc.info/SRCMOD/searchmodels/viewmodel/s2010ELMAYO01MEND/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonblank_lines(f):\n",
    "    for l in f:\n",
    "        line = l.rstrip()\n",
    "        if line:\n",
    "            yield line\n",
    "\n",
    "def load_slip_model(slip_file_path):\n",
    "    myfile = open(slip_file_path)\n",
    "    Wei_2011 = []\n",
    "    sliplon = []; sliplat = []; slipdep = []; slipslip = []\n",
    "    for line in nonblank_lines(myfile):\n",
    "        if not line.startswith(\"%\"):\n",
    "            line = line.split()\n",
    "            Wei_2011.append({'Lat':float(line[0]),\n",
    "                            'Lon':float(line[1]),\n",
    "                            'Depth':float(line[4]),\n",
    "                            'Slip':float(line[5])})\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    myfile.close()\n",
    "    slip_df = pd.DataFrame.from_dict(Wei_2011)\n",
    "    print('\\n')\n",
    "    print('Slip model loaded, this has %d cells' % len(slip_df))\n",
    "    print('\\n')\n",
    "    print('Done')\n",
    "    return slip_df\n",
    "\n",
    "slip_file_path = '../data/SRCmod/El_Mayor_Cucupah/Wei_2011/s2010ELMAYO01WEIx.fsp'\n",
    "Wei_2011 = load_slip_model(slip_file_path)\n",
    "\n",
    "slip_file_path = '../data/SRCmod/El_Mayor_Cucupah/Mendoza_2013/s2010ELMAYO01MEND.fsp'\n",
    "Mendoza_2013 = load_slip_model(slip_file_path)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = fig.add_subplot(121, projection=ccrs.PlateCarree())\n",
    "cm = plt.get_cmap('jet')\n",
    "cNorm = mcolors.Normalize(vmin=Wei_2011['Slip'].min(), vmax=Wei_2011['Slip'].max())\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "Wei_2011_nonzero = Wei_2011.loc[Wei_2011['Slip']!=0].copy()\n",
    "sc = ax.scatter(Wei_2011_nonzero['Lon'], Wei_2011_nonzero['Lat'], c=Wei_2011_nonzero['Slip'], cmap=cm, norm=cNorm, alpha=0.5, zorder=3, marker='D')\n",
    "cbar = fig.colorbar(scalarMap, ax=ax)\n",
    "cbar.set_label('Slip')\n",
    "gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, zorder=0)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xlines = False\n",
    "gl.ylines = False\n",
    "gl.xlabel_style = {'size': 15}\n",
    "gl.ylabel_style = {'size': 15}\n",
    "\n",
    "ax = fig.add_subplot(122, projection=ccrs.PlateCarree())\n",
    "cm = plt.get_cmap('jet')\n",
    "cNorm = mcolors.Normalize(vmin=Mendoza_2013['Slip'].min(), vmax=Mendoza_2013['Slip'].max())\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "Mendoza_2013_nonzero = Mendoza_2013.loc[Mendoza_2013['Slip']!=0].copy()\n",
    "sc = ax.scatter(Mendoza_2013_nonzero['Lon'], Mendoza_2013_nonzero['Lat'], c=Mendoza_2013_nonzero['Slip'], cmap=cm, norm=cNorm, alpha=0.5, zorder=3, marker='D')\n",
    "cbar = fig.colorbar(scalarMap, ax=ax)\n",
    "cbar.set_label('Slip')\n",
    "gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, zorder=0)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xlines = False\n",
    "gl.ylines = False\n",
    "gl.xlabel_style = {'size': 15}\n",
    "gl.ylabel_style = {'size': 15}\n",
    "# plt.savefig('../outputs/rough/El_Mayor_slip_models.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "i = 0\n",
    "EM_legend_labels = []\n",
    "options = ['Neither', 'Both', 'FET', 'MDET']\n",
    "plot_scalar = 3\n",
    "for option in options:\n",
    "    EM_data = El_Mayor_data.loc[El_Mayor_data['Selection']==option].copy()\n",
    "    print(option, len(EM_data))\n",
    "\n",
    "    if option=='Neither':\n",
    "        colour='grey'\n",
    "        EM_size = 10\n",
    "        i-=1\n",
    "    else:\n",
    "        colour=colours[i]\n",
    "        EM_size = (plot_scalar/6)*np.exp(EM_data['MAGNITUDE'])\n",
    "\n",
    "    EM_legend_labels.append(f\"{option}: {len(EM_data)}\")\n",
    "    ax.scatter(EM_data['LON'], EM_data['LAT'], s=EM_size, label=f\"{option} selected\", transform=ccrs.PlateCarree(),\n",
    "           color=colour, edgecolors='white', linewidth=0.5)\n",
    "    i+=1\n",
    "\n",
    "ax.scatter(all_sizes_EM['LON'], all_sizes_EM['LAT'], s=0.125*np.exp(all_sizes_EM['MAGNITUDE']), zorder=0, color='grey', alpha=0.1, label='all seismicity', transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.scatter(El_Mayor['LON'], El_Mayor['LAT'], s=plot_scalar*np.exp(7.2), marker='*', color='red', zorder=0, alpha=0.2)\n",
    "\n",
    "cm = plt.get_cmap('jet')\n",
    "cNorm = mcolors.Normalize(vmin=Wei_2011['Slip'].min(), vmax=Wei_2011['Slip'].max())\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "sc = ax.scatter(Wei_2011_nonzero['Lon'], Wei_2011_nonzero['Lat'], c=Wei_2011_nonzero['Slip'], cmap=cm, norm=cNorm, alpha=0.1, zorder=0, marker='D')\n",
    "cbar = fig.colorbar(scalarMap, ax=ax)\n",
    "cbar.set_label('Slip')\n",
    "\n",
    "ax.legend()\n",
    "gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, zorder=0)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xlines = False\n",
    "gl.ylines = False\n",
    "gl.xlabel_style = {'size': 15}\n",
    "gl.ylabel_style = {'size': 15}\n",
    "# ax.set_extent([-116.7, -114.3, 31.7, 34.1], crs=ccrs.PlateCarree())\n",
    "ax.set_extent([-116.1, -114.5, 31.8, 32.9], crs=ccrs.PlateCarree())\n",
    "# plt.savefig('../outputs/rough/El_Mayor_aftershocks_map.png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculating foreshock rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Testing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example mainshock plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catalogue_name = 'QTM_12'\n",
    "IDs = [37301704, 11006189]\n",
    "for catalogue_name in ['SCSN', 'QTM_12']:\n",
    "    for ID in IDs:\n",
    "        for mcut in [True, False]:\n",
    "            earthquake_catalogue = catalogue_dict[catalogue_name].copy()\n",
    "            mainshock_file = mainshock_dict[catalogue_name].copy()\n",
    "            # mainshock = mainshock_file.loc[mainshock_file['ID']==ID]\n",
    "            # RowTuple = namedtuple('RowTuple', mainshock.columns)\n",
    "            # mainshock = [RowTuple(*row) for row in mainshock.values][0]\n",
    "            mainshock = statseis.iterable_mainshock(ID=ID, mainshock_file=mainshock_file)\n",
    "            local_cat = statseis.create_local_catalogue(mainshock, earthquake_catalogue, catalogue_name=catalogue_name)\n",
    "            if mcut==True:\n",
    "                local_cat = local_cat.loc[local_cat['MAGNITUDE']>=mainshock.Mc].copy()\n",
    "            # statseis.plot_local_cat(mainshock=mainshock, local_cat=local_cat, catalogue_name=catalogue_name, Mc_cut=Mc_cut, stations=station_df)\n",
    "            results_dict, file_dict = statseis.identify_foreshocks_short(local_catalogue=local_cat, mainshock=mainshock, earthquake_catalogue=earthquake_catalogue)\n",
    "            statseis.plot_models(mainshock=mainshock, results_dict=results_dict, file_dict=file_dict, Mc_cut=mcut, catalogue_name=catalogue_name)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "for catalog in ['SCSN', 'QTM_12']:\n",
    "    statseis.move_plots(DDET_mainshocks.loc[DDET_mainshocks['ID'].isin(IDs)], catalog=catalog, plot_type='model_plots', out_folder_name=f'Example_mshocks/{catalog}')\n",
    "for catalog in ['SCSN', 'QTM_12']:\n",
    "    statseis.move_plots(DDET_mainshocks.loc[DDET_mainshocks['ID'].isin(IDs)], catalog=catalog, plot_type='Mc_cut/model_plots', out_folder_name=f'Example_mshocks/{catalog}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID = 1137108\n",
    "ID = 14998780\n",
    "name = 'QTM_12'\n",
    "Mc_cut = False\n",
    "statseis.plot_single_mainshock(ID=ID, mainshock_file=mainshock_dict[name], earthquake_catalogue=catalogue_dict[name], catalogue_name=name, Mc_cut=Mc_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 14998780\n",
    "catalogue_name = 'QTM_12'\n",
    "Mc_cut = False\n",
    "statseis.plot_single_mainshock(ID=ID, mainshock_file=mainshock_dict[name], earthquake_catalogue=catalogue_dict[name], catalogue_name=name, Mc_cut=Mc_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID = 37301704 #10527789, 15481673]\n",
    "# ID = 14998780\n",
    "IDs = [10319593, 14481152, 10148002, 14692972, 14850084, 15017828, 15220393]\n",
    "ID = IDs[2]\n",
    "\n",
    "# catalogue_name = 'QTM_12'\n",
    "catalogue_name = 'SCSN'\n",
    "earthquake_catalogue = catalogue_dict[catalogue_name].copy()\n",
    "mainshock_file = mainshock_dict[catalogue_name].copy()\n",
    "mainshock = mainshock_file.loc[mainshock_file['ID']==ID]\n",
    "RowTuple = namedtuple('RowTuple', mainshock.columns)\n",
    "mainshock = [RowTuple(*row) for row in mainshock.values][0]\n",
    "local_cat = statseis.create_local_catalogue(mainshock, earthquake_catalogue, catalogue_name=catalogue_name)\n",
    "\n",
    "min_days, max_days = 365, 0\n",
    "\n",
    "outside = local_cat.loc[(local_cat['DISTANCE_TO_MAINSHOCK']>=10) &\\\n",
    "                            (local_cat['DAYS_TO_MAINSHOCK'] < min_days) &\\\n",
    "                            (local_cat['DAYS_TO_MAINSHOCK'] > max_days+20)].copy()\n",
    "\n",
    "outside_foreshocks = local_cat.loc[(local_cat['DISTANCE_TO_MAINSHOCK']>=10) &\\\n",
    "                            (local_cat['DAYS_TO_MAINSHOCK'] < 20) &\\\n",
    "                            (local_cat['DAYS_TO_MAINSHOCK'] > 0)].copy()\n",
    "\n",
    "modelling_events = local_cat.loc[(local_cat['DAYS_TO_MAINSHOCK'] < min_days) &\\\n",
    "                                    (local_cat['DAYS_TO_MAINSHOCK'] > max_days+20) &\\\n",
    "                                    (local_cat['DISTANCE_TO_MAINSHOCK']<10)].copy()\n",
    "\n",
    "QTM_12_local_cat = local_cat.loc[(local_cat['DAYS_TO_MAINSHOCK'] < 0.1) &\\\n",
    "                                    (local_cat['DAYS_TO_MAINSHOCK'] > -0.1)].copy()\n",
    "\n",
    "QTM_12_local_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catalogue_name = 'SCSN'\n",
    "catalogue_name = 'QTM_12'\n",
    "earthquake_catalogue = catalogue_dict[catalogue_name].copy()\n",
    "SCSN_local_cat = statseis.create_local_catalogue(mainshock, earthquake_catalogue, catalogue_name=catalogue_name)\n",
    "shift = 0.1\n",
    "SCSN_local_cat = SCSN_local_cat.loc[(SCSN_local_cat['DAYS_TO_MAINSHOCK'] < shift) &\\\n",
    "                                    (SCSN_local_cat['DAYS_TO_MAINSHOCK'] > -shift)].copy()\n",
    "\n",
    "# SCSN_local_cat = SCSN_local_cat.loc[(SCSN_local_cat['DISTANCE_TO_MAINSHOCK'] < 24)].copy()\n",
    "\n",
    "SCSN_local_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = [10319593, 14481152, 10148002, 14692972, 14850084, 15017828, 15220393]\n",
    "\n",
    "for ID in IDs:\n",
    "    statseis.plot_single_mainshock(ID=ID, mainshock_file=mainshock_dict[name], earthquake_catalogue=catalogue_dict[name], catalogue_name=name, Mc_cut=Mc_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_to_copy = [str(x) + '.png' for x in IDs]\n",
    "\n",
    "source_folder = '../outputs/SCSN/data_plots/'\n",
    "\n",
    "destination_folder = '../outputs/SCSN/data_plots/SCSN_only/'\n",
    "\n",
    "for filename in filenames_to_copy:\n",
    "    source_file = os.path.join(source_folder, filename)\n",
    "    \n",
    "    if os.path.exists(source_file):\n",
    "        destination_file = os.path.join(destination_folder, filename)\n",
    "        \n",
    "        shutil.copyfile(source_file, destination_file)\n",
    "        print(f\"File '{filename}' copied to '{destination_folder}'\")\n",
    "    else:\n",
    "        print(f\"File '{filename}' not found in '{source_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 10319593\n",
    "# ID = 14481152\n",
    "# ID = 10148002\n",
    "# ID = 14692972\n",
    "# ID = 14850084\n",
    "# ID = 15017828\n",
    "# ID = 15220393\n",
    "\n",
    "catalogue_name = 'SCSN'\n",
    "Mc_cut = False\n",
    "\n",
    "earthquake_catalogue = catalogue_dict[catalogue_name].copy()\n",
    "statseis.find_event_in_catalog(ID, catalog=earthquake_catalogue)\n",
    "\n",
    "plot_single_mainshock(ID=ID, catalogue_name=catalogue_name, Mc_cut=Mc_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_name = 'QTM_12'\n",
    "\n",
    "earthquake_catalogue = catalogue_dict[catalogue_name].copy()\n",
    "statseis.find_event_in_catalog(ID, catalog=earthquake_catalogue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Example of functions for a single mainshock\n",
    "Solve problem: Why are my example functions model and local cat plots different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_name = 'QTM_12'\n",
    "# catalogue_name = 'SCSN'\n",
    "# ID = 37301704 #10527789, 15481673]\n",
    "# ID = 11006189\n",
    "# ID = 11049285\n",
    "ID = 37506472\n",
    "earthquake_catalogue = catalogue_dict[catalogue_name].copy()\n",
    "mainshock_file = mainshock_dict[catalogue_name].copy()\n",
    "mainshock = statseis.iterable_mainshock(ID=ID, mainshock_file=mainshock_file)\n",
    "local_cat = statseis.create_local_catalogue(mainshock, earthquake_catalogue, catalogue_name=catalogue_name)\n",
    "local_cat.loc[local_cat['DAYS_TO_MAINSHOCK']==0]\n",
    "Mc_cut = False\n",
    "# Mc_cut = True\n",
    "if Mc_cut==True:\n",
    "    # local_cat = apply_Mc_cut(local_cat)\n",
    "    local_cat = local_cat.loc[local_cat['MAGNITUDE']>=mainshock.Mc].copy()\n",
    "local_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statseis.plot_local_cat(mainshock=mainshock, local_cat=local_cat, catalogue_name=catalogue_name, earthquake_catalogue=earthquake_catalogue, Mc_cut=Mc_cut, stations=station_df, radius_km=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict, file_dict = statseis.identify_foreshocks_short(local_catalogue=local_cat, mainshock=mainshock, earthquake_catalogue=earthquake_catalogue)\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statseis.plot_models(mainshock=mainshock, results_dict=results_dict, file_dict=file_dict, Mc_cut=False, catalogue_name=catalogue_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Mc cut True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_name = 'QTM_12'\n",
    "ID = 10374021 #37301704 #15481673\n",
    "mainshock = statseis.iterable_mainshock(ID=ID, mainshock_file=mainshock_dict[catalogue_name])\n",
    "local_cat = statseis.create_local_catalogue(mainshock, catalogue_dict[catalogue_name], catalogue_name=catalogue_name)\n",
    "local_cat.loc[local_cat['DAYS_TO_MAINSHOCK']==0]\n",
    "# Mc_cut = False\n",
    "Mc_cut = True\n",
    "if Mc_cut==True:\n",
    "    # local_cat = apply_Mc_cut(local_cat)\n",
    "    local_cat = local_cat.loc[local_cat['MAGNITUDE']>mainshock.Mc].copy()\n",
    "local_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statseis.create_spatial_plot(mainshock=mainshock, local_cat=local_cat, Mc_cut=Mc_cut, catalogue_name=catalogue_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict, file_dict = statseis.identify_foreshocks_short(local_catalogue=local_cat, mainshock=mainshock, earthquake_catalogue=earthquake_catalogue)\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statseis.plot_models(mainshock=mainshock, results_dict=results_dict, file_dict=file_dict, Mc_cut=Mc_cut, catalogue_name=catalogue_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing process mainshocks function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_name = ['QTM_9_5', 'QTM_12', 'SCSN']\n",
    "catalogue_name = catalogue_name[1]\n",
    "ID = [10527789, 15481673]\n",
    "ID = ID[0]\n",
    "earthquake_catalogue = catalogue_dict[catalogue_name].copy()\n",
    "mainshocks_file = mainshock_dict[catalogue_name]\n",
    "mainshocks_file = mainshocks_file.loc[mainshocks_file['ID']==ID]\n",
    "Mc_cut = False\n",
    "Mc_cut = True\n",
    "save = False\n",
    "# RowTuple = namedtuple('RowTuple', mainshock.columns)\n",
    "# mainshock = [RowTuple(*row) for row in mainshock.values][0]\n",
    "results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_name = 'QTM_12'\n",
    "earthquake_catalogue = QTM_12.copy()\n",
    "mainshocks_file = QTM_12_mainshocks.iloc[30:34]\n",
    "Mc_cut = False\n",
    "Mc_cut = True\n",
    "save = False\n",
    "results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Getting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for just my DDET mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "for v in [True, False]:\n",
    "    Mc_cut = v\n",
    "    for catalogue_name, earthquake_catalogue in catalogue_dict.items():\n",
    "        mainshocks_file = mainshocks_in_all_dict[catalogue_name].copy()\n",
    "        mainshocks_file = mainshocks_file.loc[mainshocks_file['ID'].isin(good_mainshocks['ID'])].copy()\n",
    "        results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All catalogues - no Mc cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "Mc_cut = False\n",
    "for catalogue_name, earthquake_catalogue in catalogue_dict.items():\n",
    "    mainshocks_file = mainshocks_in_all_dict[catalogue_name].copy()\n",
    "    mainshocks_file = mainshocks_file.loc[mainshocks_file['Selection']=='Both'].copy()\n",
    "    results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All catalogues - Mc cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "Mc_cut = True\n",
    "for catalogue_name, earthquake_catalogue in catalogue_dict.items():\n",
    "    # mainshocks_file = mainshocks_in_all_dict[catalogue_name].copy()\n",
    "    # mainshocks_file = mainshocks_file.iloc[12:15].copy()\n",
    "    # mainshocks_file = mainshocks_file.loc[mainshocks_file['ID'].isin([10374021, 10321585])].copy()\n",
    "    # mainshocks_file = mainshocks_file.loc[mainshocks_file['Selection']=='Both'].copy()\n",
    "    mainshocks_file = DDET_mainshocks.copy()\n",
    "    results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All mainshocks across all catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "Mc_cut = True\n",
    "for catalogue_name, earthquake_catalogue in catalogue_dict.items():\n",
    "    mainshocks_file = mainshock_dict[catalogue_name].copy()\n",
    "    results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "Mc_cut = False\n",
    "for catalogue_name, earthquake_catalogue in catalogue_dict.items():\n",
    "    mainshocks_file = mainshock_dict[catalogue_name].copy()\n",
    "    results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FET and MDET only mainshocks in all catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "Mc_cut = False\n",
    "for catalogue_name, earthquake_catalogue in catalogue_dict.items():\n",
    "    mainshocks_file = mainshocks_in_all_dict[catalogue_name].copy()\n",
    "    mainshocks_file = mainshocks_file.loc[mainshocks_file['ID'].isin(merged_mainshocks_catalog.loc[merged_mainshocks_catalog['Selection'].isin(['FET', 'MDET']), 'ID'])]\n",
    "    results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save, save_name='FET_MDET_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "Mc_cut = True\n",
    "for catalogue_name, earthquake_catalogue in catalogue_dict.items():\n",
    "    mainshocks_file = mainshocks_in_all_dict[catalogue_name].copy()\n",
    "    mainshocks_file = mainshocks_file.loc[mainshocks_file['ID'].isin(merged_mainshocks_catalog.loc[merged_mainshocks_catalog['Selection'].isin(['FET', 'MDET']), 'ID'])]\n",
    "    results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save, save_name='FET_MDET_only')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FET and MDET only mainshocks in the QTM 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_selection = merged_mainshocks_catalog.loc[merged_mainshocks_catalog['Selection'].isin(['FET', 'MDET'])].copy()\n",
    "print(len(one_selection))\n",
    "one_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc_cut = False\n",
    "catalogue_name = 'SCSN'\n",
    "for mainshock in one_selection.itertuples():\n",
    "    local_cat = statseis.create_local_catalogue(mainshock, catalogue_dict[catalogue_name], catalogue_name=catalogue_name)\n",
    "    if Mc_cut==True:\n",
    "        local_cat = statseis.apply_Mc_cut(local_cat)\n",
    "    local_cat\n",
    "    statseis.plot_local_cat(mainshock=mainshock, local_cat=local_cat, Mc_cut=Mc_cut, catalogue_name=catalogue_name, radius_km=10, box_halfwidth_km=100)\n",
    "    # clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for selection in ['FET', 'MDET']:\n",
    "    mainshock_IDs = list(one_selection.loc[one_selection['Selection']==selection, 'ID'])\n",
    "    filenames_to_copy = [str(x) + '.png' for x in mainshock_IDs]\n",
    "\n",
    "    source_folder = '../outputs/SCSN/data_plots/'\n",
    "\n",
    "    destination_folder = f'../outputs/SCSN/data_plots/{selection}/'\n",
    "    Path(f'../outputs/SCSN/data_plots/{selection}/').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for filename in filenames_to_copy:\n",
    "        source_file = os.path.join(source_folder, filename)\n",
    "        \n",
    "        if os.path.exists(source_file):\n",
    "            destination_file = os.path.join(destination_folder, filename)\n",
    "            \n",
    "            shutil.move(source_file, destination_file)\n",
    "            print(f\"File '{filename}' copied to '{destination_folder}'\")\n",
    "        else:\n",
    "            print(f\"File '{filename}' not found in '{source_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_name = 'QTM_12'\n",
    "earthquake_catalogue = QTM_12.copy()\n",
    "mainshocks_file = one_selection\n",
    "Mc_cut = False\n",
    "# Mc_cut = True\n",
    "save = True\n",
    "results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for selection in ['FET', 'MDET']:\n",
    "    mainshock_IDs = list(one_selection.loc[one_selection['Selection']==selection, 'ID'])\n",
    "    files = [str(x) + '.png' for x in mainshock_IDs]\n",
    "\n",
    "    # source_folder = '../outputs/QTM_12/model_plots/'\n",
    "    source_folder = '../outputs/QTM_12/spatial_plots/'\n",
    "    files = [str(x) + '_10km_365_to_0.png' for x in mainshock_IDs]\n",
    "\n",
    "\n",
    "    # Destination folder to copy files to\n",
    "    destination_folder = f'../outputs/QTM_12/selections/{selection}/'\n",
    "\n",
    "    # List of filenames to check\n",
    "    filenames_to_copy = files\n",
    "\n",
    "    # Iterate over the filenames\n",
    "    for filename in filenames_to_copy:\n",
    "        # Construct the source file path\n",
    "        source_file = os.path.join(source_folder, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if os.path.exists(source_file):\n",
    "            # Construct the destination file path\n",
    "            destination_file = os.path.join(destination_folder, filename)\n",
    "            \n",
    "            # Copy the file to the destination folder\n",
    "            shutil.copyfile(source_file, destination_file)\n",
    "            print(f\"File '{filename}' copied to '{destination_folder}'\")\n",
    "        else:\n",
    "            print(f\"File '{filename}' not found in '{source_folder}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 km 3 day window to match other studies\n",
    "Maybe later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save = True\n",
    "# Mc_cut = False\n",
    "# for catalogue_name, earthquake_catalogue in catalogue_dict.items():\n",
    "#     mainshocks_file = mainshocks_in_all_dict[catalogue_name].copy()\n",
    "#     results = statseis.process_mainshocks(mainshocks_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=Mc_cut, save=save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offshore/bad mainshocks\n",
    "Mainshocks with <275 events within 50 km (cannot robustly estimate Mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_mainshocks = pd.read_csv('../outputs/bad_mainshocks.csv')\n",
    "utils.string_to_datetime_df(bad_mainshocks)\n",
    "\n",
    "catalogue_name = 'QTM_12'\n",
    "earthquake_catalogue = catalogue_dict[catalogue_name].copy()\n",
    "mainshock_file = QTM_12_mainshocks.loc[QTM_12_mainshocks['ID'].isin(bad_mainshocks['ID'])].copy()\n",
    "results = statseis.process_mainshocks(mainshock_file, earthquake_catalogue=earthquake_catalogue, catalogue_name=catalogue_name, Mc_cut=False, save=False)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load in results\n",
    "Restrict to mainshocks 2009 onwards (1 yr post catalog start date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_table(catalogue_name, \n",
    "                         restrict = False,\n",
    "                         event_threshold=0,\n",
    "                         Mc_threshold=4,\n",
    "                         region = [-180, 180, -90, 90],\n",
    "                         magnitude_threshold=4,\n",
    "                         start_date = dt.datetime(1900, 1, 1),\n",
    "                         end_date = dt.datetime(2100, 1, 1),\n",
    "                         depth_threshold=90,\n",
    "                         IDs=[],\n",
    "                         significance_level = 0.01,\n",
    "                         min_observation_period=365,\n",
    "                         Mc_cut=False,\n",
    "                         filepath='None',\n",
    "                         merged_cat_only=True\n",
    "                         ):\n",
    "    \n",
    "    # results_file_name = f\"Mw_{mainshock_Mw_threshold}_iter{iterations}_Mc_cut_{Mc_cutoff}_{search_radius}km_{foreshock_window}day_{modelling_period}day\"\n",
    "    # orignal_results_file = pd.read_csv(f'../data/{catalogue_name}/foreshocks/{results_file_name}.csv') # Old way\n",
    "    if Mc_cut == False:\n",
    "        results_files = glob.glob(f\"../data/{catalogue_name}/foreshocks/default_params_*.csv\")\n",
    "    elif Mc_cut == True:\n",
    "        results_files = glob.glob(f\"../data/{catalogue_name}/Mc_cut/foreshocks/default_params_*.csv\")\n",
    "    newest_file = max(results_files, key=os.path.getctime)\n",
    "    print(newest_file)\n",
    "    # date = 0\n",
    "    # for f in results_files:\n",
    "    #     date = int(f[-10:-4])\n",
    "    #     print(date)\n",
    "    #     if date >= first_date:\n",
    "    #         newest_file = f\n",
    "    # print(newest_file)\n",
    "    orignal_results_file = pd.read_csv(newest_file)\n",
    "    if filepath!='None':\n",
    "        orignal_results_file = pd.read_csv(filepath)\n",
    "    utils.string_to_datetime_df(orignal_results_file)\n",
    "    orignal_results_file = orignal_results_file.loc[orignal_results_file['time_since_catalogue_start']>min_observation_period].copy()\n",
    "    if merged_cat_only == True:\n",
    "        merged_mainshocks_catalog = pd.read_csv('../data/mainshocks/merged_mainshock_catalog.csv')\n",
    "        orignal_results_file = orignal_results_file.loc[orignal_results_file['ID'].isin(merged_mainshocks_catalog['ID'])]\n",
    "    # print(orignal_results_file.columns)\n",
    "    # if 'local_catalogue_length' in orignal_results_file.columns:\n",
    "    #     orignal_results_file.rename(columns={'local_catalogue_length': '1yr_local_catalogue'}, inplace=True)\n",
    "    #     orignal_results_file.to_csv(f'../data/{catalogue_name}/foreshocks/{results_file_name}.csv', index=False)\n",
    "    orignal_results_file.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if len(IDs)!=0:\n",
    "        orignal_results_file = orignal_results_file.loc[orignal_results_file['ID'].isin(IDs)]\n",
    "\n",
    "    if restrict==True:\n",
    "        orignal_results_file = utils.restrict_catalogue_geographically(orignal_results_file, region=region)\n",
    "\n",
    "        orignal_results_file = orignal_results_file.loc[((orignal_results_file['n_regular_seismicity_events'] + orignal_results_file['n_events_in_foreshock_window']) >= event_threshold) &\\\n",
    "                                                        (orignal_results_file['Mc']<= Mc_threshold) &\\\n",
    "                                                        (orignal_results_file['MAGNITUDE']>=magnitude_threshold) &\\\n",
    "                                                        (orignal_results_file['DATETIME']>start_date) &\\\n",
    "                                                        (orignal_results_file['DATETIME']<end_date) &\\\n",
    "                                                        (orignal_results_file['DEPTH']<=depth_threshold)]\n",
    "\n",
    "    results_file = orignal_results_file.copy()\n",
    "    total_mainshocks = len(results_file)\n",
    "    number_of_mainshocks_with_Wetzler_foreshocks = len(results_file[results_file['n_Wetzler_foreshocks']>0])\n",
    "    # number_of_mainshocks_with_ge5_Wetzler_foreshocks = len(results_file[results_file['n_Wetzler_foreshocks']>=5])\n",
    "\n",
    "    all_results_dict = {'Count type':['Mainshocks selected',\n",
    "                                    'Type A'#,\n",
    "                                    # 'Wetzler time window ge5'\n",
    "                                    ],\n",
    "                        f\"{catalogue_name}\":[total_mainshocks,\n",
    "                                        number_of_mainshocks_with_Wetzler_foreshocks#,\n",
    "                                        # number_of_mainshocks_with_ge5_Wetzler_foreshocks\n",
    "                                        ],\n",
    "                        f\"{catalogue_name}_nans\":[float('nan'), float('nan')]\n",
    "                                        }\n",
    "\n",
    "    results_table = []\n",
    "\n",
    "    method_dict = {\n",
    "                \"ESR\":'ESR',\n",
    "                # \"Max_window\":'Max_rate',\n",
    "                #    \"VA_2nd_method\":'ESR',\n",
    "                \"VA_method\":'G-IET',\n",
    "                # \"VA_half_method\":'Random Inter-Event Times',\n",
    "                \"TR_method\":'BP',\n",
    "                }\n",
    "    \n",
    "    for model in method_dict.values():\n",
    "        # print(model)\n",
    "        number_of_mainshocks_with_foreshocks_significance_level = len(results_file[results_file[model] < significance_level])\n",
    "        nans = results_file[model].isna().sum()\n",
    "\n",
    "        all_results_dict['Count type'].append(model)\n",
    "        all_results_dict[f\"{catalogue_name}\"].append(number_of_mainshocks_with_foreshocks_significance_level)\n",
    "        all_results_dict[f\"{catalogue_name}_nans\"].append(nans)\n",
    "        \n",
    "    results_table = pd.DataFrame.from_dict(all_results_dict)\n",
    "    results_table[f\"{catalogue_name}_Perc\"] = 100*results_table[f\"{catalogue_name}\"]/total_mainshocks\n",
    "\n",
    "    # results_table.to_csv(f\"../outputs/{catalogue}/{results_file_name}/results_table.csv\", index=False)\n",
    "    return results_table, orignal_results_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tab, res_file = create_results_table(catalogue_name='QTM_12', restrict=True, IDs=good_mainshocks['ID'])\n",
    "res_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_foreshocks = res_file.loc[res_file['ESR']<0.01].copy()\n",
    "without_foreshocks = res_file.loc[res_file['ESR']>=0.01].copy()\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "ax.add_feature(cfeature.OCEAN, edgecolor='none')\n",
    "gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, zorder=0)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xlabel_style = {'size': 15}\n",
    "gl.ylabel_style = {'size': 15}\n",
    "gl.xlines = False\n",
    "gl.ylines = False\n",
    "\n",
    "ax.scatter(with_foreshocks['LON'], with_foreshocks['LAT'], label='Mainshocks with foreshocks', fc=plot_color_dict['teal'], ec='white', linewidth=0.25, s=200, zorder=103)\n",
    "ax.scatter(without_foreshocks['LON'], without_foreshocks['LAT'], label='Mainshocks without foreshocks', fc=plot_color_dict['orange'], ec='white', linewidth=0.25, s=200, zorder=101)\n",
    "ax.scatter(station_no_dup['LON'], station_no_dup['LAT'],  marker='^', fc=plot_color_dict['pink'], linewidth=0.25, ec='white', #fc='None',\n",
    "            label='STA', zorder=102)\n",
    "extent = utils.get_catalogue_extent(mainshock_file, buffer=0.5)\n",
    "ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "ax.legend()\n",
    "\n",
    "# plt.savefig('../outputs/rough/with_foreshocks_maps.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Mc cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_names =['QTM_9_5', 'QTM_12', 'SCSN']\n",
    "results_tables_list = []\n",
    "Mc_cut_false_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, IDs=good_mainshocks['ID']\n",
    "                                             )\n",
    "    Mc_cut_false_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "Mc_cut_false_subset_results = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "# Mc_cut_false_subset_results.to_csv(f'../outputs/figures/main_results.csv', index=False)\n",
    "Mc_cut_false_subset_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreshock_file_all_cats = pd.merge(pd.merge(Mc_cut_false_foreshock_file_dict['SCSN'], Mc_cut_false_foreshock_file_dict['QTM_12'],on='ID', suffixes=('', '_QTM_12')), \n",
    "                                   Mc_cut_false_foreshock_file_dict['QTM_9_5'], on='ID', suffixes=('', '_QTM_9_5'))\n",
    "\n",
    "foreshock_file_all_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreshock_file_all_cats.loc[(foreshock_file_all_cats['ESR']>=0.01) & (foreshock_file_all_cats['G-IET']>=0.01) &\\\n",
    "                            (foreshock_file_all_cats['ESR_QTM_12']<0.01) & (foreshock_file_all_cats['G-IET_QTM_12']<0.01)][['ID', 'ESR', 'ESR_QTM_12', 'G-IET', 'G-IET_QTM_12', 'Mc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_changes = []\n",
    "for d in ['ESR', 'G-IET', 'BP']:\n",
    "    f = foreshock_file_all_cats.loc[(foreshock_file_all_cats[d]<0.01) & ((foreshock_file_all_cats[f'{d}_QTM_12']>=0.01) | (foreshock_file_all_cats[f'{d}_QTM_9_5']>=0.01))].copy()\n",
    "    print(d, len(f), f['ID'].values)\n",
    "    unexpected_changes = unexpected_changes + list(f['ID'])\n",
    "print(unexpected_changes)\n",
    "unexpected_changes = foreshock_file_all_cats.loc[foreshock_file_all_cats['ID'].isin(unexpected_changes)]\n",
    "unexpected_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_type in ['data_plots', 'model_plots']:\n",
    "    for cat in catalogue_dict.keys():\n",
    "        statseis.move_plots(unexpected_changes, catalog=cat, plot_type=plot_type, out_folder_name=f'unexpected_changes/{cat}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mc cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables_list = []\n",
    "Mc_cut_true_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, Mc_cut=True, IDs=good_mainshocks['ID'])\n",
    "    Mc_cut_true_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "Mc_cut_true_subset_results = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "# Mc_cut_true_subset_results.to_csv(f'../outputs/figures/main_results_Mc_cut.csv', index=False)\n",
    "Mc_cut_true_subset_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreshock_file_all_cats_mcut = pd.merge(pd.merge(Mc_cut_true_foreshock_file_dict['SCSN'], Mc_cut_true_foreshock_file_dict['QTM_12'],on='ID', suffixes=('', '_QTM_12')), \n",
    "                                   Mc_cut_true_foreshock_file_dict['QTM_9_5'], on='ID', suffixes=('', '_QTM_9_5'))\n",
    "\n",
    "foreshock_file_all_cats_mcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreshock_file_all_cats_mcut.loc[(foreshock_file_all_cats_mcut['ESR']>=0.01) & (foreshock_file_all_cats_mcut['G-IET']>=0.01) &\\\n",
    "                            (foreshock_file_all_cats_mcut['ESR_QTM_12']<0.01) & (foreshock_file_all_cats_mcut['G-IET_QTM_12']<0.01)][['ESR', 'ESR_QTM_12']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_changes_mcut = []\n",
    "for d in ['ESR', 'G-IET', 'BP']:\n",
    "    f = foreshock_file_all_cats_mcut.loc[(foreshock_file_all_cats_mcut[d]<0.01) & ((foreshock_file_all_cats_mcut[f'{d}_QTM_12']>=0.01) | (foreshock_file_all_cats_mcut[f'{d}_QTM_9_5']>=0.01))].copy()\n",
    "    print(d, len(f), f['ID'].values)\n",
    "    unexpected_changes_mcut = unexpected_changes_mcut + list(f['ID'])\n",
    "unexpected_changes_mcut = foreshock_file_all_cats_mcut.loc[foreshock_file_all_cats_mcut['ID'].isin(unexpected_changes_mcut)]\n",
    "unexpected_changes_mcut[['ID', 'n_regular_seismicity_events', 'n_regular_seismicity_events_QTM_12', 'n_regular_seismicity_events_QTM_9_5',\n",
    "                         'n_events_in_foreshock_window', 'n_events_in_foreshock_window_QTM_12', 'n_events_in_foreshock_window_QTM_9_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mainshocks_catalog.loc[merged_mainshocks_catalog['ID']==14898996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_changes_mcut.loc[unexpected_changes_mcut['ID']==14898996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_type in ['data_plots', 'Mc_cut/model_plots']:\n",
    "    for cat in catalogue_dict.keys():\n",
    "        statseis.move_plots(unexpected_changes_mcut, catalog=cat, plot_type=plot_type, out_folder_name=f'unexpected_changes_mcut/{cat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat = 'SCSN'\n",
    "files = []\n",
    "for cat in catalogue_dict.keys():\n",
    "    mshock = statseis.iterable_mainshock(ID=14898996, mainshock_file=mainshock_dict[cat])\n",
    "    f = statseis.create_local_catalogue(mainshock=mshock, earthquake_catalogue=catalogue_dict[cat], catalogue_name=cat, radius_km=10)\n",
    "    f_cut = f.loc[f['MAGNITUDE']>mshock.Mc].copy()\n",
    "    files.append(f)\n",
    "    Path(f'../outputs/bad_changes').mkdir(exist_ok=True, parents=True)\n",
    "    statseis.plot_fmd(local_cat=f, save_path=f'../outputs/bad_changes/{mshock.ID}_{cat}.png')\n",
    "    plt.close()\n",
    "    print(cat, mshock.Mc, mshock.Maxc_50, len(f), len(f_cut), len(f.loc[(f['DAYS_TO_MAINSHOCK']>0) & (f['DAYS_TO_MAINSHOCK']<20)]))\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i, local_cat in enumerate(files):\n",
    "    magnitudes = np.array(local_cat['MAGNITUDE'])\n",
    "    bins = np.arange(math.floor(magnitudes.min()), math.ceil(magnitudes.max()), 0.1)\n",
    "    values, base = np.histogram(magnitudes, bins=bins)\n",
    "    cumulative = np.cumsum(values)\n",
    "    Mc, this_fmd, b, b_avg, shibolt_unc = statseis.get_mbs(mag=magnitudes, mbin=0.1)\n",
    "    a, b_value, _a, _b = statseis.b_est(mag=magnitudes, mbin=0.1, mc=Mc)\n",
    "    N = [10**(a-b_value*M) for M in this_fmd]\n",
    "    ratio_above_Mc = round(100*len(magnitudes[magnitudes>Mc])/len(magnitudes))\n",
    "    ax.step(base[:-1], len(magnitudes)-cumulative, color=plot_colors[i])\n",
    "    ax.axvline(x=Mc, linestyle='--', label=r'$M_{c}$: ' + str(round(Mc,1)), color=plot_colors[i])\n",
    "    ax.plot(this_fmd, N, label=f'b: {round(b_value,2)}',  color=plot_colors[i])\n",
    "    # ax.plot([], [], label=f\"ID: {ID}\", marker=None, linestyle='')\n",
    "    ax.plot([], [], label=f'{ratio_above_Mc}% above $M_c$', marker=None, linestyle='')\n",
    "    ax.plot([], [], label=f'N: {len(magnitudes)}', marker=None, linestyle='')\n",
    "# ax.plot([], [], label=f'Radius: {radius}', marker=None, linestyle='')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Magnitude')\n",
    "ax.set_ylabel('N')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "\n",
    "# plt.savefig('../outputs/rough/bad_Mc_in_QTMs_MBS.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i, local_cat in enumerate(files):\n",
    "    magnitudes = np.array(local_cat['MAGNITUDE'])\n",
    "    bins = np.arange(math.floor(magnitudes.min()), math.ceil(magnitudes.max()), 0.1)\n",
    "    values, base = np.histogram(magnitudes, bins=bins)\n",
    "    cumulative = np.cumsum(values)\n",
    "    Mc =  statseis.get_maxc(mag=magnitudes, mbin=0.1)\n",
    "    a, b_value, _a, _b = statseis.b_est(mag=magnitudes, mbin=0.1, mc=Mc)\n",
    "    N = [10**(a-b_value*M) for M in this_fmd]\n",
    "    ratio_above_Mc = round(100*len(magnitudes[magnitudes>Mc])/len(magnitudes))\n",
    "    ax.step(base[:-1], len(magnitudes)-cumulative, color=plot_colors[i])\n",
    "    ax.axvline(x=Mc, linestyle='--', label=r'$M_{c}$: ' + str(round(Mc,1)), color=plot_colors[i])\n",
    "    ax.plot(this_fmd, N, label=f'b: {round(b_value,2)}',  color=plot_colors[i])\n",
    "    # ax.plot([], [], label=f\"ID: {ID}\", marker=None, linestyle='')\n",
    "    ax.plot([], [], label=f'{ratio_above_Mc}% above $M_c$', marker=None, linestyle='')\n",
    "    ax.plot([], [], label=f'N: {len(magnitudes)}', marker=None, linestyle='')\n",
    "# ax.plot([], [], label=f'Radius: {radius}', marker=None, linestyle='')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Magnitude')\n",
    "ax.set_ylabel('N')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "# plt.savefig('../outputs/rough/bad_Mc_in_QTMs_maxc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FET and MDET only mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming runs for FET and MDET only mainshocks so my results function does not plot them instead of DDET mainshocks.\n",
    "\n",
    "# only_dict = {}\n",
    "# for key, item in catalogue_dict.items():\n",
    "#     try:\n",
    "#         f = pd.read_csv(f'../data/{key}/foreshocks/default_params_240805.csv')\n",
    "#     except:\n",
    "#         f = pd.read_csv(f'../data/{key}/foreshocks/FET_and_MDET_240805.csv')\n",
    "#     mainshock_file = mainshock_dict[key].copy()\n",
    "#     f = pd.merge(f, mainshock_file[['ID', 'Selection']], how='inner', on='ID')\n",
    "#     FET_only = f.loc[f['Selection']=='FET'].copy()\n",
    "#     MDET_only = f.loc[f['Selection']=='MDET'].copy()\n",
    "#     FET_only.to_csv(f'../data/{key}/foreshocks/FET_only_240805.csv', index=False)\n",
    "#     MDET_only.to_csv(f'../data/{key}/foreshocks/MDET_only_240805.csv', index=False)\n",
    "#     try:\n",
    "#         os.rename(f'../data/{key}/foreshocks/default_params_240805.csv', f'../data/{key}/foreshocks/FET_and_MDET_240805.csv')\n",
    "#     except:\n",
    "#         'probs renamed already'\n",
    "#     only_dict[key] = f\n",
    "\n",
    "# only_dict = {}\n",
    "# for key, item in catalogue_dict.items():\n",
    "#     try:\n",
    "#         f = pd.read_csv(f'../data/{key}/Mc_cut/foreshocks/default_params_240806.csv')\n",
    "#     except:\n",
    "#         f = pd.read_csv(f'../data/{key}/Mc_cut/foreshocks/FET_and_MDET_240806.csv')\n",
    "#     mainshock_file = mainshock_dict[key].copy()\n",
    "#     f = pd.merge(f, mainshock_file[['ID', 'Selection']], how='inner', on='ID')\n",
    "#     FET_only = f.loc[f['Selection']=='FET'].copy()\n",
    "#     MDET_only = f.loc[f['Selection']=='MDET'].copy()\n",
    "#     FET_only.to_csv(f'../data/{key}/Mc_cut/foreshocks/FET_only_240805.csv', index=False)\n",
    "#     MDET_only.to_csv(f'../data/{key}/Mc_cut/foreshocks/MDET_only_240805.csv', index=False)\n",
    "#     try:\n",
    "#         os.rename(f'../data/{key}/Mc_cut/foreshocks/default_params_240806.csv', f'../data/{key}/Mc_cut/foreshocks/FET_and_MDET_240806.csv')\n",
    "#     except:\n",
    "#         'probs renamed already'\n",
    "#     only_dict[key] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables_list = []\n",
    "Mc_cut_true_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, Mc_cut=False, filepath=f'../data/{catalogue_name}/foreshocks/FET_only_240805.csv', merged_cat_only=False)\n",
    "    Mc_cut_true_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "FET_only_results = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "# Mc_cut_true_subset_results.to_csv(f'../outputs/figures/main_results_Mc_cut.csv', index=False)\n",
    "FET_only_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables_list = []\n",
    "Mc_cut_true_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, Mc_cut=True, filepath=f'../data/{catalogue_name}/Mc_cut/foreshocks/FET_only_240805.csv', merged_cat_only=False)\n",
    "    Mc_cut_true_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "FET_only_results_Mc = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "# Mc_cut_true_subset_results.to_csv(f'../outputs/figures/main_results_Mc_cut.csv', index=False)\n",
    "FET_only_results_Mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables_list = []\n",
    "Mc_cut_true_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, Mc_cut=False, filepath=f'../data/{catalogue_name}/foreshocks/MDET_only_240805.csv')\n",
    "    Mc_cut_true_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "MDET_only_results = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "# Mc_cut_true_subset_results.to_csv(f'../outputs/figures/main_results_Mc_cut.csv', index=False)\n",
    "MDET_only_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables_list = []\n",
    "Mc_cut_true_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, Mc_cut=True, filepath=f'../data/{catalogue_name}/Mc_cut/foreshocks/MDET_only_240805.csv')\n",
    "    Mc_cut_true_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "MDET_only_results_Mc = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "# Mc_cut_true_subset_results.to_csv(f'../outputs/figures/main_results_Mc_cut.csv', index=False)\n",
    "MDET_only_results_Mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offshore vs onshore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_mainshocks = pd.read_csv('../outputs/bad_mainshocks.csv')\n",
    "utils.string_to_datetime_df(bad_mainshocks)\n",
    "bad_mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tab, res_file = create_results_table(catalogue_name='QTM_12', IDs=bad_mainshocks['ID'], filepath='../data/QTM_12/foreshocks/default_params_240801.csv')\n",
    "res_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tab, res_file = create_results_table(catalogue_name='QTM_12', IDs=good_mainshocks['ID'])\n",
    "res_tab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in preferred results (QTM 12 FET & MDET ESR 10 events prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_results = Mc_cut_false_foreshock_file_dict['QTM_12'].copy()\n",
    "QTM_12 = pd.read_csv('../../catalogues/reformatted/QTM_12_reformat.csv')\n",
    "preferred_results = pd.merge(preferred_results, QTM_12[['ID', 'MAGNITUDE']], on='ID', how='left')\n",
    "preferred_results.to_csv('../outputs/QTM_12/preferred_results.csv', index=False)\n",
    "preferred_results\n",
    "\n",
    "# ESR_results_10e = preferred_results.loc[((preferred_results['n_regular_seismicity_events']+preferred_results['n_events_in_foreshock_window'])>=10) &\\\n",
    "#                                   (preferred_results['cut_off_day']==365)].copy()\n",
    "# mainshocks_with_foreshocks = ESR_results_10e.loc[ESR_results_10e['ESR']<0.01].copy()\n",
    "# mainshocks_no_foreshocks = ESR_results_10e.loc[ESR_results_10e['ESR']>=0.01].copy()\n",
    "# ESR_results_10e.to_csv('../outputs/QTM_12/ESR_results_10e.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESR_results_10e = preferred_results.loc[(preferred_results['n_regular_seismicity_events'] + preferred_results['n_events_in_foreshock_window']) >= 10].copy()\n",
    "ESR_results_10e.to_csv('../outputs/QTM_12/ESR_results_10e.csv', index=False)\n",
    "ESR_results_10e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving plots of mainshocks with foreshocks to new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainshock_IDs = list(preferred_results.loc[preferred_results['ESR']<0.01, 'ID'])\n",
    "print(len(mainshock_IDs))\n",
    "filenames_to_copy = [str(x) + '.png' for x in mainshock_IDs]\n",
    "\n",
    "source_folder = '../outputs/QTM_12/data_plots/'\n",
    "\n",
    "destination_folder = '../outputs/QTM_12/data_plots/foreshocks/'\n",
    "\n",
    "for filename in filenames_to_copy:\n",
    "    source_file = os.path.join(source_folder, filename)\n",
    "    \n",
    "    if os.path.exists(source_file):\n",
    "        destination_file = os.path.join(destination_folder, filename)\n",
    "        \n",
    "        shutil.copyfile(source_file, destination_file)\n",
    "        print(f\"File '{filename}' copied to '{destination_folder}'\")\n",
    "    else:\n",
    "        print(f\"File '{filename}' not found in '{source_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Results figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Foreshock rates by mainshock method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_args = dict(boxstyle=\"round,pad=0.2\", fc='white', alpha=0.8)\n",
    "\n",
    "model_dict = {'BP':{'color':plot_color_dict['orange']},\n",
    "              'Type A':{'color':plot_color_dict['yellow']},\n",
    "              'G-IET':{'color':plot_color_dict['teal']},\n",
    "              'ESR':{'color':plot_color_dict['purple']}}\n",
    "\n",
    "foreshock_file_all_cats = pd.merge(Mc_cut_false_foreshock_file_dict['SCSN'],\n",
    "                                    pd.merge(Mc_cut_false_foreshock_file_dict['QTM_12'],\n",
    "                                            Mc_cut_false_foreshock_file_dict['QTM_9_5'],on='ID', suffixes=('_QTM_12', '_QTM_9_5')),\n",
    "                                            on='ID', suffixes=('_SCSN', '_x'))\n",
    "\n",
    "foreshock_file_all_cats_Mc_cut = pd.merge(Mc_cut_true_foreshock_file_dict['SCSN'],\n",
    "                                    pd.merge(Mc_cut_true_foreshock_file_dict['QTM_12'],\n",
    "                                            Mc_cut_true_foreshock_file_dict['QTM_9_5'],on='ID', suffixes=('_QTM_12', '_QTM_9_5')),\n",
    "                                            on='ID', suffixes=('_SCSN', '_x'))\n",
    "\n",
    "ten_event_subset_all_cats = foreshock_file_all_cats_Mc_cut.loc[(foreshock_file_all_cats_Mc_cut['n_regular_seismicity_events']>=10) &\\\n",
    "                                                               (foreshock_file_all_cats_Mc_cut['n_regular_seismicity_events_QTM_12']>=10) &\\\n",
    "                                                                (foreshock_file_all_cats_Mc_cut['n_regular_seismicity_events_QTM_9_5']>=10)].copy()\n",
    "\n",
    "results_options = {'Mc_false':{'DDET':Mc_cut_false_subset_results, 'FET-only':FET_only_results, 'MDET-only':MDET_only_results},\n",
    "                         'Mc_true':{'DDET':Mc_cut_true_subset_results, 'FET-only':FET_only_results_Mc, 'MDET-only':MDET_only_results_Mc}}\n",
    "\n",
    "# results_options = {}\n",
    "# for option, results_dict in {'Mc_false':foreshock_file_all_cats, 'Mc_true':foreshock_file_all_cats_Mc_cut}.items():\n",
    "#     subset_results_tables = {}\n",
    "#     if option=='Mc_false':\n",
    "#         Mc_cut=False\n",
    "#     elif option=='Mc_true':\n",
    "#         Mc_cut=True\n",
    "#     for key, f in subset_results_files.items():\n",
    "#         results_tables_list = []\n",
    "#         for catalogue_name in catalogue_dict.keys():\n",
    "#             res_tab, res_file = create_results_table(catalogue_name=catalogue_name, Mc_cut=Mc_cut, IDs=f['ID'])\n",
    "#             results_tables_list.append(res_tab)\n",
    "#         res_tab_all_cats = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "#         subset_results_tables.update({key:res_tab_all_cats})\n",
    "\n",
    "#     results_options.update({option:subset_results_tables})\n",
    "\n",
    "alphabet = string.ascii_lowercase\n",
    "rows = 1\n",
    "cols = 3\n",
    "n_plots = rows*cols\n",
    "subplot_labels = [letter + ')' for letter in alphabet[:n_plots]]\n",
    "ylims = -2.5, 95\n",
    "alpha=1\n",
    "\n",
    "figure = plt.figure(figsize=(12,6))\n",
    "i = 0\n",
    "for key, Mc_false_table in results_options['Mc_false'].items():\n",
    "    Mc_true_table = results_options['Mc_true'][key].copy()\n",
    "    ax_perc = figure.add_subplot(1,3,i+1)\n",
    "    ax_perc.annotate(f\"N={Mc_false_table.loc[Mc_false_table['Count type']=='Mainshocks selected', 'SCSN'].values[0]} ({key})\", (2,11), fontsize=15, bbox=bbox_args, ha='left', zorder=100)\n",
    "    # ax.annotate('SCSN', EPSG_transformer((-113.9, 37))[0], fontsize=size_dict['textsize']*scale/3, zorder=5, bbox=bbox_args, ha='right')\n",
    "    ax_perc.set_title(subplot_labels[i], loc='left', fontsize=20)\n",
    "    # ax_counts = ax_perc.twinx()\n",
    "    Tables_T = {}\n",
    "    for key, table in {'Mc_false':Mc_false_table, 'Mc_true':Mc_true_table}.items():\n",
    "        table_perc = table.drop([0])\n",
    "        table_perc = table_perc.loc[:, table_perc.columns.str.contains('Perc') | (table_perc.columns.str.contains('type'))].copy()\n",
    "        table_T = table_perc.T.copy()\n",
    "        table_T.columns = table_perc['Count type']\n",
    "        table_T = table_T.iloc[1:,:].copy()\n",
    "        table_T.index = table_T.index.map(lambda x: x.strip('_Perc'))\n",
    "        Tables_T.update({key:table_T})\n",
    "    # raise KeyboardInterrupt        \n",
    "    c=0\n",
    "    for model, params in model_dict.items():\n",
    "        ax_perc.plot(range(0,3), Tables_T['Mc_false'][model], marker='x', label=model, alpha=alpha, color=params['color'])\n",
    "        ax_perc.plot(range(0,3), Tables_T['Mc_true'][model], marker='o', \n",
    "                    #  label=fr\"$M_c$ cut\",\n",
    "                       linestyle='--', color=params['color'])\n",
    "        c+=1\n",
    "    ax_perc.set_xlabel('Catalogue')\n",
    "    ax_perc.set_ylabel('Foreshock Rate (%)')\n",
    "    ax_perc.set_xticks(np.arange(3))\n",
    "    # ax_perc.set_xticklabels(table_T.index)\n",
    "    ax_perc.set_xticklabels([r'QTM$_{9.5}$', r'QTM$_{12}$', 'SCSN'])\n",
    "    ax_perc.yaxis.grid(True, which='major', linestyle='--', color='gray', alpha=0.5, linewidth=0.5)\n",
    "    ax_perc.set_ylim(ylims)\n",
    "    ax_perc.invert_xaxis()\n",
    "    # ax_perc.legend()\n",
    "    if i==0:\n",
    "        ax_perc.legend(loc='upper right')\n",
    "        handles, labels = ax_perc.get_legend_handles_labels()\n",
    "        custom_legend_items = Line2D([0], [0], color='black', marker='o', markersize=6,\n",
    "                        label=r\"M$_c$ cut\", linestyle='--')\n",
    "                        \n",
    "        handles.append(custom_legend_items)\n",
    "        print(handles)\n",
    "        ax_perc.legend(handles=handles, loc='upper left', fontsize=15)\n",
    "    i+=1\n",
    "\n",
    "# custom_legend_items = [Line2D([], [], color=colours[0], marker='o', markersize=10,\n",
    "#                                label=f\"QTM 12 ($r_s$={round(stats_dict['QTM_12_spearmanr'],2)})\", linestyle='None'),\n",
    "#                        Line2D([], [], color=colours[1], marker='o', markersize=10, \n",
    "#                               label=f\"SCSN ($r_s$={round(stats_dict['SCSN_spearmanr'],2)})\", linestyle='None'),\n",
    "#                     #    Line2D([0], [0], color='black', lw=0, marker='', label=f\"Spearmanr: {round(stats_dict['QTM_12_spearmanr'],2)}\"),\n",
    "#                     #    Line2D([0], [0], color='black', lw=0, marker='', label=f\"Spearmanr: {round(stats_dict['SCSN_spearmanr'],2)}\")\n",
    "#                        ]\n",
    "# ax.legend(handles=custom_legend_items, loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3)\n",
    "\n",
    "# plt.savefig(f\"../outputs/figures/foreshock_rate_plot_mainshock_method_v5.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: Foreshock rate estimates for our mainshock catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_names =['QTM_9_5', 'QTM_12', 'SCSN']\n",
    "results_tables_list = []\n",
    "Mc_cut_false_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, IDs=good_mainshocks['ID']\n",
    "                                             )\n",
    "    Mc_cut_false_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "Mc_cut_false_subset_results = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "# Mc_cut_false_subset_results.to_csv(f'../outputs/figures/main_results.csv', index=False)\n",
    "Mc_cut_false_subset_results\n",
    "\n",
    "results_tables_list = []\n",
    "Mc_cut_true_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, Mc_cut=True, IDs=good_mainshocks['ID'])\n",
    "    Mc_cut_true_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "Mc_cut_true_subset_results = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "# Mc_cut_true_subset_results.to_csv(f'../outputs/figures/main_results_Mc_cut.csv', index=False)\n",
    "Mc_cut_true_subset_results\n",
    "\n",
    "bbox_args = dict(boxstyle=\"round,pad=0.2\", fc='white', alpha=0.8)\n",
    "\n",
    "model_dict = {'BP':{'color':plot_color_dict['orange']},\n",
    "              'Type A':{'color':plot_color_dict['yellow']},\n",
    "              'G-IET':{'color':plot_color_dict['teal']},\n",
    "              'ESR':{'color':plot_color_dict['purple']}}\n",
    "\n",
    "foreshock_file_all_cats = pd.merge(Mc_cut_false_foreshock_file_dict['SCSN'],\n",
    "                                    pd.merge(Mc_cut_false_foreshock_file_dict['QTM_12'],\n",
    "                                            Mc_cut_false_foreshock_file_dict['QTM_9_5'],on='ID', suffixes=('_QTM_12', '_QTM_9_5')),\n",
    "                                            on='ID', suffixes=('_SCSN', '_x'))\n",
    "\n",
    "foreshock_file_all_cats_Mc_cut = pd.merge(Mc_cut_true_foreshock_file_dict['SCSN'],\n",
    "                                    pd.merge(Mc_cut_true_foreshock_file_dict['QTM_12'],\n",
    "                                            Mc_cut_true_foreshock_file_dict['QTM_9_5'],on='ID', suffixes=('_QTM_12', '_QTM_9_5')),\n",
    "                                            on='ID', suffixes=('_SCSN', '_x'))\n",
    "\n",
    "ten_event_subset_all_cats = foreshock_file_all_cats_Mc_cut.loc[(foreshock_file_all_cats_Mc_cut['n_regular_seismicity_events']>=10) &\\\n",
    "                                                               (foreshock_file_all_cats_Mc_cut['n_regular_seismicity_events_QTM_12']>=10) &\\\n",
    "                                                                (foreshock_file_all_cats_Mc_cut['n_regular_seismicity_events_QTM_9_5']>=10)].copy()\n",
    "\n",
    "region = [-118.80, -115.40, 32.68, 36.20]\n",
    "red_box_subset_all_cats = utils.restrict_catalogue_geographically(foreshock_file_all_cats, region=region)\n",
    "\n",
    "subset_results_files = {'Unrestricted':foreshock_file_all_cats, \n",
    "                        '>10 events':ten_event_subset_all_cats,\n",
    "                        'Low Mc':red_box_subset_all_cats}\n",
    "\n",
    "results_options = {}\n",
    "for option, results_dict in {'Mc_false':foreshock_file_all_cats, 'Mc_true':foreshock_file_all_cats_Mc_cut}.items():\n",
    "    subset_results_tables = {}\n",
    "    if option=='Mc_false':\n",
    "        Mc_cut=False\n",
    "    elif option=='Mc_true':\n",
    "        Mc_cut=True\n",
    "    for key, f in subset_results_files.items():\n",
    "        results_tables_list = []\n",
    "        for catalogue_name in catalogue_dict.keys():\n",
    "            res_tab, res_file = create_results_table(catalogue_name=catalogue_name, Mc_cut=Mc_cut, IDs=f['ID'])\n",
    "            results_tables_list.append(res_tab)\n",
    "        res_tab_all_cats = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "        subset_results_tables.update({key:res_tab_all_cats})\n",
    "\n",
    "    results_options.update({option:subset_results_tables})\n",
    "\n",
    "alphabet = string.ascii_lowercase\n",
    "rows = 1\n",
    "cols = 3\n",
    "n_plots = rows*cols\n",
    "subplot_labels = [letter + ')' for letter in alphabet[:n_plots]]\n",
    "ylims = 15, 80\n",
    "alpha=1\n",
    "\n",
    "figure = plt.figure(figsize=(12,6))\n",
    "i = 0\n",
    "for key, Mc_false_table in results_options['Mc_false'].items():\n",
    "    Mc_true_table = results_options['Mc_true'][key].copy()\n",
    "    ax_perc = figure.add_subplot(1,3,i+1)\n",
    "    ax_perc.annotate(f\"N={Mc_false_table.loc[Mc_false_table['Count type']=='Mainshocks selected', 'SCSN'].values[0]} ({key})\", (0,17), fontsize=15, bbox=bbox_args, ha='right', zorder=100)\n",
    "    # ax.annotate('SCSN', EPSG_transformer((-113.9, 37))[0], fontsize=size_dict['textsize']*scale/3, zorder=5, bbox=bbox_args, ha='right')\n",
    "    ax_perc.set_title(subplot_labels[i], loc='left', fontsize=20)\n",
    "    # ax_counts = ax_perc.twinx()\n",
    "    Tables_T = {}\n",
    "    for Mc_key, table in {'Mc_false':Mc_false_table, 'Mc_true':Mc_true_table}.items():\n",
    "        table_perc = table.drop([0])\n",
    "        table_perc = table_perc.loc[:, table_perc.columns.str.contains('Perc') | (table_perc.columns.str.contains('type'))].copy()\n",
    "        table_T = table_perc.T.copy()\n",
    "        table_T.columns = table_perc['Count type']\n",
    "        table_T = table_T.iloc[1:,:].copy()\n",
    "        table_T.index = table_T.index.map(lambda x: x.strip('_Perc'))\n",
    "        Tables_T.update({Mc_key:table_T})\n",
    "    # raise KeyboardInterrupt        \n",
    "    c=0\n",
    "    for model, params in model_dict.items():\n",
    "        # if (model == 'G-IET') & (key=='>10 events'):\n",
    "        #     linestyle = '-'\n",
    "        #     marker='s'\n",
    "        # else:\n",
    "        #     linestyle='--'\n",
    "        #     marker='o'\n",
    "        linestyle='--'\n",
    "        marker='o'\n",
    "        ax_perc.plot(range(0,3), Tables_T['Mc_false'][model], marker='x', label=model, alpha=alpha, color=params['color'])\n",
    "        ax_perc.plot(range(0,3), Tables_T['Mc_true'][model], marker=marker, \n",
    "                    #  label=fr\"$M_c$ cut\",\n",
    "                       linestyle=linestyle, color=params['color'])\n",
    "        c+=1\n",
    "    ax_perc.set_xlabel('Catalogue')\n",
    "    ax_perc.set_ylabel('Foreshock Rate (%)')\n",
    "    ax_perc.set_xticks(np.arange(3))\n",
    "    # ax_perc.set_xticklabels(table_T.index)\n",
    "    ax_perc.set_xticklabels([r'QTM$_{9.5}$', r'QTM$_{12}$', 'SCSN'])\n",
    "    ax_perc.yaxis.grid(True, which='major', linestyle='--', color='gray', alpha=0.5, linewidth=0.5)\n",
    "    ax_perc.set_ylim(ylims)\n",
    "    ax_perc.invert_xaxis()\n",
    "    # ax_perc.legend()\n",
    "    if i==0:\n",
    "        ax_perc.legend(loc='upper right')\n",
    "        handles, labels = ax_perc.get_legend_handles_labels()\n",
    "        custom_legend_items = Line2D([0], [0], color='black', marker='o', markersize=6,\n",
    "                        label=r\"M$_c$ cut\", linestyle='--')\n",
    "                        \n",
    "        handles.append(custom_legend_items)\n",
    "        print(handles)\n",
    "        ax_perc.legend(handles=handles, loc='upper left', fontsize=15)\n",
    "    i+=1\n",
    "\n",
    "# custom_legend_items = [Line2D([], [], color=colours[0], marker='o', markersize=10,\n",
    "#                                label=f\"QTM 12 ($r_s$={round(stats_dict['QTM_12_spearmanr'],2)})\", linestyle='None'),\n",
    "#                        Line2D([], [], color=colours[1], marker='o', markersize=10, \n",
    "#                               label=f\"SCSN ($r_s$={round(stats_dict['SCSN_spearmanr'],2)})\", linestyle='None'),\n",
    "#                     #    Line2D([0], [0], color='black', lw=0, marker='', label=f\"Spearmanr: {round(stats_dict['QTM_12_spearmanr'],2)}\"),\n",
    "#                     #    Line2D([0], [0], color='black', lw=0, marker='', label=f\"Spearmanr: {round(stats_dict['SCSN_spearmanr'],2)}\")\n",
    "#                        ]\n",
    "# ax.legend(handles=custom_legend_items, loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3)\n",
    "\n",
    "plt.savefig(f\"../outputs/figures/final_foreshock_rate_plot_v5.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_options['Mc_true']['Unrestricted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_options['Mc_true']['Low Mc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_options['Mc_true']['>10 events']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: 99 CI plot for mainshock subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'BP':{'color':plot_color_dict['orange']},\n",
    "              'Type A':{'color':'black'},\n",
    "              'G-IET':{'color':plot_color_dict['teal']},\n",
    "              'ESR':{'color':plot_color_dict['purple']}}\n",
    "\n",
    "params = {'alpha':1, 'markersize':75, 'linewidth':3}\n",
    "\n",
    "# preferred_results = Mc_cut_false_foreshock_file_dict['QTM_12'].copy()\n",
    "preferred_results = Mc_cut_true_foreshock_file_dict['QTM_12'].copy()\n",
    "# preferred_results = preferred_results.loc[~preferred_results['n_regular_seismicity_events'].isnull()].copy()\n",
    "probability_results = preferred_results[['ID', 'n_regular_seismicity_events', 'n_events_in_foreshock_window', 'n_Wetzler_foreshocks', \n",
    "                                         'ESR', 'G-IET', 'BP','ESR_99CI', 'G-IET_99CI', 'BP_99CI']].copy()\n",
    "\n",
    "mins = []\n",
    "maxs = []\n",
    "for probs in zip(#probability_results['N_obs']),probability_results['Nbinom_99CI']\n",
    "                 probability_results['ESR_99CI'],\n",
    "                 probability_results['G-IET_99CI'],\n",
    "                 probability_results['BP_99CI']\n",
    "                 ):\n",
    "    mins.append(min(probs))\n",
    "    maxs.append(max(probs))\n",
    "                \n",
    "probability_results['mins'] = mins\n",
    "probability_results['maxs'] = maxs\n",
    "probability_results['all_agree'] = 0\n",
    "# probability_results['all_agree'].loc[probability_results['N_obs'] > probability_results['maxs']] = 1\n",
    "probability_results.loc[probability_results['n_events_in_foreshock_window'] > probability_results['maxs'], 'all_agree'] = 1\n",
    "probability_results.loc[probability_results['n_events_in_foreshock_window'] <= probability_results['mins'], 'all_agree'] = -1\n",
    "print(len(probability_results.loc[probability_results['all_agree']=='Y']))\n",
    "\n",
    "x_axis = range(1,len(probability_results)+1)\n",
    "# sorted_results = results_file.sort_values(by='event_count_prob')\n",
    "probability_results.sort_values(by='maxs', inplace=True)\n",
    "sorted_results = probability_results.copy()\n",
    "sorted_results.reset_index(inplace=True)\n",
    "\n",
    "figure = plt.figure(figsize=(8,6))\n",
    "plt.yscale(\"symlog\")  \n",
    "plt.plot((x_axis,x_axis),([i for i in sorted_results['mins']], [j for j in sorted_results['maxs']]),c='black',\n",
    "         zorder=0, alpha=0.3, linewidth=2)\n",
    "# plt.scatter(x_axis, sorted_results['N_obs'], alpha=0.9, label=r'$N_{obs}$', marker='*', c=probability_results['all_agree'])\n",
    "plt.scatter(sorted_results.loc[sorted_results['all_agree']==1].index+1, sorted_results.loc[sorted_results['all_agree']==1, 'n_events_in_foreshock_window'],\n",
    "             alpha=0.9, label=r'$N_{obs}$' + ' > all thresholds', marker='*', color=plot_color_dict['pink'], s=params['markersize'])\n",
    "plt.scatter(sorted_results.loc[sorted_results['all_agree']==0].index+1, sorted_results.loc[sorted_results['all_agree']==0, 'n_events_in_foreshock_window'],\n",
    "             alpha=0.9, label=r'$N_{obs}$' + ' not > or <= all thresholds', marker='*', color=plot_color_dict['green'], s=params['markersize'])\n",
    "plt.scatter(sorted_results.loc[sorted_results['all_agree']==-1].index+1, sorted_results.loc[sorted_results['all_agree']==-1, 'n_events_in_foreshock_window'],\n",
    "             alpha=0.9, label=r'$N_{obs}$' + '<= all thresholds', marker='*', color=plot_color_dict['brown'], s=params['markersize'])\n",
    "plt.scatter(x_axis, sorted_results['BP_99CI'], alpha=params['alpha'], label='BP', marker='_', color=model_dict['BP']['color'], s=params['markersize'])\n",
    "plt.scatter(x_axis, sorted_results['G-IET_99CI'], alpha=params['alpha'], label='G-IET', marker='_', color=model_dict['G-IET']['color'], s=params['markersize'])\n",
    "plt.scatter(x_axis, sorted_results['ESR_99CI'], alpha=params['alpha'], label='ESR', marker='_', color=model_dict['ESR']['color'], s=params['markersize'])\n",
    "plt.grid(True, which='major', linestyle='--', color='gray', alpha=0.5, linewidth=0.5, axis='y')\n",
    "\n",
    "Mw3_foreshocks = sorted_results.loc[sorted_results['n_Wetzler_foreshocks']>0].copy()\n",
    "# plt.scatter(Mw3_foreshocks.index+1, Mw3_foreshocks['n_events_in_foreshock_window'], alpha=1, label='Type A', marker='.', color=model_dict['Type A']['color'], s=params['markersize'])\n",
    "plt.scatter(Mw3_foreshocks.index+1, Mw3_foreshocks['n_events_in_foreshock_window'], alpha=1, label='Type A', marker='*', fc='None', ec=model_dict['Type A']['color'], s=params['markersize']+75)\n",
    "\n",
    "# plt.scatter(x_axis, sorted_results['Nbinom_99CI'], alpha=0.9, label='Negative Binomial', marker='_', color='#1f77b4')\n",
    "plt.xlabel(fr'Mainshock index')\n",
    "plt.ylabel('N earthquakes')\n",
    "# plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.legend(fontsize=15, framealpha=0, loc='upper left', bbox_to_anchor=(-0.03,1.025))\n",
    "# plt.savefig('../outputs/figures/mainshock_subset_99CI_plot_QTM_12_no_mcut.png')\n",
    "# plt.savefig('../outputs/figures/mainshock_subset_99CI_plot_QTM_12_mcut.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantifying thresholds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_results.loc[~(preferred_results['G-IET_99CI']>preferred_results['ESR_99CI']) & ~(preferred_results['ESR_99CI']>preferred_results['G-IET_99CI'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preferred_results.loc[preferred_results['ESR_99CI']>preferred_results['G-IET_99CI']]),\n",
    "      len(preferred_results.loc[~(preferred_results['G-IET_99CI']>preferred_results['ESR_99CI']) & ~(preferred_results['ESR_99CI']>preferred_results['G-IET_99CI'])]),\n",
    "      len(preferred_results.loc[preferred_results['G-IET_99CI']>preferred_results['ESR_99CI']])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preferred_results.loc[preferred_results['ESR_99CI']>preferred_results['G-IET_99CI']])/len(preferred_results),\n",
    "      len(preferred_results.loc[~(preferred_results['G-IET_99CI']>preferred_results['ESR_99CI']) & ~(preferred_results['ESR_99CI']>preferred_results['G-IET_99CI'])])/len(preferred_results),\n",
    "      len(preferred_results.loc[preferred_results['G-IET_99CI']>preferred_results['ESR_99CI']])/len(preferred_results),\n",
    "      len(preferred_results.loc[preferred_results['G-IET_99CI']==preferred_results['ESR_99CI']])/len(preferred_results)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Mc_cut_false_foreshock_file_dict['QTM_12'].copy()\n",
    "x.loc[x['ID']==37506472]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Mc_cut_true_foreshock_file_dict['QTM_12'].copy()\n",
    "x.loc[x['ID']==37506472]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_results.loc[~(preferred_results['G-IET_99CI']>preferred_results['ESR_99CI']) & ~(preferred_results['ESR_99CI']>preferred_results['G-IET_99CI'])][['ID', 'Mc', 'n_regular_seismicity_events', 'n_events_in_foreshock_window', 'n_Wetzler_foreshocks', \n",
    "                                         'ESR', 'G-IET', 'BP','ESR_99CI', 'G-IET_99CI', 'BP_99CI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preferred_results.loc[preferred_results['G-IET_99CI']>preferred_results['BP_99CI']])/len(preferred_results),\n",
    "      len(preferred_results.loc[preferred_results['BP_99CI']>preferred_results['G-IET_99CI']])/len(preferred_results),\n",
    "      len(preferred_results.loc[preferred_results['BP_99CI'].isna()])/len(preferred_results),\n",
    "      len(preferred_results.loc[preferred_results['G-IET_99CI'].isna()])/len(preferred_results)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(probability_results.loc[probability_results['n_Wetzler_foreshocks']>0]),\n",
    "      len(probability_results.loc[probability_results['all_agree']==1]),\n",
    "      len(probability_results.loc[(probability_results['G-IET']<0.01) & (probability_results['ESR']<0.01)]),\n",
    "      len(probability_results.loc[(probability_results['G-IET']<0.01) & (probability_results['ESR']>=0.01)]),\n",
    "      len(probability_results.loc[(probability_results['G-IET']>=0.01) & (probability_results['ESR']<0.01)])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_results.loc[(probability_results['G-IET']<0.01) & (probability_results['ESR']>=0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_results.loc[(probability_results['G-IET']>=0.01) & (probability_results['ESR']<0.01)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: SCSN 99CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'BP':{'color':plot_color_dict['orange']},\n",
    "              'Type A':{'color':'black'},\n",
    "              'G-IET':{'color':plot_color_dict['teal']},\n",
    "              'ESR':{'color':plot_color_dict['purple']}}\n",
    "\n",
    "params = {'alpha':1, 'markersize':75}\n",
    "\n",
    "preferred_results = Mc_cut_false_foreshock_file_dict['SCSN'].copy()\n",
    "# preferred_results = preferred_results.loc[~preferred_results['n_regular_seismicity_events'].isnull()].copy()\n",
    "probability_results = preferred_results[['ID', 'n_regular_seismicity_events', 'n_events_in_foreshock_window', 'n_Wetzler_foreshocks', \n",
    "                                         'ESR', 'G-IET', 'BP','ESR_99CI', 'G-IET_99CI', 'BP_99CI',]].copy()\n",
    "\n",
    "mins = []\n",
    "maxs = []\n",
    "for probs in zip(#probability_results['N_obs']),probability_results['Nbinom_99CI']\n",
    "                 probability_results['ESR_99CI'],\n",
    "                 probability_results['G-IET_99CI'],\n",
    "                 probability_results['BP_99CI']\n",
    "                 ):\n",
    "    mins.append(min(probs))\n",
    "    maxs.append(max(probs))\n",
    "                \n",
    "probability_results['mins'] = mins\n",
    "probability_results['maxs'] = maxs\n",
    "probability_results['all_agree'] = 0\n",
    "# probability_results['all_agree'].loc[probability_results['N_obs'] > probability_results['maxs']] = 1\n",
    "probability_results.loc[probability_results['n_events_in_foreshock_window'] > probability_results['maxs'], 'all_agree'] = 1\n",
    "probability_results.loc[probability_results['n_events_in_foreshock_window'] <= probability_results['mins'], 'all_agree'] = -1\n",
    "print(len(probability_results.loc[probability_results['all_agree']=='Y']))\n",
    "\n",
    "x_axis = range(1,len(probability_results)+1)\n",
    "# sorted_results = results_file.sort_values(by='event_count_prob')\n",
    "probability_results.sort_values(by='maxs', inplace=True)\n",
    "sorted_results = probability_results.copy()\n",
    "sorted_results.reset_index(inplace=True)\n",
    "\n",
    "figure = plt.figure(figsize=(8,6))\n",
    "plt.yscale(\"symlog\")  \n",
    "plt.plot((x_axis,x_axis),([i for i in sorted_results['mins']], [j for j in sorted_results['maxs']]),c='black',\n",
    "         zorder=0, alpha=0.3, linewidth=2)\n",
    "# plt.scatter(x_axis, sorted_results['N_obs'], alpha=0.9, label=r'$N_{obs}$', marker='*', c=probability_results['all_agree'])\n",
    "plt.scatter(sorted_results.loc[sorted_results['all_agree']==1].index+1, sorted_results.loc[sorted_results['all_agree']==1, 'n_events_in_foreshock_window'],\n",
    "             alpha=0.9, label=r'$N_{obs}$' + ' > all models', marker='*', color=plot_color_dict['pink'], s=params['markersize'])\n",
    "plt.scatter(sorted_results.loc[sorted_results['all_agree']==0].index+1, sorted_results.loc[sorted_results['all_agree']==0, 'n_events_in_foreshock_window'],\n",
    "             alpha=0.9, label=r'$N_{obs}$' + ' not > or <= all models', marker='*', color=plot_color_dict['green'], s=params['markersize'])\n",
    "plt.scatter(sorted_results.loc[sorted_results['all_agree']==-1].index+1, sorted_results.loc[sorted_results['all_agree']==-1, 'n_events_in_foreshock_window'],\n",
    "             alpha=0.9, label=r'$N_{obs}$' + '<= all models', marker='*', color=plot_color_dict['brown'], s=params['markersize'])\n",
    "plt.scatter(x_axis, sorted_results['BP_99CI'], alpha=params['alpha'], label='BP', marker='_', color=model_dict['BP']['color'], s=params['markersize'])\n",
    "plt.scatter(x_axis, sorted_results['G-IET_99CI'], alpha=params['alpha'], label='G-IET', marker='_', color=model_dict['G-IET']['color'], s=params['markersize'])\n",
    "plt.scatter(x_axis, sorted_results['ESR_99CI'], alpha=params['alpha'], label='ESR', marker='_', color=model_dict['ESR']['color'], s=params['markersize'])\n",
    "plt.grid(True, which='major', linestyle='--', color='gray', alpha=0.5, linewidth=0.5, axis='y')\n",
    "\n",
    "Mw3_foreshocks = sorted_results.loc[sorted_results['n_Wetzler_foreshocks']>0].copy()\n",
    "# plt.scatter(Mw3_foreshocks.index+1, Mw3_foreshocks['n_events_in_foreshock_window'], alpha=1, label='Type A', marker='.', color=model_dict['Type A']['color'], s=params['markersize'])\n",
    "plt.scatter(Mw3_foreshocks.index+1, Mw3_foreshocks['n_events_in_foreshock_window'], alpha=1, label='Type A', marker='*', fc='None', ec=model_dict['Type A']['color'], s=params['markersize']+75)\n",
    "\n",
    "# plt.scatter(x_axis, sorted_results['Nbinom_99CI'], alpha=0.9, label='Negative Binomial', marker='_', color='#1f77b4')\n",
    "plt.xlabel(fr'Mainshocks (ordered by model 99th percentiles)')\n",
    "plt.ylabel('N earthquakes')\n",
    "# plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.legend(fontsize=15, framealpha=0, loc='upper left', bbox_to_anchor=(-0.03,1.025))\n",
    "# plt.savefig('../outputs/figures/mainshock_subset_99CI_plot_SCSN_v3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: SCSN to QTM for ESR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No mcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCSN_results = Mc_cut_false_foreshock_file_dict['SCSN'].copy()\n",
    "# preferred_results = preferred_results.loc[~preferred_results['n_regular_seismicity_events'].isnull()].copy()\n",
    "SCSN_results = SCSN_results[['ID', 'LON', 'LAT', 'DATETIME', 'MAGNITUDE', 'n_regular_seismicity_events', 'n_events_in_foreshock_window', 'n_Wetzler_foreshocks', \n",
    "                                         'ESR', 'G-IET', 'BP','ESR_99CI', 'G-IET_99CI', 'BP_99CI',]].copy()\n",
    "\n",
    "QTM_12_results = Mc_cut_false_foreshock_file_dict['QTM_12'].copy()\n",
    "# preferred_results = preferred_results.loc[~preferred_results['n_regular_seismicity_events'].isnull()].copy()\n",
    "QTM_12_results = QTM_12_results[['ID', 'LON', 'LAT', 'DATETIME', 'MAGNITUDE', 'n_regular_seismicity_events', 'n_events_in_foreshock_window', 'n_Wetzler_foreshocks', \n",
    "                                         'ESR', 'G-IET', 'BP','ESR_99CI', 'G-IET_99CI', 'BP_99CI',]].copy()\n",
    "\n",
    "merged_results = pd.merge(SCSN_results, QTM_12_results, on='ID', how='inner', suffixes=('_SCSN', '_QTM_12'))\n",
    "\n",
    "ordered_merged_results = merged_results.sort_values(by=['ESR_SCSN', 'ESR_QTM_12'], ascending=False)\n",
    "ordered_merged_results.reset_index(inplace=True)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.scatter(merged_results['ESR_SCSN'], merged_results['ESR_QTM_12'], alpha=0.9, ec='white')\n",
    "# ax.axvline(0.01, linestyle='-')\n",
    "# ax.axhline(0.01, linestyle='-')\n",
    "ax.axhline(0.01, linestyle='--', color=plot_color_dict['purple'], alpha=0.9)\n",
    "for i in ordered_merged_results.index:\n",
    "    SCSN_point = ordered_merged_results['ESR_SCSN'][i]\n",
    "    QTM_12_point = ordered_merged_results['ESR_QTM_12'][i]\n",
    "    if SCSN_point < QTM_12_point:\n",
    "        ax.plot([i, i], [ordered_merged_results['ESR_SCSN'][i], ordered_merged_results['ESR_QTM_12'][i]], color=plot_color_dict['grey'], linestyle='-', alpha=0.3, zorder=0)\n",
    "    elif QTM_12_point < SCSN_point:\n",
    "        ax.plot([i, i], [ordered_merged_results['ESR_SCSN'][i], ordered_merged_results['ESR_QTM_12'][i]], color=plot_color_dict['pink'], linestyle='-', alpha=0.3,zorder=0)\n",
    "ax.scatter(ordered_merged_results.index, ordered_merged_results['ESR_SCSN'], alpha=0.9, ec='white', fc=plot_color_dict['teal'], label='SCSN')\n",
    "ax.scatter(ordered_merged_results.index, ordered_merged_results['ESR_QTM_12'], alpha=0.9, ec='white', fc=plot_color_dict['orange'], label='QTM 12')\n",
    "# ax.axvline(0.01, linestyle='--')\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_yscale('log')\n",
    "ax.set_xlabel(r'ID')\n",
    "ax.set_ylabel(r'p-value')\n",
    "ax.legend(fontsize=15)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../outputs/figures/p_values_SCSN_to_QTM_12_no_mcut.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_foreshocks = ordered_merged_results.loc[ordered_merged_results['ESR_QTM_12']<0.01].copy()\n",
    "without_foreshocks = ordered_merged_results.loc[ordered_merged_results['ESR_QTM_12']>=0.01].copy()\n",
    "dynamic_duo = ordered_merged_results.loc[(ordered_merged_results['ESR_QTM_12']<0.01) & (ordered_merged_results['ESR_SCSN']>=0.01)].copy()\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "gs = fig.add_gridspec(3,2)\n",
    "\n",
    "ax = fig.add_subplot(gs[0:1,:])\n",
    "ax.set_title('a)', loc='left', fontsize=20)\n",
    "\n",
    "ax.scatter(with_foreshocks['DATETIME_QTM_12'], with_foreshocks['MAGNITUDE_QTM_12'], label='foreshocks', fc=plot_color_dict['teal'], ec='white', linewidth=0.25, s=200, zorder=103, alpha=0.7)\n",
    "ax.scatter(dynamic_duo['DATETIME_QTM_12'], dynamic_duo['MAGNITUDE_QTM_12'], label=f'$\\delta$ SCSN to QTM', fc=plot_color_dict['teal'], ec=plot_color_dict['pink'], linewidth=1, s=200, zorder=104)\n",
    "ax.scatter(without_foreshocks['DATETIME_QTM_12'], without_foreshocks['MAGNITUDE_QTM_12'], label='no foreshocks', fc=plot_color_dict['orange'], ec='white', linewidth=0.25, s=200, zorder=101, alpha=0.7)\n",
    "\n",
    "ax = fig.add_subplot(gs[1:,:], projection=ccrs.PlateCarree())\n",
    "ax.set_title('b)', loc='left', fontsize=20)\n",
    "\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "ax.add_feature(cfeature.OCEAN, edgecolor='none')\n",
    "gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, zorder=0)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xlabel_style = {'size': 20}\n",
    "gl.ylabel_style = {'size': 20}\n",
    "gl.xlines = False\n",
    "gl.ylines = False\n",
    "\n",
    "ax.scatter(with_foreshocks['LON_QTM_12'], with_foreshocks['LAT_QTM_12'], label='Mainshocks with foreshocks', fc=plot_color_dict['teal'], ec='white', linewidth=0.25, s=200, zorder=103, alpha=0.7)\n",
    "ax.scatter(dynamic_duo['LON_QTM_12'], dynamic_duo['LAT_QTM_12'], label=f'$\\delta$ SCSN to QTM', fc=plot_color_dict['teal'], ec=plot_color_dict['pink'], linewidth=2, s=200, zorder=104)\n",
    "ax.scatter(without_foreshocks['LON_QTM_12'], without_foreshocks['LAT_QTM_12'], label='Mainshocks w/o foreshocks', fc=plot_color_dict['orange'], ec='white', linewidth=0.25, s=200, zorder=101, alpha=0.7)\n",
    "ax.scatter(station_no_dup['LON'], station_no_dup['LAT'],  marker='^', fc='black', linewidth=0.25, ec='white', alpha=0.7,#fc='None',\n",
    "            label='STA', zorder=102)\n",
    "extent = utils.get_catalogue_extent(mainshock_file, buffer=0.5)\n",
    "ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "# plt.savefig('../outputs/figures/SCSN_QTM_foreshocks_map_no_mcut.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_mcut_flipped_mainshocks = ordered_merged_results.loc[(ordered_merged_results['ESR_QTM_12']<0.01) & (ordered_merged_results['ESR_SCSN']>=0.01)].copy()\n",
    "no_mcut_flipped_mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in ['SCSN', 'QTM_12']:\n",
    "    for mainshock in no_mcut_flipped_mainshocks.itertuples():\n",
    "        \n",
    "        filename = f'{mainshock.ID}.png'\n",
    "\n",
    "        source_folder = f'../outputs/{cat}/model_plots/'\n",
    "\n",
    "        destination_folder = f'../outputs/no_mcut_flipped_mshocks/'\n",
    "        Path(destination_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        source_file = os.path.join(source_folder, filename)\n",
    "        \n",
    "        if os.path.exists(source_file):\n",
    "            filename = f'{mainshock.ID}_{cat}.png'\n",
    "            destination_file = os.path.join(destination_folder, filename)\n",
    "            \n",
    "            shutil.copyfile(source_file, destination_file)\n",
    "            print(f\"File '{filename}' copied to '{destination_folder}'\")\n",
    "        else:\n",
    "            print(f\"File '{filename}' not found in '{source_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(merged_results['ESR_SCSN'] - merged_results['ESR_QTM_12'])\n",
    "print(len(merged_results.loc[merged_results['ESR_QTM_12']<merged_results['ESR_SCSN']]),\n",
    "      len(merged_results.loc[merged_results['ESR_QTM_12']==merged_results['ESR_SCSN']]),\n",
    "      len(merged_results.loc[merged_results['ESR_QTM_12']>merged_results['ESR_SCSN']])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCSN_results = Mc_cut_false_foreshock_file_dict['SCSN'].copy()\n",
    "SCSN_results = Mc_cut_true_foreshock_file_dict['SCSN'].copy()\n",
    "# preferred_results = preferred_results.loc[~preferred_results['n_regular_seismicity_events'].isnull()].copy()\n",
    "SCSN_results = SCSN_results[['ID', 'LON', 'LAT', 'DATETIME', 'MAGNITUDE', 'n_regular_seismicity_events', 'n_events_in_foreshock_window', 'n_Wetzler_foreshocks', \n",
    "                                         'ESR', 'G-IET', 'BP','ESR_99CI', 'G-IET_99CI', 'BP_99CI',]].copy()\n",
    "\n",
    "# QTM_12_results = Mc_cut_false_foreshock_file_dict['QTM_12'].copy()\n",
    "QTM_12_results = Mc_cut_true_foreshock_file_dict['QTM_12'].copy()\n",
    "# preferred_results = preferred_results.loc[~preferred_results['n_regular_seismicity_events'].isnull()].copy()\n",
    "QTM_12_results = QTM_12_results[['ID', 'LON', 'LAT', 'DATETIME', 'MAGNITUDE', 'n_regular_seismicity_events', 'n_events_in_foreshock_window', 'n_Wetzler_foreshocks', \n",
    "                                         'ESR', 'G-IET', 'BP','ESR_99CI', 'G-IET_99CI', 'BP_99CI',]].copy()\n",
    "\n",
    "merged_results = pd.merge(SCSN_results, QTM_12_results, on='ID', how='inner', suffixes=('_SCSN', '_QTM_12'))\n",
    "\n",
    "ordered_merged_results = merged_results.sort_values(by=['ESR_SCSN', 'ESR_QTM_12'], ascending=False)\n",
    "ordered_merged_results.reset_index(inplace=True)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.scatter(merged_results['ESR_SCSN'], merged_results['ESR_QTM_12'], alpha=0.9, ec='white')\n",
    "# ax.axvline(0.01, linestyle='-')\n",
    "# ax.axhline(0.01, linestyle='-')\n",
    "ax.axhline(0.01, linestyle='--', color=plot_color_dict['purple'], alpha=0.9)\n",
    "for i in ordered_merged_results.index:\n",
    "    SCSN_point = ordered_merged_results['ESR_SCSN'][i]\n",
    "    QTM_12_point = ordered_merged_results['ESR_QTM_12'][i]\n",
    "    if SCSN_point < QTM_12_point:\n",
    "        ax.plot([i, i], [ordered_merged_results['ESR_SCSN'][i], ordered_merged_results['ESR_QTM_12'][i]], color=plot_color_dict['grey'], linestyle='-', alpha=0.3, zorder=0)\n",
    "    elif QTM_12_point < SCSN_point:\n",
    "        ax.plot([i, i], [ordered_merged_results['ESR_SCSN'][i], ordered_merged_results['ESR_QTM_12'][i]], color=plot_color_dict['pink'], linestyle='-', alpha=0.3,zorder=0)\n",
    "ax.scatter(ordered_merged_results.index, ordered_merged_results['ESR_SCSN'], alpha=0.9, ec='white', fc=plot_color_dict['teal'], label='SCSN')\n",
    "ax.scatter(ordered_merged_results.index, ordered_merged_results['ESR_QTM_12'], alpha=0.9, ec='white', fc=plot_color_dict['orange'], label='QTM 12')\n",
    "# ax.axvline(0.01, linestyle='--')\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_yscale('log')\n",
    "ax.set_xlabel(r'ID')\n",
    "ax.set_ylabel(r'p-value')\n",
    "ax.legend(fontsize=15)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../outputs/figures/p_values_SCSN_to_QTM_12_mcut_v2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_foreshocks = ordered_merged_results.loc[ordered_merged_results['ESR_QTM_12']<0.01].copy()\n",
    "without_foreshocks = ordered_merged_results.loc[ordered_merged_results['ESR_QTM_12']>=0.01].copy()\n",
    "dynamic_duo = ordered_merged_results.loc[(ordered_merged_results['ESR_QTM_12']<0.01) & (ordered_merged_results['ESR_SCSN']>=0.01)].copy()\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "gs = fig.add_gridspec(3,2)\n",
    "\n",
    "ax = fig.add_subplot(gs[0:1,:])\n",
    "ax.set_title('a)', loc='left', fontsize=20)\n",
    "\n",
    "ax.scatter(with_foreshocks['DATETIME_QTM_12'], with_foreshocks['MAGNITUDE_QTM_12'], label='foreshocks', fc=plot_color_dict['teal'], ec='white', linewidth=0.25, s=200, zorder=103, alpha=0.7)\n",
    "ax.scatter(dynamic_duo['DATETIME_QTM_12'], dynamic_duo['MAGNITUDE_QTM_12'], label=f'$\\delta$ SCSN to QTM', fc=plot_color_dict['teal'], ec=plot_color_dict['pink'], linewidth=2, s=200, zorder=104)\n",
    "ax.scatter(without_foreshocks['DATETIME_QTM_12'], without_foreshocks['MAGNITUDE_QTM_12'], label='no foreshocks', fc=plot_color_dict['orange'], ec='white', linewidth=0.25, s=200, zorder=101, alpha=0.7)\n",
    "\n",
    "ax = fig.add_subplot(gs[1:,:], projection=ccrs.PlateCarree())\n",
    "ax.set_title('b)', loc='left', fontsize=20)\n",
    "\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "ax.add_feature(cfeature.OCEAN, edgecolor='none')\n",
    "gl = ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, zorder=0)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xlabel_style = {'size': 20}\n",
    "gl.ylabel_style = {'size': 20}\n",
    "gl.xlines = False\n",
    "gl.ylines = False\n",
    "\n",
    "ax.scatter(with_foreshocks['LON_QTM_12'], with_foreshocks['LAT_QTM_12'], label='Mainshocks with foreshocks', fc=plot_color_dict['teal'], ec='white', linewidth=0.25, s=200, zorder=103, alpha=0.7)\n",
    "ax.scatter(dynamic_duo['LON_QTM_12'], dynamic_duo['LAT_QTM_12'], label=f'$\\delta$ SCSN to QTM', fc=plot_color_dict['teal'], ec=plot_color_dict['pink'], linewidth=2, s=200, zorder=104)\n",
    "ax.scatter(without_foreshocks['LON_QTM_12'], without_foreshocks['LAT_QTM_12'], label='Mainshocks w/o foreshocks', fc=plot_color_dict['orange'], ec='white', linewidth=0.25, s=200, zorder=101, alpha=0.7)\n",
    "ax.scatter(station_no_dup['LON'], station_no_dup['LAT'],  marker='^', fc='black', linewidth=0.25, ec='white', alpha=0.7,#fc='None',\n",
    "            label='STA', zorder=102)\n",
    "extent = utils.get_catalogue_extent(mainshock_file, buffer=0.5)\n",
    "ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "# plt.savefig('../outputs/figures/SCSN_QTM_foreshocks_map_mcut_v2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcut_flipped_mainshocks = ordered_merged_results.loc[(ordered_merged_results['ESR_QTM_12']<0.01) & (ordered_merged_results['ESR_SCSN']>=0.01)].copy()\n",
    "mcut_flipped_mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in ['SCSN', 'QTM_12']:\n",
    "    for mainshock in mcut_flipped_mainshocks.itertuples():\n",
    "        \n",
    "        filename = f'{mainshock.ID}.png'\n",
    "\n",
    "        source_folder = f'../outputs/{cat}/Mc_cut/model_plots/'\n",
    "\n",
    "        destination_folder = f'../outputs/mcut_flipped_mshocks/'\n",
    "        Path(destination_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        source_file = os.path.join(source_folder, filename)\n",
    "        \n",
    "        if os.path.exists(source_file):\n",
    "            filename = f'{mainshock.ID}_{cat}.png'\n",
    "            destination_file = os.path.join(destination_folder, filename)\n",
    "            \n",
    "            shutil.copyfile(source_file, destination_file)\n",
    "            print(f\"File '{filename}' copied to '{destination_folder}'\")\n",
    "        else:\n",
    "            print(f\"File '{filename}' not found in '{source_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_merged_results.loc[ordered_merged_results['ID'].isin(no_mcut_flipped_mainshocks['ID'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Investigating results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipped mainshocks, how close are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_merged_results.loc[ordered_merged_results['n_events_in_foreshock_window_SCSN']==ordered_merged_results['ESR_99CI_SCSN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_merged_results.loc[ordered_merged_results['n_events_in_foreshock_window_QTM_12']==ordered_merged_results['ESR_99CI_QTM_12']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcut_flipped_mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'SCSN'\n",
    "# name = 'QTM_12'\n",
    "cat = catalogue_dict[name].copy()\n",
    "# results_Mcut = Mc_cut_false_foreshock_file_dict[name].copy()\n",
    "results_Mcut = Mc_cut_true_foreshock_file_dict[name].copy()\n",
    "IDs = mcut_flipped_mainshocks['ID'].values\n",
    "ID = IDs[3]\n",
    "# mainshock_file = mainshock_dict[name].copy()\n",
    "mainshock_file = Mc_cut_true_foreshock_file_dict[name].copy()\n",
    "mainshock_file.rename(columns={col: col.replace('-', '_') for col in mainshock_file.columns}, inplace=True)\n",
    "mshock = statseis.iterable_mainshock(ID, mainshock_file)\n",
    "local_cat = statseis.create_local_catalogue(mainshock=mshock, earthquake_catalogue=cat, catalogue_name=name, radius_km=10)\n",
    "local_cat = local_cat.loc[local_cat['MAGNITUDE']>=mshock.Mc].copy()\n",
    "model_results_dict, model_file_dict = identify_foreshocks_short(local_catalogue=local_cat, mainshock=mshock, earthquake_catalogue=cat)\n",
    "model_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(mainshock=mshock, results_dict=model_results_dict, file_dict=model_file_dict, Mc_cut=True, catalogue_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = model_file_dict['sliding_window_counts']\n",
    "sns.histplot(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_99 = np.percentile(n, 99)\n",
    "CI_99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n[n==CI_99]), len(n[n==CI_99])/len(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_sum = []\n",
    "for ni in range(0, max(n)+1):\n",
    "    P = len(n[n==ni])/len(n)\n",
    "    print(ni, P, sum(P_sum))\n",
    "    P_sum.append(P)\n",
    "sum(P_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating false detection rates for type A and B foreshock definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alarm based false positive assessment\n",
    "\n",
    "If the 99th percentile is exceeded or a M3 occurs, an alarm is raised for 20 days.  \n",
    "\n",
    "If a M4 then occurs in the alarm period, it is a true positive alarm.  \n",
    "\n",
    "M4s that occur without alarms are false negatives.  \n",
    "\n",
    "Alarms that occur without M4s are false positives.  \n",
    "\n",
    "I am going to look at the time spent in alarm that does not lead to M4s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_A_and_B_alarm_model(mainshock, earthquake_catalogue, local_catalogue, mshock_mag_thresh=4, type_A_mag=3, plot=False, cat_name='Misc', Mcut=True, view=False,\n",
    "                            local_catalogue_radius = 10, foreshock_window = 20, modelling_time_period=345, points_per_day=1, alarm_window=20):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create an empirical model of the seismicity rate the year before a mainshock.\n",
    "    If the 99th percentile is exceeded or a M3 occurs, an alarm is raised for 20 days.  \n",
    "    If a M4 then occurs in the alarm period, it is a true positive alarm.  \n",
    "    M4s that occur without alarms are false negatives.  \n",
    "    Alarms that occur without M4s are false positives.  \n",
    "    I am going to look at the time spent in alarm that does not lead to M4s.\n",
    "    \"\"\"\n",
    "\n",
    "    if Mcut==True:\n",
    "        local_catalogue = local_catalogue.loc[local_catalogue['MAGNITUDE']>=mainshock.Mc].copy()\n",
    "\n",
    "    local_catalogue = local_catalogue[(local_catalogue['DATETIME'] <= mainshock.DATETIME) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] >= 0)  &\\\n",
    "                                        (local_catalogue['DISTANCE_TO_MAINSHOCK'] < local_catalogue_radius) #&\\\n",
    "                                        # (local_catalogue['ID'] != mainshock.ID)\n",
    "                                        ].copy()\n",
    "\n",
    "    n_mainshocks = len(local_catalogue.loc[local_catalogue['MAGNITUDE']>=4].copy())\n",
    "\n",
    "    # if n_mainshocks<1:\n",
    "    #     sys.exit(0)\n",
    "\n",
    "    regular_seismicity_period = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] >= foreshock_window)].copy()\n",
    "    foreshocks = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] < foreshock_window) & (local_catalogue['ID']!=mainshock.ID)].copy()\n",
    "\n",
    "    regular_M3s = len(regular_seismicity_period.loc[regular_seismicity_period['MAGNITUDE']>=3])\n",
    "    foreshock_M3s = len(foreshocks.loc[foreshocks['MAGNITUDE']>=3])\n",
    "\n",
    "    n_local_catalogue = len(local_catalogue)\n",
    "    n_regular_seismicity_events = len(regular_seismicity_period)\n",
    "    n_events_in_foreshock_window = len(foreshocks)\n",
    "    foreshock_distance = np.median(foreshocks['DISTANCE_TO_MAINSHOCK'])\n",
    "\n",
    "    catalogue_start_date = earthquake_catalogue['DATETIME'].iloc[0]\n",
    "    time_since_catalogue_start = (mainshock.DATETIME - catalogue_start_date).total_seconds()/3600/24\n",
    "\n",
    "    cut_off_day = math.floor(time_since_catalogue_start)\n",
    "    if cut_off_day > modelling_time_period+foreshock_window:\n",
    "        cut_off_day = modelling_time_period+foreshock_window\n",
    "    range_scaler = points_per_day\n",
    "    window_points = np.array(np.arange((-cut_off_day+foreshock_window)*range_scaler, 0*range_scaler+1, 1))/range_scaler*-1\n",
    "    print(window_points.max(), window_points.min())\n",
    "    window_results = []\n",
    "    A_alarm_counter, B_alarm_counter, G_IET_alarm_counter, BP_alarm_counter = [0]*4\n",
    "    for point in window_points:\n",
    "        window_20_back = local_catalogue.loc[(local_catalogue['DAYS_TO_MAINSHOCK'] >= point) &\\\n",
    "                                               (local_catalogue['DAYS_TO_MAINSHOCK'] < (point + foreshock_window))&\\\n",
    "                                               (local_catalogue['DAYS_TO_MAINSHOCK']!=0)].copy()\n",
    "        count = len(window_20_back)\n",
    "        if count>mainshock.ESR_99CI:\n",
    "            B_alarm=True\n",
    "            B_alarm_counter=alarm_window\n",
    "        elif count<=mainshock.ESR_99CI:\n",
    "            B_alarm=False\n",
    "        if B_alarm_counter>0:\n",
    "            B_alarm_counter-=1/points_per_day\n",
    "            B_alarm_counter = round(B_alarm_counter, 2)\n",
    "        if B_alarm_counter>0:\n",
    "            B_alarm=True\n",
    "        else:\n",
    "            B_alarm=False\n",
    "\n",
    "        if count>mainshock.G_IET_99CI:\n",
    "            G_IET_alarm=True\n",
    "            G_IET_alarm_counter=alarm_window\n",
    "        elif count<=mainshock.G_IET_99CI:\n",
    "            G_IET_alarm=False\n",
    "        if G_IET_alarm_counter>0:\n",
    "            G_IET_alarm_counter-=1/points_per_day\n",
    "            G_IET_alarm_counter = round(G_IET_alarm_counter, 2)\n",
    "        if G_IET_alarm_counter>0:\n",
    "            G_IET_alarm=True\n",
    "        else:\n",
    "            G_IET_alarm=False\n",
    "\n",
    "        if count>mainshock.BP_99CI:\n",
    "            BP_alarm=True\n",
    "            BP_alarm_counter=alarm_window\n",
    "        elif count<=mainshock.BP_99CI:\n",
    "            BP_alarm=False\n",
    "        if BP_alarm_counter>0:\n",
    "            BP_alarm_counter-=1/points_per_day\n",
    "            BP_alarm_counter = round(BP_alarm_counter, 2)\n",
    "        if BP_alarm_counter>0:\n",
    "            BP_alarm=True\n",
    "        else:\n",
    "            BP_alarm=False\n",
    "\n",
    "        small_window = local_catalogue.loc[(local_catalogue['DAYS_TO_MAINSHOCK'] >= point) &\\\n",
    "                                         (local_catalogue['DAYS_TO_MAINSHOCK'] < (point + points_per_day))].copy()\n",
    "        mainshocks = small_window.loc[small_window['MAGNITUDE']>=mshock_mag_thresh].copy()\n",
    "        \n",
    "        # type_As = small_window.loc[(small_window['MAGNITUDE']>=type_A_mag) & (small_window['MAGNITUDE']<mshock_mag_thresh)].copy()\n",
    "        type_As = small_window.loc[(small_window['MAGNITUDE']>=type_A_mag) & (small_window['ID']!=mainshock.ID)].copy()\n",
    "        if (len(type_As)>0):\n",
    "            A_alarm = True\n",
    "            A_alarm_counter=alarm_window\n",
    "        elif len(type_As)==0:\n",
    "            A_alarm=False\n",
    "        if A_alarm_counter>0:\n",
    "            A_alarm_counter-=1/points_per_day\n",
    "            A_alarm_counter = round(A_alarm_counter, 2)\n",
    "        if A_alarm_counter >0:\n",
    "            A_alarm = True\n",
    "        else:\n",
    "            A_alarm = False\n",
    "        \n",
    "        A_alarm_window = local_catalogue.loc[(local_catalogue['DAYS_TO_MAINSHOCK'] <= point) &\\\n",
    "                                         (local_catalogue['DAYS_TO_MAINSHOCK'] > (point - A_alarm_counter))].copy()\n",
    "        B_alarm_window = local_catalogue.loc[(local_catalogue['DAYS_TO_MAINSHOCK'] <= point) &\\\n",
    "                                         (local_catalogue['DAYS_TO_MAINSHOCK'] > (point - B_alarm_counter))].copy()\n",
    "        G_IET_alarm_window = local_catalogue.loc[(local_catalogue['DAYS_TO_MAINSHOCK'] <= point) &\\\n",
    "                                         (local_catalogue['DAYS_TO_MAINSHOCK'] > (point - G_IET_alarm_counter))].copy()\n",
    "        BP_alarm_window = local_catalogue.loc[(local_catalogue['DAYS_TO_MAINSHOCK'] <= point) &\\\n",
    "                                         (local_catalogue['DAYS_TO_MAINSHOCK'] > (point - BP_alarm_counter))].copy()\n",
    "        \n",
    "\n",
    "        A_positive = len(A_alarm_window.loc[A_alarm_window['MAGNITUDE']>=mshock_mag_thresh])\n",
    "        B_positive = len(B_alarm_window.loc[B_alarm_window['MAGNITUDE']>=mshock_mag_thresh])\n",
    "        G_IET_positive = len(G_IET_alarm_window.loc[G_IET_alarm_window['MAGNITUDE']>=mshock_mag_thresh])\n",
    "        BP_positive = len(BP_alarm_window.loc[BP_alarm_window['MAGNITUDE']>=mshock_mag_thresh])\n",
    "\n",
    "        window_results.append({'point':point,\n",
    "                               'type_As':len(type_As),\n",
    "                               'A_alarm':A_alarm,\n",
    "                               'A_alarm_counter':A_alarm_counter,\n",
    "                               'A_positive':A_positive,\n",
    "                               'count':count,\n",
    "                               'B_alarm':B_alarm,\n",
    "                               'B_alarm_counter':B_alarm_counter,\n",
    "                               'B_positive':B_positive,\n",
    "                               'G_IET_alarm':G_IET_alarm,\n",
    "                               'G_IET_alarm_counter':G_IET_alarm_counter,\n",
    "                               'G_IET_positive':B_positive,\n",
    "                               'BP_alarm':BP_alarm,\n",
    "                               'BP_alarm_counter':BP_alarm_counter,\n",
    "                               'BP_positive':BP_positive,\n",
    "                               'mainshocks':len(mainshocks)})\n",
    "    window_results = pd.DataFrame.from_dict(window_results)\n",
    "\n",
    "    if plot==True:\n",
    "        scale_eq_marker = (lambda x: 100 + np.exp(2*x))\n",
    "        xlims = -10, 370\n",
    "\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "        ax= fig.add_subplot(211)\n",
    "        ax.set_title(f\"{mainshock.ID} - {mainshock.DATETIME.strftime('%b %d %Y')} - {name}\", fontsize=20, loc='right')\n",
    "        ax.scatter(local_catalogue['DAYS_TO_MAINSHOCK'], local_catalogue['MAGNITUDE'], c=local_catalogue['DAYS_TO_MAINSHOCK'], alpha=0.7, ec='white', s=scale_eq_marker(local_catalogue['MAGNITUDE']))\n",
    "        ax.set_xlim(xlims)\n",
    "        ax.set_xlabel('Days to mainshock')\n",
    "        ax.set_ylabel('Magnitude')\n",
    "        ax.axhline(4, linestyle='--', color='grey', alpha=0.5)\n",
    "        ax.axhline(3, linestyle='--', color='grey', alpha=0.5)\n",
    "        ax.axvline(20, linestyle='--', color='grey', alpha=0.5)\n",
    "        ax.invert_xaxis()\n",
    "\n",
    "        ax = fig.add_subplot(212)\n",
    "        ax.plot(window_results['point'], window_results['A_alarm_counter'], alpha=0.5, label='A pred', color=plot_color_dict['yellow'])\n",
    "        ax.plot(window_results['point'], window_results['B_alarm_counter'], alpha=0.5, label='ESR pred', color=plot_color_dict['purple'])\n",
    "        ax.plot(window_results['point'], window_results['G_IET_alarm_counter'], alpha=0.5, label='G-IET pred', color=plot_color_dict['teal'])\n",
    "        ax.plot(window_results['point'], window_results['BP_alarm_counter'], alpha=0.5, label='BP pred', color=plot_color_dict['orange'])\n",
    "        ax.set_xlabel('Days to mainshock')\n",
    "        ax.set_ylabel('Alarm days')\n",
    "        # twin_ax = ax.twinx()\n",
    "        # twin_ax.plot(window_results['point'], window_results['mainshocks'], alpha=0.5, label='Actual', color=plot_color_dict['pink'])\n",
    "        # twin_ax.set_ylabel('Mainshocks')\n",
    "        # twin_ax.plot(window_results['point'], window_results['A_positive'], alpha=0.5, label='Actual', color=plot_color_dict['orange'])\n",
    "        ax.set_xlim(xlims)\n",
    "        ax.invert_xaxis()\n",
    "        # h1, l1 = ax.get_legend_handles_labels()\n",
    "        # h2, l2 = twin_ax.get_legend_handles_labels()\n",
    "        # ax.legend(h1+h2, l1+l2, loc='upper left')\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "        Path(f'../outputs/{cat_name}/alarm_plots').mkdir(parents=True, exist_ok=True)\n",
    "        print('saving')\n",
    "        plt.savefig(f'../outputs/{cat_name}/alarm_plots/{mainshock.ID}.png')\n",
    "        if view==True:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    A_alarm_days = len(window_results.loc[window_results['A_alarm_counter']>0])\n",
    "    B_alarm_days = len(window_results.loc[window_results['B_alarm_counter']>0])\n",
    "    G_IET_alarm_days = len(window_results.loc[window_results['G_IET_alarm_counter']>0])\n",
    "    BP_alarm_days = len(window_results.loc[window_results['BP_alarm_counter']>0])\n",
    "\n",
    "    A_TP_days = len(window_results.loc[(window_results['A_alarm_counter']>0) & (window_results['A_positive']>0)])\n",
    "    B_TP_days = len(window_results.loc[(window_results['B_alarm_counter']>0)  & (window_results['B_positive']>0)])\n",
    "\n",
    "    A_FP_days = A_alarm_days-A_TP_days\n",
    "    B_FP_days = B_alarm_days-B_TP_days\n",
    "\n",
    "    # A_pos_events = len(window_results.loc[(window_results['A_alarm_counter']>0) & (window_results['mainshocks']>0)])\n",
    "    # B_pos_events = len(window_results.loc[(window_results['B_alarm_counter']>0)  & (window_results['mainshocks']>0)])\n",
    "    # A_neg_events = len(window_results.loc[(window_results['A_alarm_counter']==0) & (window_results['mainshocks']>0)])\n",
    "    # B_neg_events = len(window_results.loc[(window_results['B_alarm_counter']==0)  & (window_results['mainshocks']>0)])\n",
    "\n",
    "    A_pos_events = len(window_results.loc[(window_results['point']==0) & (window_results['A_alarm']==True)])\n",
    "    B_pos_events = len(window_results.loc[(window_results['point']==0) & (window_results['B_alarm']==True)])\n",
    "    G_IET_pos_events = len(window_results.loc[(window_results['point']==0) & (window_results['G_IET_alarm']==True)])\n",
    "    BP_pos_events = len(window_results.loc[(window_results['point']==0) & (window_results['BP_alarm']==True)])\n",
    "\n",
    "    A_neg_events = len(window_results.loc[(window_results['point']==0) & (window_results['A_alarm']==False)])\n",
    "    B_neg_events = len(window_results.loc[(window_results['point']==0) & (window_results['B_alarm']==False)])\n",
    "    G_IET_neg_events = len(window_results.loc[(window_results['point']==0) & (window_results['G_IET_alarm']==False)])\n",
    "    BP_neg_events = len(window_results.loc[(window_results['point']==0) & (window_results['BP_alarm']==False)])\n",
    "\n",
    "    A_TN_days = len(window_results.loc[(window_results['A_alarm_counter']==0) & (window_results['mainshocks']==0)])\n",
    "    B_TN_days = len(window_results.loc[(window_results['B_alarm_counter']==0) & (window_results['mainshocks']==0)])\n",
    "\n",
    "    A_FN_days = len(window_results.loc[(window_results['A_alarm_counter']==0) & (window_results['mainshocks']>0)])\n",
    "    B_FN_days = len(window_results.loc[(window_results['B_alarm_counter']==0) & (window_results['mainshocks']>0)])\n",
    "\n",
    "    # type_dict = {'A':{}, 'B':{}}\n",
    "    # for key, item in type_dict.items():\n",
    "    #     TN, FP, FN, TP  = confusion_matrix(window_results['Actual'], window_results[f'type_{key}_predicted']).ravel()\n",
    "    #     type_dict[key] = {'TN':TN,\n",
    "    #                  'FP':FP,\n",
    "    #                  'FN':FN,\n",
    "    #                  'TP':TP,\n",
    "    #                  'TPR': TP/(TP+FN),\n",
    "    #                  'TNR': TN/(TN+FP),\n",
    "    #                  'PPV': TP/(TP+FP),\n",
    "    #                  'NPV': TN/(TN+FN),\n",
    "    #                  'FPR': FP/(FP+TN),\n",
    "    #                  'FNR': FN/(TP+FN),\n",
    "    #                  'FDR': FP/(TP+FP),\n",
    "    #                  'ACC': (TP+TN)/(TP+FP+FN+TN)\n",
    "    #                  }\n",
    "\n",
    "    results_dict = {'ID':mainshock.ID,\n",
    "                    'MAGNITUDE':mainshock.MAGNITUDE,\n",
    "                    'LON':mainshock.LON,\n",
    "                    'LAT':mainshock.LAT,\n",
    "                    'DATETIME':mainshock.DATETIME,\n",
    "                    'DEPTH':mainshock.DEPTH,\n",
    "                    'Mc':mainshock.Mc,\n",
    "                    'time_since_catalogue_start':time_since_catalogue_start,\n",
    "                    'n_regular_seismicity_events':n_regular_seismicity_events,\n",
    "                    'n_events_in_foreshock_window':n_events_in_foreshock_window,\n",
    "                    'N_obs_old':mainshock.n_events_in_foreshock_window,\n",
    "                    # 'max_20day_rate':max_window,\n",
    "                    'ESR':mainshock.ESR,\n",
    "                    'ESR_99CI':mainshock.ESR_99CI,\n",
    "                    'cut_off_day':cut_off_day,\n",
    "                    'days':len(window_results),\n",
    "                    'A_alarm_days':A_alarm_days,\n",
    "                    'B_alarm_days':B_alarm_days,\n",
    "                    'G_IET_alarm_days':G_IET_alarm_days,\n",
    "                    'BP_alarm_days':BP_alarm_days,\n",
    "                    'A_TP_days':A_TP_days,\n",
    "                    'B_TP_days':B_TP_days,\n",
    "                    'A_FP_days':A_FP_days,\n",
    "                    'B_FP_days':B_FP_days,\n",
    "                    'A_TN_days':A_TN_days,\n",
    "                    'B_TN_days':B_TN_days,\n",
    "                    'A_FN_days':A_FN_days,\n",
    "                    'B_FN_days':B_FN_days,\n",
    "                    'mainshocks':n_mainshocks,\n",
    "                    'A_pos_events':A_pos_events,\n",
    "                    'B_pos_events':B_pos_events,\n",
    "                    'G_IET_pos_events':G_IET_pos_events,\n",
    "                    'BP_pos_events':BP_pos_events,\n",
    "                    'A_neg_events':A_neg_events,\n",
    "                    'B_neg_events':B_neg_events,\n",
    "                    'G_IET_neg_events':G_IET_neg_events,\n",
    "                    'BP_neg_events':BP_neg_events\n",
    "                    }\n",
    "        \n",
    "    file_dict = {'local_catalogue':local_catalogue,\n",
    "                 'foreshocks':foreshocks}\n",
    "    \n",
    "    return results_dict, file_dict, window_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single mainshock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'SCSN'\n",
    "name = 'QTM_12'\n",
    "cat = catalogue_dict[name].copy()\n",
    "# results_Mcut = Mc_cut_false_foreshock_file_dict[name].copy()\n",
    "results_Mcut = Mc_cut_true_foreshock_file_dict[name].copy()\n",
    "# ID = 10370141\n",
    "ID = 14433456\n",
    "ID = 10370141\n",
    "IDs = [10406593, 14598228, 11006189]\n",
    "ID = IDs[1]\n",
    "# mainshock_file = mainshock_dict[name].copy()\n",
    "mainshock_file = Mc_cut_true_foreshock_file_dict[name].copy()\n",
    "mainshock_file.rename(columns={col: col.replace('-', '_') for col in mainshock_file.columns}, inplace=True)\n",
    "mshock = statseis.iterable_mainshock(ID, mainshock_file)\n",
    "local_cat = statseis.create_local_catalogue(mainshock=mshock, earthquake_catalogue=cat, catalogue_name=name, radius_km=10)\n",
    "res_dict, file_dict, window_results = type_A_and_B_alarm_model(mainshock=mshock, earthquake_catalogue=cat, local_catalogue=local_cat, points_per_day=1, plot=True, view=True, cat_name=name)\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_results.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_cat = local_cat.loc[local_cat['MAGNITUDE']>=mshock.Mc].copy()\n",
    "model_results_dict, model_file_dict = identify_foreshocks_short(local_catalogue=local_cat, mainshock=mshock, earthquake_catalogue=cat)\n",
    "model_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(mainshock=mshock, results_dict=model_results_dict, file_dict=model_file_dict, Mc_cut=True, catalogue_name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'SCSN'\n",
    "name = 'QTM_12'\n",
    "cat = catalogue_dict[name].copy()\n",
    "mshock_window_results = []\n",
    "for mshock in tqdm(mainshock_file.itertuples(), total=len(mainshock_file)):\n",
    "    local_cat = statseis.create_local_catalogue(mshock, earthquake_catalogue=cat, catalogue_name=name, radius_km=10)\n",
    "    res_dict, file_dict, window_results = type_A_and_B_alarm_model(mainshock=mshock, earthquake_catalogue=cat, local_catalogue=local_cat, points_per_day=1, cat_name=name, plot=True, Mcut=True)\n",
    "    mshock_window_results.append(res_dict)\n",
    "    clear_output(wait=True)\n",
    "mshock_window_results = pd.DataFrame.from_dict(mshock_window_results)\n",
    "mshock_window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = mshock_window_results['days'].sum()\n",
    "A_alarm_days = mshock_window_results['A_alarm_days'].sum()\n",
    "B_alarm_days = mshock_window_results['B_alarm_days'].sum()\n",
    "G_IET_alarm_days = mshock_window_results['G_IET_alarm_days'].sum()\n",
    "BP_alarm_days = mshock_window_results['BP_alarm_days'].sum()\n",
    "\n",
    "A_alarm_fraction = A_alarm_days/days\n",
    "B_alarm_fraction = B_alarm_days/days\n",
    "G_IET_alarm_fraction = G_IET_alarm_days/days\n",
    "BP_alarm_fraction = BP_alarm_days/days\n",
    "\n",
    "A_miss_rate = mshock_window_results['A_neg_events'].sum()/len(mshock_window_results)\n",
    "B_miss_rate = mshock_window_results['B_neg_events'].sum()/len(mshock_window_results)\n",
    "G_IET_miss_rate = mshock_window_results['G_IET_neg_events'].sum()/len(mshock_window_results)\n",
    "BP_miss_rate = mshock_window_results['BP_neg_events'].sum()/len(mshock_window_results)\n",
    "\n",
    "alarm_results_dict = {'Definition': ['Type A', 'ESR', 'G-IET', 'BP'],\n",
    "                      'Alarm_time_fraction': [A_alarm_fraction, B_alarm_fraction, G_IET_alarm_fraction, BP_alarm_fraction],\n",
    "                      'Miss_rate': [A_miss_rate, B_miss_rate, G_IET_miss_rate, BP_miss_rate],\n",
    "                      'Hits':[mshock_window_results['A_pos_events'].sum(), mshock_window_results['B_pos_events'].sum(), mshock_window_results['G_IET_pos_events'].sum(), mshock_window_results['BP_pos_events'].sum()]\n",
    "                      }\n",
    "alarm_results_df = pd.DataFrame(alarm_results_dict)\n",
    "for col in ['Alarm_time_fraction', 'Miss_rate']:\n",
    "    alarm_results_df[col] = [round(x,2) for x in alarm_results_df[col]]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(A_alarm_fraction, A_miss_rate, label='Type A', alpha=0.7, color=plot_color_dict['yellow'])\n",
    "ax.scatter(B_alarm_fraction, B_miss_rate, label='ESR', alpha=0.7, color=plot_color_dict['purple'])\n",
    "ax.scatter(G_IET_alarm_fraction, G_IET_miss_rate, label='G-IET', alpha=0.7, color=plot_color_dict['teal'])\n",
    "ax.scatter(BP_alarm_fraction, BP_miss_rate, label='BP', alpha=0.7, color=plot_color_dict['orange'])\n",
    "\n",
    "ax.plot([0,1], [1,0], label='Unskilled alarms', linestyle='--', color='grey', alpha=0.7)\n",
    "ax.set_xlabel('Time fraction in alarm')\n",
    "ax.set_ylabel('Miss rate')\n",
    "ax.legend()\n",
    "plt.savefig(f'../outputs/rough/Molchan_74_{name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alarm_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('Type A')\n",
    "# metrics = ['tn_A', 'fp_A', 'fn_A', 'tp_A']\n",
    "# metrics = ['A_TN_days', 'A_FP_days', 'A_FN_days', 'A_TP_days']\n",
    "metrics = ['A_TN_days', 'A_FP_days', 'A_neg_events', 'A_pos_events']\n",
    "type_A_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_A_cm[0, i] = mshock_window_results[m].sum() \n",
    "    else:\n",
    "        type_A_cm[1, i - 2] = mshock_window_results[m].sum()\n",
    "type_A_cm = type_A_cm.astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_A_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title('Type B')\n",
    "# metrics = ['tn_B', 'fp_B', 'fn_B', 'tp_B']\n",
    "# metrics = ['B_TN_days', 'B_FP_days', 'B_FN_days', 'B_TP_days']\n",
    "metrics = ['B_TN_days', 'B_FP_days', 'B_neg_events', 'B_pos_events']\n",
    "type_B_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_B_cm[0, i] = mshock_window_results[m].sum()\n",
    "    else:\n",
    "        type_B_cm[1, i - 2] = mshock_window_results[m].sum()\n",
    "type_B_cm = type_B_cm.astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_B_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    for text in ax.texts:\n",
    "        text.set_fontsize(20) \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../outputs/rough/alarm_CM_all_mainshocks.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {'A':{}, 'B':{}}\n",
    "cm_dict = {'A':type_A_cm, 'B':type_B_cm}\n",
    "for key, item in cm_dict.items():\n",
    "    TN, FP, FN, TP  = item.ravel()\n",
    "    type_dict[key] = {\n",
    "                    # 'TN':TN,\n",
    "                    # 'FP':FP,\n",
    "                    # 'FN':FN,\n",
    "                    # 'TP':TP,\n",
    "                    'TPR': round(TP/(TP+FN),2),\n",
    "                    'TNR': round(TN/(TN+FP),2),\n",
    "                    'PPV': round(TP/(TP+FP),2),\n",
    "                    'NPV': round(TN/(TN+FN),2),\n",
    "                    'FPR': round(FP/(FP+TN),2),\n",
    "                    'FNR': round(FN/(TP+FN),2),\n",
    "                    'FDR': round(FP/(TP+FP),2),\n",
    "                    'ACC': round((TP+TN)/(TP+FP+FN+TN),2)\n",
    "                    }\n",
    "\n",
    "type_df = pd.DataFrame.from_dict(type_dict)\n",
    "type_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mshock_window_results.loc[mshock_window_results['BP_pos_events']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_pos = mshock_window_results.loc[mshock_window_results['B_pos_events']==1].copy()\n",
    "oh_no = B_pos.loc[~(B_pos['ID'].isin(mainshock_file.loc[mainshock_file['ESR']<0.01, 'ID']))]\n",
    "oh_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainshock_file.loc[mainshock_file['ID'].isin(oh_no['ID'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just sequences with a single mainshock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_mainshocks = mshock_window_results.loc[mshock_window_results['mainshocks']>1].copy()\n",
    "multiple_mainshocks.sort_values(by=\"MAGNITUDE\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_to_copy = [str(x) + '.png' for x in multiple_mainshocks['ID']]\n",
    "\n",
    "source_folder = f'../outputs/{name}/alarm_plots/'\n",
    "\n",
    "destination_folder = f'../outputs/{name}/alarm_plots/multiple_mshocks/'\n",
    "Path(destination_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for filename in filenames_to_copy:\n",
    "    source_file = os.path.join(source_folder, filename)\n",
    "    \n",
    "    if os.path.exists(source_file):\n",
    "        destination_file = os.path.join(destination_folder, filename)\n",
    "        \n",
    "        shutil.copyfile(source_file, destination_file)\n",
    "        print(f\"File '{filename}' copied to '{destination_folder}'\")\n",
    "    else:\n",
    "        print(f\"File '{filename}' not found in '{source_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_mainshocks = mshock_window_results.loc[mshock_window_results['mainshocks']==1].copy()\n",
    "single_mainshocks.sort_values(by=\"MAGNITUDE\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = single_mainshocks.copy()\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('Type A')\n",
    "# metrics = ['tn_A', 'fp_A', 'fn_A', 'tp_A']\n",
    "# metrics = ['A_TN_days', 'A_FP_days', 'A_FN_days', 'A_TP_days']\n",
    "metrics = ['A_TN_days', 'A_FP_days', 'A_neg_events', 'A_pos_events']\n",
    "type_A_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_A_cm[0, i] = infile[m].sum() \n",
    "    else:\n",
    "        type_A_cm[1, i - 2] = infile[m].sum()\n",
    "type_A_cm = type_A_cm.astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_A_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title('Type B')\n",
    "# metrics = ['tn_B', 'fp_B', 'fn_B', 'tp_B']\n",
    "# metrics = ['B_TN_days', 'B_FP_days', 'B_FN_days', 'B_TP_days']\n",
    "metrics = ['B_TN_days', 'B_FP_days', 'B_neg_events', 'B_pos_events']\n",
    "type_B_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_B_cm[0, i] = infile[m].sum()\n",
    "    else:\n",
    "        type_B_cm[1, i - 2] = infile[m].sum()\n",
    "type_B_cm = type_B_cm.astype(int)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_B_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    for text in ax.texts:\n",
    "        text.set_fontsize(20) \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../outputs/rough/alarm_CM_single_mainshock_sequences.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {'A':{}, 'B':{}}\n",
    "cm_dict = {'A':type_A_cm, 'B':type_B_cm}\n",
    "for key, item in cm_dict.items():\n",
    "    TN, FP, FN, TP  = item.ravel()\n",
    "    type_dict[key] = {\n",
    "                    # 'TN':TN,\n",
    "                    # 'FP':FP,\n",
    "                    # 'FN':FN,\n",
    "                    # 'TP':TP,\n",
    "                    'TPR': round(TP/(TP+FN),2),\n",
    "                    'TNR': round(TN/(TN+FP),2),\n",
    "                    'PPV': round(TP/(TP+FP),2),\n",
    "                    'NPV': round(TN/(TN+FN),2),\n",
    "                    'FPR': round(FP/(FP+TN),2),\n",
    "                    'FNR': round(FN/(TP+FN),2),\n",
    "                    'FDR': round(FP/(TP+FP),2),\n",
    "                    'ACC': round((TP+TN)/(TP+FP+FN+TN),2)\n",
    "                    }\n",
    "\n",
    "type_df = pd.DataFrame.from_dict(type_dict)\n",
    "type_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = single_mainshocks['days'].sum()\n",
    "A_alarm_days = single_mainshocks['A_alarm_days'].sum()\n",
    "B_alarm_days = single_mainshocks['B_alarm_days'].sum()\n",
    "A_alarm_fraction = A_alarm_days/days\n",
    "B_alarm_fraction = B_alarm_days/days\n",
    "A_miss_rate = single_mainshocks['A_neg_events'].sum()/single_mainshocks['mainshocks'].sum()\n",
    "B_miss_rate = single_mainshocks['B_neg_events'].sum()/single_mainshocks['mainshocks'].sum()\n",
    "\n",
    "A_alarm_fraction, B_alarm_fraction, A_miss_rate, B_miss_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(A_alarm_fraction, A_miss_rate, label='Type A', alpha=0.7)\n",
    "ax.scatter(B_alarm_fraction, B_miss_rate, label='Type B', alpha=0.7)\n",
    "ax.plot([0,1], [1,0], label='Unskilled alarms', linestyle='--', color='grey', alpha=0.7)\n",
    "ax.set_xlabel('Time fraction in alarm')\n",
    "ax.set_ylabel('Miss rate')\n",
    "ax.legend()\n",
    "plt.savefig('../outputs/rough/Molchan_62.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 1 year local catalogs\n",
    "Trust this function for giving good results (correct CM) 1 year prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESR_model(mainshock, earthquake_catalogue, local_catalogue, mainshock_mag=4, type_A_mag=3,\n",
    "              local_catalogue_radius = 10, foreshock_window = 20, modelling_time_period=345, points_per_day=10, calibration_period=365):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate signals prior to mainshocks in a sliding window: seismicity rates, distances to mainshock.\n",
    "    \"\"\"\n",
    "    \n",
    "    mainshock_ID = mainshock.ID\n",
    "    mainshock_LON = mainshock.LON\n",
    "    mainshock_LAT = mainshock.LAT\n",
    "    mainshock_DATETIME = mainshock.DATETIME\n",
    "    mainshock_Mc = mainshock.Mc\n",
    "    mainshock_MAG = mainshock.MAGNITUDE\n",
    "\n",
    "    local_cat_all = local_catalogue.copy()\n",
    "\n",
    "    local_catalogue = local_catalogue[(local_catalogue['DATETIME'] < mainshock_DATETIME) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] > 0)  &\\\n",
    "                                        (local_catalogue['DISTANCE_TO_MAINSHOCK'] < local_catalogue_radius) &\\\n",
    "                                        (local_catalogue['ID'] != mainshock_ID)\n",
    "                                        ].copy()\n",
    "\n",
    "    regular_seismicity_period = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] >= foreshock_window)].copy()\n",
    "    foreshocks = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] < foreshock_window)].copy()\n",
    "\n",
    "    regular_M3s = len(regular_seismicity_period.loc[regular_seismicity_period['MAGNITUDE']>=3])\n",
    "    foreshock_M3s = len(foreshocks.loc[foreshocks['MAGNITUDE']>=3])\n",
    "\n",
    "    n_local_catalogue = len(local_catalogue)\n",
    "    n_regular_seismicity_events = len(regular_seismicity_period)\n",
    "    n_events_in_foreshock_window = len(foreshocks)\n",
    "    foreshock_distance = np.median(foreshocks['DISTANCE_TO_MAINSHOCK'])\n",
    "\n",
    "    catalogue_start_date = earthquake_catalogue['DATETIME'].iloc[0]\n",
    "    time_since_catalogue_start = (mainshock_DATETIME - catalogue_start_date).total_seconds()/3600/24\n",
    "    cut_off_day = math.floor(time_since_catalogue_start)\n",
    "    if cut_off_day > calibration_period:\n",
    "        cut_off_day = calibration_period\n",
    "        \n",
    "    range_scaler = 100\n",
    "    sliding_window_points = np.array(np.arange((-cut_off_day+foreshock_window)*range_scaler, -foreshock_window*range_scaler+1/range_scaler, 1))/range_scaler*-1\n",
    "\n",
    "    sliding_window_counts = np.array([len(regular_seismicity_period.loc[(regular_seismicity_period['DAYS_TO_MAINSHOCK'] > point) &\\\n",
    "                                                                    (regular_seismicity_period['DAYS_TO_MAINSHOCK'] <= (point + foreshock_window))]) for point in sliding_window_points])\n",
    "    sliding_window_distances = np.array([np.median(regular_seismicity_period.loc[(regular_seismicity_period['DAYS_TO_MAINSHOCK'] > point) &\\\n",
    "                                                                    (regular_seismicity_period['DAYS_TO_MAINSHOCK'] <= (point + foreshock_window)), 'DISTANCE_TO_MAINSHOCK']) for point in sliding_window_points])\n",
    "\n",
    "    try:\n",
    "        distance_probability = len(sliding_window_distances[sliding_window_distances >= foreshock_distance])/len(sliding_window_distances)\n",
    "    except:\n",
    "        distance_probability = float('nan')\n",
    "\n",
    "    try:\n",
    "        max_window = max(sliding_window_counts)\n",
    "    except:\n",
    "        max_window = float('nan')\n",
    "\n",
    "    if n_events_in_foreshock_window > max_window:\n",
    "        max_window_method = 0.0\n",
    "    elif n_events_in_foreshock_window <= max_window:\n",
    "        max_window_method = 1.0\n",
    "    else:\n",
    "        max_window_method = float('nan')\n",
    "\n",
    "    if (len(sliding_window_counts)==0) & (n_events_in_foreshock_window > 0):\n",
    "        sliding_window_probability = 0.00\n",
    "        sliding_window_99CI = float('nan')\n",
    "    elif (len(sliding_window_counts)==0) & (n_events_in_foreshock_window == 0):    \n",
    "        sliding_window_probability = 1.00\n",
    "        sliding_window_99CI = float('nan')\n",
    "    else:\n",
    "        sliding_window_probability = len(sliding_window_counts[sliding_window_counts >= n_events_in_foreshock_window])/len(sliding_window_counts)\n",
    "        sliding_window_99CI = np.percentile(sliding_window_counts,99)\n",
    "\n",
    "    counts_ge_Nobs = len(sliding_window_counts[sliding_window_counts>=n_events_in_foreshock_window])\n",
    "    counts_ge_ESR_99 = len(sliding_window_counts[sliding_window_counts>=sliding_window_99CI])\n",
    "\n",
    "    cut_off_day = math.floor(time_since_catalogue_start)\n",
    "    if cut_off_day > modelling_time_period+foreshock_window:\n",
    "        cut_off_day = modelling_time_period+foreshock_window\n",
    "    range_scaler = points_per_day\n",
    "    window_points = np.array(np.arange((-cut_off_day+foreshock_window)*range_scaler, 0*range_scaler+1, 1))/range_scaler*-1\n",
    "    print(window_points.max(), window_points.min())\n",
    "    window_results = []\n",
    "    for point in window_points:\n",
    "        window_20_back = local_cat_all.loc[(local_cat_all['DAYS_TO_MAINSHOCK'] >= point) &\\\n",
    "                                               (local_cat_all['DAYS_TO_MAINSHOCK'] < (point + foreshock_window))&\\\n",
    "                                               (local_cat_all['DAYS_TO_MAINSHOCK']!=0)].copy()\n",
    "        window_20_forward = local_cat_all.loc[(local_cat_all['DAYS_TO_MAINSHOCK'] <= point) &\\\n",
    "                                               (local_cat_all['DAYS_TO_MAINSHOCK'] > (point - foreshock_window))].copy()\n",
    "        count = len(window_20_back)\n",
    "        type_As = window_20_back.loc[(window_20_back['MAGNITUDE']>=type_A_mag) & (window_20_back['MAGNITUDE']<mainshock_mag)].copy()\n",
    "        mainshocks = window_20_forward.loc[window_20_forward['MAGNITUDE']>=mainshock_mag].copy()\n",
    "        window_results.append({'point':point,\n",
    "                               'count':count,\n",
    "                               'type_As':len(type_As),\n",
    "                               'mainshocks':len(mainshocks)})\n",
    "    window_results = pd.DataFrame.from_dict(window_results)\n",
    "    window_results['Actual'] = (window_results['mainshocks']>0)\n",
    "    window_results['type_A_predicted'] = window_results['type_As'] > 0\n",
    "    window_results['type_B_predicted'] = window_results['count'] > sliding_window_99CI\n",
    "\n",
    "    foreshock_window_results = window_results.loc[window_results['point']<20].copy()\n",
    "    calibration_period_results = window_results.loc[window_results['point']>=20].copy()\n",
    "\n",
    "    # TP_days = len(foreshock_window_results.loc[(foreshock_window_results['count']>=sliding_window_99CI) & (calibration_period_results['Actual']==False)])\n",
    "    # FP_days = len(calibration_period_results.loc[(calibration_period_results['count']>=sliding_window_99CI) & (calibration_period_results['Actual']==False)])\n",
    "    # TN_days = len(calibration_period_results.loc[(calibration_period_results['count']<sliding_window_99CI) & (calibration_period_results['Actual']==False)])\n",
    "    # FN_days = len(calibration_period_results.loc[(calibration_period_results['Actual']==True) & (calibration_period_results['Actual']==False)])\n",
    "\n",
    "    if sliding_window_probability<0.01:\n",
    "        TP_days =1\n",
    "    else:\n",
    "        TP_days=0 \n",
    "\n",
    "    # TP_days = len(foreshock_window_results.loc[(foreshock_window_results['count']>=sliding_window_99CI)])\n",
    "    FP_days = len(calibration_period_results.loc[(calibration_period_results['count']>=sliding_window_99CI)])\n",
    "    TN_days = len(calibration_period_results.loc[(calibration_period_results['count']<sliding_window_99CI)])\n",
    "    FN_days = len(calibration_period_results.loc[(calibration_period_results['Actual']==True)])\n",
    "    days = len(window_results)\n",
    "\n",
    "    windows_ge_Nobs = len(window_results.loc[window_results['count']>=n_events_in_foreshock_window])\n",
    "    windows_ge_ESR_99 = len(window_results.loc[window_results['count']>=sliding_window_99CI])\n",
    "\n",
    "    type_dict = {'A':{}, 'B':{}}\n",
    "    for key, item in type_dict.items():\n",
    "        TN, FP, FN, TP  = confusion_matrix(window_results['Actual'], window_results[f'type_{key}_predicted']).ravel()\n",
    "        type_dict[key] = {'TN':TN,\n",
    "                     'FP':FP,\n",
    "                     'FN':FN,\n",
    "                     'TP':TP,\n",
    "                     'TPR': TP/(TP+FN),\n",
    "                     'TNR': TN/(TN+FP),\n",
    "                     'PPV': TP/(TP+FP),\n",
    "                     'NPV': TN/(TN+FN),\n",
    "                     'FPR': FP/(FP+TN),\n",
    "                     'FNR': FN/(TP+FN),\n",
    "                     'FDR': FP/(TP+FP),\n",
    "                     'ACC': (TP+TN)/(TP+FP+FN+TN)\n",
    "                     }\n",
    "\n",
    "    results_dict = {'ID':mainshock_ID,\n",
    "                    'MAGNITUDE':mainshock_MAG,\n",
    "                    'LON':mainshock_LON,\n",
    "                    'LAT':mainshock_LAT,\n",
    "                    'DATETIME':mainshock_DATETIME,\n",
    "                    'DEPTH':mainshock.DEPTH,\n",
    "                    'Mc':mainshock_Mc,\n",
    "                    'time_since_catalogue_start':time_since_catalogue_start,\n",
    "                    'n_regular_seismicity_events':n_regular_seismicity_events,\n",
    "                    'n_events_in_foreshock_window':n_events_in_foreshock_window,\n",
    "                    'max_20day_rate':max_window,\n",
    "                    'ESR':sliding_window_probability,\n",
    "                    'ESR_99CI':sliding_window_99CI,\n",
    "                    'ESD':distance_probability,\n",
    "                    'cut_off_day':cut_off_day,\n",
    "                    'regular_M3s':regular_M3s,\n",
    "                    'foreshock_M3s':foreshock_M3s,\n",
    "                    'TP_days':TP_days,\n",
    "                    'FP_days':FP_days,\n",
    "                    'TN_days':TN_days,\n",
    "                    'FN_days':FN_days,\n",
    "                    'days':days,\n",
    "                    # 'counts_ge_Nobs':counts_ge_Nobs,\n",
    "                    # 'counts_ge_ESR':counts_ge_ESR_99,\n",
    "                    # 'counts':len(sliding_window_counts),\n",
    "                    # 'windows_ge_Nobs':windows_ge_Nobs,\n",
    "                    # 'windows_ge_ESR_99':windows_ge_ESR_99,\n",
    "                    # 'windows':len(window_results),\n",
    "                    'A_fp_rate':type_dict['A']['FPR'],\n",
    "                    'B_fp_rate':type_dict['B']['FPR'],\n",
    "                    'tn_A':type_dict['A']['TN'],\n",
    "                    'fp_A':type_dict['A']['FP'],\n",
    "                    'fn_A':type_dict['A']['FN'],\n",
    "                    'tp_A':type_dict['A']['TP'],\n",
    "                    'tn_B':type_dict['B']['TN'],\n",
    "                    'fp_B':type_dict['B']['FP'],\n",
    "                    'fn_B':type_dict['B']['FN'],\n",
    "                    'tp_B':type_dict['B']['TP']\n",
    "                    }\n",
    "        \n",
    "    file_dict = {'local_catalogue':local_catalogue,\n",
    "                #  'local_catalogue_pre_Mc_cutoff':local_catalogue_pre_Mc_cutoff,\n",
    "                #  'local_catalogue_below_Mc':local_catalogue_below_Mc,\n",
    "                 'foreshocks':foreshocks,\n",
    "                #  'foreshocks_below_Mc':foreshocks_below_Mc,\n",
    "                 'sliding_window_points':sliding_window_points,\n",
    "                 'sliding_window_counts':sliding_window_counts,\n",
    "                 'sliding_window_distances':sliding_window_distances\n",
    "                 }\n",
    "    \n",
    "    return results_dict, file_dict, window_results\n",
    "    # return window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'SCSN'\n",
    "name = 'QTM_12'\n",
    "cat = catalogue_dict[name].copy()\n",
    "results_Mcut = Mc_cut_false_foreshock_file_dict[name].copy()\n",
    "ID = 10370141\n",
    "ID = 14433456\n",
    "mainshock_file = mainshock_dict[name].copy()\n",
    "mshock = statseis.iterable_mainshock(ID, mainshock_file)\n",
    "local_cat = statseis.create_local_catalogue(mainshock=mshock, earthquake_catalogue=cat, catalogue_name=name, radius_km=10)\n",
    "res_dict, file_dict, window_results = ESR_model(mainshock=mshock, earthquake_catalogue=cat, local_catalogue=local_cat, points_per_day=1)\n",
    "res_dict\n",
    "# window_results = ESR_model(mainshock=mshock, earthquake_catalogue=cat, local_catalogue=local_cat, points_per_day=1)\n",
    "# window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_catalogue_radius=10\n",
    "foreshock_window=20\n",
    "modelling_time_period=345\n",
    "\n",
    "local_cat_1yr = local_cat[(local_cat['DATETIME'] < mshock.DATETIME) &\\\n",
    "                          (local_cat['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                          (local_cat['DAYS_TO_MAINSHOCK'] > 0)  &\\\n",
    "                          (local_cat['DISTANCE_TO_MAINSHOCK'] < local_catalogue_radius) &\\\n",
    "                          (local_cat['ID'] != mshock.ID)].copy()\n",
    "\n",
    "M4s = local_cat.loc[(local_cat['MAGNITUDE']>=4) & (local_cat['DATETIME'] <= mshock.DATETIME)].copy()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.step(file_dict['sliding_window_points'], file_dict['sliding_window_counts'], color=plot_color_dict['pink'])\n",
    "ax.step(window_results['point'], window_results['count'], color=plot_color_dict['pink'])\n",
    "axtwin = ax.twinx()\n",
    "axtwin.scatter(local_cat_1yr['DAYS_TO_MAINSHOCK'], local_cat_1yr['MAGNITUDE'], alpha=0.9, ec='white', linewidth=0.25, c=local_cat_1yr['DAYS_TO_MAINSHOCK'])\n",
    "axtwin.scatter(M4s['DAYS_TO_MAINSHOCK'], M4s['MAGNITUDE'], alpha=0.9, ec='white', linewidth=0.25, marker='*', s=300)\n",
    "ax.axhline(res_dict['ESR_99CI'], color='grey', linestyle='--', alpha=0.7)\n",
    "axtwin.axhline(3, color='grey', linestyle='--', alpha=0.7)\n",
    "ax.axvline(20, color='grey', linestyle='--', alpha=0.7)\n",
    "axtwin.invert_xaxis()\n",
    "ax.set_xlabel('Days to mainshock')\n",
    "ax.set_ylabel('Count')\n",
    "axtwin.set_ylabel('Magnitude')\n",
    "# plt.savefig(f'../outputs/rough/{ID}_time_series_1yr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('Type A')\n",
    "metrics = ['tn_A', 'fp_A', 'fn_A', 'tp_A']\n",
    "type_A_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_A_cm[0, i] = res_dict[m]\n",
    "    else:\n",
    "        type_A_cm[1, i - 2] = res_dict[m]\n",
    "type_A_cm = type_A_cm.astype(int)\n",
    "type_A_cm\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_A_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "    \n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title('Type B')\n",
    "metrics = ['tn_B', 'fp_B', 'fn_B', 'tp_B']\n",
    "type_B_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_B_cm[0, i] = res_dict[m]\n",
    "    else:\n",
    "        type_B_cm[1, i - 2] = res_dict[m]\n",
    "type_B_cm = type_B_cm.astype(int)\n",
    "type_B_cm\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_B_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    for text in ax.texts:\n",
    "        text.set_fontsize(20) \n",
    "\n",
    "# plt.savefig(f'../outputs/rough/{ID}_cm_1yr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_results.loc[window_results['count']>res_dict['ESR_99CI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(window_results.loc[window_results['count']>=res_dict['ESR_99CI']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing for all mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'SCSN'\n",
    "# name = 'QTM_12'\n",
    "cat = catalogue_dict[name].copy()\n",
    "mshock_window_results = []\n",
    "for mshock in tqdm(good_mainshocks.itertuples(), total=len(good_mainshocks)):\n",
    "    local_cat = statseis.create_local_catalogue(mshock, earthquake_catalogue=cat, catalogue_name=name, radius_km=10)\n",
    "    res_dict, file_dict, window_results = ESR_model(mainshock=mshock, earthquake_catalogue=cat, local_catalogue=local_cat, points_per_day=1)\n",
    "    mshock_window_results.append(res_dict)\n",
    "mshock_window_results = pd.DataFrame.from_dict(mshock_window_results)\n",
    "mshock_window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('Type A')\n",
    "metrics = ['tn_A', 'fp_A', 'fn_A', 'tp_A']\n",
    "type_A_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_A_cm[0, i] = mshock_window_results[m].sum() \n",
    "    else:\n",
    "        type_A_cm[1, i - 2] = mshock_window_results[m].sum()\n",
    "type_A_cm = type_A_cm.astype(int)\n",
    "type_A_cm\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_A_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title('Type B')\n",
    "metrics = ['tn_B', 'fp_B', 'fn_B', 'tp_B']\n",
    "type_B_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_B_cm[0, i] = mshock_window_results[m].sum()\n",
    "    else:\n",
    "        type_B_cm[1, i - 2] = mshock_window_results[m].sum()\n",
    "type_B_cm = type_B_cm.astype(int)\n",
    "type_B_cm\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_B_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    for text in ax.texts:\n",
    "        text.set_fontsize(20) \n",
    "\n",
    "# plt.savefig('../outputs/rough/CM_all_mainshocks.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_dict = {'type_A':type_A_cm,\n",
    "           'type_B':type_B_cm}\n",
    "df_list = []\n",
    "for key, cm in cm_dict.items():\n",
    "    TN, FP, FN, TP =  cm.ravel()\n",
    "    metric_dict = {'TPR (sensitivity/recall)': TP/(TP+FN),\n",
    "                    'TNR (specificity)': TN/(TN+FP),\n",
    "                    'PPV (precision)': TP/(TP+FP),\n",
    "                    'NPV': TN/(TN+FN),\n",
    "                    'FPR': FP/(FP+TN),\n",
    "                    'FNR': FN/(TP+FN),\n",
    "                    'FDR': FP/(TP+FP),\n",
    "                    'ACC': (TP+TN)/(TP+FP+FN+TN)\n",
    "                    }\n",
    "    metric_dict = {k: round(m, 2) for k, m in metric_dict.items()}\n",
    "    metric_df = pd.DataFrame.from_dict(metric_dict, orient='index', columns=[key])\n",
    "    df_list.append(metric_df)\n",
    "\n",
    "metric_df = pd.concat(df_list, axis=1)\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['regular_M3s', 'foreshock_M3s', 'TP_days', 'FP_days', 'TN_days', 'FN_days', 'days']\n",
    "for m in metrics:\n",
    "    print(m, mshock_window_results[m].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_A_FDR = mshock_window_results['regular_M3s'].sum()/(mshock_window_results['regular_M3s'].sum() + mshock_window_results['foreshock_M3s'].sum())\n",
    "type_A_FDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_B_FP = len(mshock_window_results.loc[mshock_window_results['max_20day_rate']>=mshock_window_results['n_events_in_foreshock_window']])\n",
    "type_B_TP = len(mshock_window_results.loc[mshock_window_results['max_20day_rate']<mshock_window_results['n_events_in_foreshock_window']])\n",
    "type_B_FDR = type_B_FP/(type_B_TP + type_B_FP)\n",
    "type_B_FDR, type_B_FP, type_B_TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_B_FDR = mshock_window_results['FP_days'].sum()/(mshock_window_results['FP_days'].sum() + mshock_window_results['TP_days'].sum())\n",
    "type_B_FPR = mshock_window_results['FP_days'].sum()/(mshock_window_results['FP_days'].sum() + mshock_window_results['TN_days'].sum())\n",
    "type_B_FDR, type_B_FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(mshock_window_results['FP_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mshock_window_results['FP_days'].sum()/mshock_window_results['days'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For entire local catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESR_model_full_cats_v1(mainshock, earthquake_catalogue, local_catalogue, mainshock_mag=4, type_A_mag=3,\n",
    "                        local_catalogue_radius = 10, foreshock_window = 20, modelling_time_period=345, points_per_day=10, calibration_period=365):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate signals prior to mainshocks in a sliding window: seismicity rates, distances to mainshock.\n",
    "    \"\"\"\n",
    "    \n",
    "    mainshock_ID = mainshock.ID\n",
    "    mainshock_LON = mainshock.LON\n",
    "    mainshock_LAT = mainshock.LAT\n",
    "    mainshock_DATETIME = mainshock.DATETIME\n",
    "    mainshock_Mc = mainshock.Mc\n",
    "    mainshock_MAG = mainshock.MAGNITUDE\n",
    "\n",
    "    local_cat_all = local_catalogue.copy()\n",
    "\n",
    "    local_catalogue = local_catalogue[(local_catalogue['DATETIME'] < mainshock_DATETIME) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] > 0)  &\\\n",
    "                                        (local_catalogue['DISTANCE_TO_MAINSHOCK'] < local_catalogue_radius) &\\\n",
    "                                        (local_catalogue['ID'] != mainshock_ID)\n",
    "                                        ].copy()\n",
    "\n",
    "    regular_seismicity_period = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] >= foreshock_window)].copy()\n",
    "    foreshocks = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] < foreshock_window)].copy()\n",
    "\n",
    "    n_local_catalogue = len(local_catalogue)\n",
    "    n_regular_seismicity_events = len(regular_seismicity_period)\n",
    "    n_events_in_foreshock_window = len(foreshocks)\n",
    "    foreshock_distance = np.median(foreshocks['DISTANCE_TO_MAINSHOCK'])\n",
    "\n",
    "    catalogue_start_date = earthquake_catalogue['DATETIME'].iloc[0]\n",
    "    time_since_catalogue_start = (mainshock_DATETIME - catalogue_start_date).total_seconds()/3600/24\n",
    "    cut_off_day = math.floor(time_since_catalogue_start)\n",
    "    if cut_off_day > calibration_period:\n",
    "        cut_off_day = calibration_period\n",
    "        \n",
    "    range_scaler = 100\n",
    "    sliding_window_points = np.array(np.arange((-cut_off_day+foreshock_window)*range_scaler, -foreshock_window*range_scaler+1/range_scaler, 1))/range_scaler*-1\n",
    "\n",
    "    sliding_window_counts = np.array([len(regular_seismicity_period.loc[(regular_seismicity_period['DAYS_TO_MAINSHOCK'] > point) &\\\n",
    "                                                                    (regular_seismicity_period['DAYS_TO_MAINSHOCK'] <= (point + foreshock_window))]) for point in sliding_window_points])\n",
    "    sliding_window_distances = np.array([np.median(regular_seismicity_period.loc[(regular_seismicity_period['DAYS_TO_MAINSHOCK'] > point) &\\\n",
    "                                                                    (regular_seismicity_period['DAYS_TO_MAINSHOCK'] <= (point + foreshock_window)), 'DISTANCE_TO_MAINSHOCK']) for point in sliding_window_points])\n",
    "\n",
    "    try:\n",
    "        distance_probability = len(sliding_window_distances[sliding_window_distances >= foreshock_distance])/len(sliding_window_distances)\n",
    "    except:\n",
    "        distance_probability = float('nan')\n",
    "\n",
    "    try:\n",
    "        max_window = max(sliding_window_counts)\n",
    "    except:\n",
    "        max_window = float('nan')\n",
    "\n",
    "    if n_events_in_foreshock_window > max_window:\n",
    "        max_window_method = 0.0\n",
    "    elif n_events_in_foreshock_window <= max_window:\n",
    "        max_window_method = 1.0\n",
    "    else:\n",
    "        max_window_method = float('nan')\n",
    "\n",
    "    if (len(sliding_window_counts)==0) & (n_events_in_foreshock_window > 0):\n",
    "        sliding_window_probability = 0.00\n",
    "        sliding_window_99CI = float('nan')\n",
    "    elif (len(sliding_window_counts)==0) & (n_events_in_foreshock_window == 0):    \n",
    "        sliding_window_probability = 1.00\n",
    "        sliding_window_99CI = float('nan')\n",
    "    else:\n",
    "        sliding_window_probability = len(sliding_window_counts[sliding_window_counts >= n_events_in_foreshock_window])/len(sliding_window_counts)\n",
    "        sliding_window_99CI = np.percentile(sliding_window_counts,99)\n",
    "\n",
    "    cat_start_date = earthquake_catalogue['DATETIME'].iloc[0]\n",
    "    cat_end_date = earthquake_catalogue['DATETIME'].iloc[-1]\n",
    "\n",
    "    mshock_to_start = (mainshock_DATETIME - cat_start_date).total_seconds()/3600/24\n",
    "    mshock_to_end = (mainshock_DATETIME - cat_end_date).total_seconds()/3600/24\n",
    "\n",
    "    start_point = math.floor(mshock_to_start)-foreshock_window\n",
    "    end_point = math.ceil(mshock_to_end)\n",
    "\n",
    "    window_results = []\n",
    "    point = start_point\n",
    "    print('start_point, end_point', start_point, end_point)\n",
    "    while point>end_point:\n",
    "        print(point)\n",
    "        window = local_cat_all.loc[(local_cat_all['DAYS_TO_MAINSHOCK'] >= point) &\\\n",
    "                                               (local_cat_all['DAYS_TO_MAINSHOCK'] < (point + foreshock_window))].copy()\n",
    "        # if len(window)>0:\n",
    "        #     count = len(window)\n",
    "        #     mainshocks = len(window.loc[window['MAGNITUDE']>=mainshock_mag])\n",
    "        #     type_As = len(window.loc[(window['MAGNITUDE']>=type_A_mag) & (window['MAGNITUDE']<mainshock_mag)])\n",
    "\n",
    "        # else:\n",
    "        #     count, type_As, mainshocks = [0]*3\n",
    "        count = len(window)\n",
    "        mainshocks = len(window.loc[window['MAGNITUDE']>=mainshock_mag])\n",
    "        type_As = len(window.loc[(window['MAGNITUDE']>=type_A_mag) & (window['MAGNITUDE']<mainshock_mag)])\n",
    "        window_results.append({'point':point,\n",
    "                               'count':count,\n",
    "                               'type_As':type_As,\n",
    "                               'mainshocks':mainshocks})\n",
    "\n",
    "        point-=points_per_day\n",
    "    window_results = pd.DataFrame.from_dict(window_results)\n",
    "    print(window_results)\n",
    "    window_results['Actual'] = (window_results['mainshocks']>0)\n",
    "    window_results['type_A_predicted'] = window_results['type_As'] > 0\n",
    "    window_results['type_B_predicted'] = window_results['count'] > sliding_window_99CI\n",
    "\n",
    "    type_dict = {'A':{}, 'B':{}}\n",
    "    for key, item in type_dict.items():\n",
    "        TN, FP, FN, TP  = confusion_matrix(window_results['Actual'], window_results[f'type_{key}_predicted']).ravel()\n",
    "        type_dict[key] = {'TN':TN,\n",
    "                     'FP':FP,\n",
    "                     'FN':FN,\n",
    "                     'TP':TP,\n",
    "                     'TPR': TP/(TP+FN),\n",
    "                     'TNR': TN/(TN+FP),\n",
    "                     'PPV': TP/(TP+FP),\n",
    "                     'NPV': TN/(TN+FN),\n",
    "                     'FPR': FP/(FP+TN),\n",
    "                     'FNR': FN/(TP+FN),\n",
    "                     'FDR': FP/(TP+FP),\n",
    "                     'ACC': (TP+TN)/(TP+FP+FN+TN)\n",
    "                     }\n",
    "\n",
    "    results_dict = {'ID':mainshock_ID,\n",
    "                    'MAGNITUDE':mainshock_MAG,\n",
    "                    'LON':mainshock_LON,\n",
    "                    'LAT':mainshock_LAT,\n",
    "                    'DATETIME':mainshock_DATETIME,\n",
    "                    'DEPTH':mainshock.DEPTH,\n",
    "                    'Mc':mainshock_Mc,\n",
    "                    'time_since_catalogue_start':time_since_catalogue_start,\n",
    "                    'n_regular_seismicity_events':n_regular_seismicity_events,\n",
    "                    'n_events_in_foreshock_window':n_events_in_foreshock_window,\n",
    "                    'max_20day_rate':max_window,\n",
    "                    'ESR':sliding_window_probability,\n",
    "                    'ESR_99CI':sliding_window_99CI,\n",
    "                    'ESD':distance_probability,\n",
    "                    'cut_off_day':cut_off_day,\n",
    "                    'A_fp_rate':type_dict['A']['FPR'],\n",
    "                    'B_fp_rate':type_dict['B']['FPR'],\n",
    "                    'tn_A':type_dict['A']['TN'],\n",
    "                    'fp_A':type_dict['A']['FP'],\n",
    "                    'fn_A':type_dict['A']['FN'],\n",
    "                    'tp_A':type_dict['A']['TP'],\n",
    "                    'tn_B':type_dict['B']['TN'],\n",
    "                    'fp_B':type_dict['B']['FP'],\n",
    "                    'fn_B':type_dict['B']['FN'],\n",
    "                    'tp_B':type_dict['B']['TP']\n",
    "                    }\n",
    "    \n",
    "    file_dict = {'local_catalogue':local_catalogue,\n",
    "                #  'local_catalogue_pre_Mc_cutoff':local_catalogue_pre_Mc_cutoff,\n",
    "                #  'local_catalogue_below_Mc':local_catalogue_below_Mc,\n",
    "                 'foreshocks':foreshocks,\n",
    "                #  'foreshocks_below_Mc':foreshocks_below_Mc,\n",
    "                 'sliding_window_points':sliding_window_points,\n",
    "                 'sliding_window_counts':sliding_window_counts,\n",
    "                 'sliding_window_distances':sliding_window_distances\n",
    "                 }\n",
    "    \n",
    "    return results_dict, file_dict, window_results\n",
    "\n",
    "def ESR_model_full_cats_v2(mainshock, earthquake_catalogue, local_catalogue, mainshock_mag=4, type_A_mag=3,\n",
    "                        local_catalogue_radius = 10, foreshock_window = 20, modelling_time_period=345, points_per_day=10, calibration_period=365):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate signals prior to mainshocks in a sliding window: seismicity rates, distances to mainshock.\n",
    "    \"\"\"\n",
    "    \n",
    "    mainshock_ID = mainshock.ID\n",
    "    mainshock_LON = mainshock.LON\n",
    "    mainshock_LAT = mainshock.LAT\n",
    "    mainshock_DATETIME = mainshock.DATETIME\n",
    "    mainshock_Mc = mainshock.Mc\n",
    "    mainshock_MAG = mainshock.MAGNITUDE\n",
    "\n",
    "    local_cat_all = local_catalogue.copy()\n",
    "\n",
    "    local_catalogue = local_catalogue[(local_catalogue['DATETIME'] < mainshock_DATETIME) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] > 0)  &\\\n",
    "                                        (local_catalogue['DISTANCE_TO_MAINSHOCK'] < local_catalogue_radius) &\\\n",
    "                                        (local_catalogue['ID'] != mainshock_ID)\n",
    "                                        ].copy()\n",
    "\n",
    "    regular_seismicity_period = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] >= foreshock_window)].copy()\n",
    "    foreshocks = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] < foreshock_window)].copy()\n",
    "\n",
    "    n_local_catalogue = len(local_catalogue)\n",
    "    n_regular_seismicity_events = len(regular_seismicity_period)\n",
    "    n_events_in_foreshock_window = len(foreshocks)\n",
    "    foreshock_distance = np.median(foreshocks['DISTANCE_TO_MAINSHOCK'])\n",
    "\n",
    "    catalogue_start_date = earthquake_catalogue['DATETIME'].iloc[0]\n",
    "    time_since_catalogue_start = (mainshock_DATETIME - catalogue_start_date).total_seconds()/3600/24\n",
    "    cut_off_day = math.floor(time_since_catalogue_start)\n",
    "    if cut_off_day > calibration_period:\n",
    "        cut_off_day = calibration_period\n",
    "        \n",
    "    range_scaler = 100\n",
    "    sliding_window_points = np.array(np.arange((-cut_off_day+foreshock_window)*range_scaler, -foreshock_window*range_scaler+1/range_scaler, 1))/range_scaler*-1\n",
    "\n",
    "    sliding_window_counts = np.array([len(regular_seismicity_period.loc[(regular_seismicity_period['DAYS_TO_MAINSHOCK'] > point) &\\\n",
    "                                                                    (regular_seismicity_period['DAYS_TO_MAINSHOCK'] <= (point + foreshock_window))]) for point in sliding_window_points])\n",
    "    sliding_window_distances = np.array([np.median(regular_seismicity_period.loc[(regular_seismicity_period['DAYS_TO_MAINSHOCK'] > point) &\\\n",
    "                                                                    (regular_seismicity_period['DAYS_TO_MAINSHOCK'] <= (point + foreshock_window)), 'DISTANCE_TO_MAINSHOCK']) for point in sliding_window_points])\n",
    "\n",
    "    try:\n",
    "        distance_probability = len(sliding_window_distances[sliding_window_distances >= foreshock_distance])/len(sliding_window_distances)\n",
    "    except:\n",
    "        distance_probability = float('nan')\n",
    "\n",
    "    try:\n",
    "        max_window = max(sliding_window_counts)\n",
    "    except:\n",
    "        max_window = float('nan')\n",
    "\n",
    "    if n_events_in_foreshock_window > max_window:\n",
    "        max_window_method = 0.0\n",
    "    elif n_events_in_foreshock_window <= max_window:\n",
    "        max_window_method = 1.0\n",
    "    else:\n",
    "        max_window_method = float('nan')\n",
    "\n",
    "    if (len(sliding_window_counts)==0) & (n_events_in_foreshock_window > 0):\n",
    "        sliding_window_probability = 0.00\n",
    "        sliding_window_99CI = float('nan')\n",
    "    elif (len(sliding_window_counts)==0) & (n_events_in_foreshock_window == 0):    \n",
    "        sliding_window_probability = 1.00\n",
    "        sliding_window_99CI = float('nan')\n",
    "    else:\n",
    "        sliding_window_probability = len(sliding_window_counts[sliding_window_counts >= n_events_in_foreshock_window])/len(sliding_window_counts)\n",
    "        sliding_window_99CI = np.percentile(sliding_window_counts,99)\n",
    "\n",
    "    cat_start_date = earthquake_catalogue['DATETIME'].iloc[0]\n",
    "    cat_end_date = earthquake_catalogue['DATETIME'].iloc[-1]\n",
    "\n",
    "    mshock_to_start = (mainshock_DATETIME - cat_start_date).total_seconds()/3600/24\n",
    "    mshock_to_end = (mainshock_DATETIME - cat_end_date).total_seconds()/3600/24\n",
    "\n",
    "    start_point = math.floor(mshock_to_start)-foreshock_window\n",
    "    end_point = math.ceil(mshock_to_end)\n",
    "\n",
    "    window_results = []\n",
    "    point = start_point\n",
    "    while point>end_point:\n",
    "        window_20_back = local_cat_all.loc[(local_cat_all['DAYS_TO_MAINSHOCK'] >= point) &\\\n",
    "                                               (local_cat_all['DAYS_TO_MAINSHOCK'] < (point + foreshock_window))].copy()\n",
    "        window_20_forward = local_cat_all.loc[(local_cat_all['DAYS_TO_MAINSHOCK'] <= point) &\\\n",
    "                                               (local_cat_all['DAYS_TO_MAINSHOCK'] > (point - foreshock_window))].copy()\n",
    "        count = len(window_20_back)\n",
    "        type_As = window_20_back.loc[(window_20_back['MAGNITUDE']>=type_A_mag) & (window_20_back['MAGNITUDE']<mainshock_mag)].copy()\n",
    "        mainshocks = window_20_forward.loc[window_20_forward['MAGNITUDE']>=mainshock_mag].copy()\n",
    "        window_results.append({'point':point,\n",
    "                               'count':count,\n",
    "                               'type_As':len(type_As),\n",
    "                               'mainshocks':len(mainshocks)})\n",
    "        point-=points_per_day\n",
    "    window_results = pd.DataFrame.from_dict(window_results)\n",
    "    window_results['Actual'] = (window_results['mainshocks']>0)\n",
    "    window_results['type_A_predicted'] = window_results['type_As'] > 0\n",
    "    window_results['type_B_predicted'] = window_results['count'] > sliding_window_99CI\n",
    "\n",
    "    type_dict = {'A':{}, 'B':{}}\n",
    "    for key, item in type_dict.items():\n",
    "        TN, FP, FN, TP  = confusion_matrix(window_results['Actual'], window_results[f'type_{key}_predicted']).ravel()\n",
    "        type_dict[key] = {'TN':TN,\n",
    "                     'FP':FP,\n",
    "                     'FN':FN,\n",
    "                     'TP':TP,\n",
    "                     'TPR': TP/(TP+FN),\n",
    "                     'TNR': TN/(TN+FP),\n",
    "                     'PPV': TP/(TP+FP),\n",
    "                     'NPV': TN/(TN+FN),\n",
    "                     'FPR': FP/(FP+TN),\n",
    "                     'FNR': FN/(TP+FN),\n",
    "                     'FDR': FP/(TP+FP),\n",
    "                     'ACC': (TP+TN)/(TP+FP+FN+TN)\n",
    "                     }\n",
    "\n",
    "    results_dict = {'ID':mainshock_ID,\n",
    "                    'MAGNITUDE':mainshock_MAG,\n",
    "                    'LON':mainshock_LON,\n",
    "                    'LAT':mainshock_LAT,\n",
    "                    'DATETIME':mainshock_DATETIME,\n",
    "                    'DEPTH':mainshock.DEPTH,\n",
    "                    'Mc':mainshock_Mc,\n",
    "                    'time_since_catalogue_start':time_since_catalogue_start,\n",
    "                    'n_regular_seismicity_events':n_regular_seismicity_events,\n",
    "                    'n_events_in_foreshock_window':n_events_in_foreshock_window,\n",
    "                    'max_20day_rate':max_window,\n",
    "                    'ESR':sliding_window_probability,\n",
    "                    'ESR_99CI':sliding_window_99CI,\n",
    "                    'ESD':distance_probability,\n",
    "                    'cut_off_day':cut_off_day,\n",
    "                    'A_fp_rate':type_dict['A']['FPR'],\n",
    "                    'B_fp_rate':type_dict['B']['FPR'],\n",
    "                    'tn_A':type_dict['A']['TN'],\n",
    "                    'fp_A':type_dict['A']['FP'],\n",
    "                    'fn_A':type_dict['A']['FN'],\n",
    "                    'tp_A':type_dict['A']['TP'],\n",
    "                    'tn_B':type_dict['B']['TN'],\n",
    "                    'fp_B':type_dict['B']['FP'],\n",
    "                    'fn_B':type_dict['B']['FN'],\n",
    "                    'tp_B':type_dict['B']['TP']\n",
    "                    }\n",
    "    \n",
    "    file_dict = {'local_catalogue':local_catalogue,\n",
    "                #  'local_catalogue_pre_Mc_cutoff':local_catalogue_pre_Mc_cutoff,\n",
    "                #  'local_catalogue_below_Mc':local_catalogue_below_Mc,\n",
    "                 'foreshocks':foreshocks,\n",
    "                #  'foreshocks_below_Mc':foreshocks_below_Mc,\n",
    "                 'sliding_window_points':sliding_window_points,\n",
    "                 'sliding_window_counts':sliding_window_counts,\n",
    "                 'sliding_window_distances':sliding_window_distances\n",
    "                 }\n",
    "    \n",
    "    return results_dict, file_dict, window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'SCSN'\n",
    "name = 'QTM_12'\n",
    "cat = catalogue_dict[name].copy()\n",
    "results_Mcut = Mc_cut_false_foreshock_file_dict[name].copy()\n",
    "ID = 10370141\n",
    "# ID = 14433456\n",
    "ID = 37701544\n",
    "mshock = statseis.iterable_mainshock(ID, good_mainshocks)\n",
    "local_cat = statseis.create_local_catalogue(mainshock=mshock, earthquake_catalogue=cat, catalogue_name=name, radius_km=10)\n",
    "local_catalogue_radius=10\n",
    "foreshock_window=20\n",
    "modelling_time_period=3450\n",
    "res_dict, file_dict, window_results = ESR_model_full_cats_v2(mainshock=mshock, earthquake_catalogue=cat, local_catalogue=local_cat, modelling_time_period=modelling_time_period, points_per_day=1)\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M4s = local_cat.loc[(local_cat['MAGNITUDE']>=4)].copy()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.step(file_dict['sliding_window_points'], file_dict['sliding_window_counts'], color=plot_color_dict['pink'])\n",
    "ax.step(window_results['point'], window_results['count'], color=plot_color_dict['pink'])\n",
    "axtwin = ax.twinx()\n",
    "axtwin.scatter(local_cat['DAYS_TO_MAINSHOCK'], local_cat['MAGNITUDE'], alpha=0.9, ec='white', linewidth=0.25, c=local_cat['DAYS_TO_MAINSHOCK'])\n",
    "axtwin.scatter(M4s['DAYS_TO_MAINSHOCK'], M4s['MAGNITUDE'], alpha=0.9, ec='white', linewidth=0.25, marker='*', s=300)\n",
    "ax.axhline(res_dict['ESR_99CI'], color='brown', linestyle='--', alpha=0.7)\n",
    "axtwin.axhline(3, color='grey', linestyle='--', alpha=0.7)\n",
    "axtwin.axhline(4, color='grey', linestyle='--', alpha=0.7)\n",
    "ax.axvline(20, color='grey', linestyle='--', alpha=0.7)\n",
    "axtwin.invert_xaxis()\n",
    "ax.set_xlabel('Days to mainshock')\n",
    "ax.set_ylabel('Count')\n",
    "axtwin.set_ylabel('Magnitude')\n",
    "# plt.savefig('../outputs/rough/false_positives_full_cat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('Type A')\n",
    "metrics = ['tn_A', 'fp_A', 'fn_A', 'tp_A']\n",
    "type_A_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_A_cm[0, i] = res_dict[m]\n",
    "    else:\n",
    "        type_A_cm[1, i - 2] = res_dict[m]\n",
    "type_A_cm = type_A_cm.astype(int)\n",
    "type_A_cm\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_A_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "    \n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title('Type B')\n",
    "metrics = ['tn_B', 'fp_B', 'fn_B', 'tp_B']\n",
    "type_B_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_B_cm[0, i] = res_dict[m]\n",
    "    else:\n",
    "        type_B_cm[1, i - 2] = res_dict[m]\n",
    "type_B_cm = type_B_cm.astype(int)\n",
    "type_B_cm\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_B_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    for text in ax.texts:\n",
    "        text.set_fontsize(20) \n",
    "\n",
    "plt.savefig(f'../outputs/rough/{ID}_full_cat.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing aftershock signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESR_model_full_cats(mainshock, earthquake_catalogue, local_catalogue, mainshock_mag=4, type_A_mag=3,\n",
    "                        local_catalogue_radius = 10, foreshock_window = 20, modelling_time_period=345, points_per_day=10, calibration_period=365):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate signals prior to mainshocks in a sliding window: seismicity rates, distances to mainshock.\n",
    "    \"\"\"\n",
    "    \n",
    "    mainshock_ID = mainshock.ID\n",
    "    mainshock_LON = mainshock.LON\n",
    "    mainshock_LAT = mainshock.LAT\n",
    "    mainshock_DATETIME = mainshock.DATETIME\n",
    "    mainshock_Mc = mainshock.Mc\n",
    "    mainshock_MAG = mainshock.MAGNITUDE\n",
    "\n",
    "    local_cat_all = local_catalogue.copy()\n",
    "\n",
    "    local_catalogue = local_catalogue[(local_catalogue['DATETIME'] < mainshock_DATETIME) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] > 0)  &\\\n",
    "                                        (local_catalogue['DISTANCE_TO_MAINSHOCK'] < local_catalogue_radius) &\\\n",
    "                                        (local_catalogue['ID'] != mainshock_ID)\n",
    "                                        ].copy()\n",
    "\n",
    "    regular_seismicity_period = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] >= foreshock_window)].copy()\n",
    "    foreshocks = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] < foreshock_window)].copy()\n",
    "\n",
    "    n_local_catalogue = len(local_catalogue)\n",
    "    n_regular_seismicity_events = len(regular_seismicity_period)\n",
    "    n_events_in_foreshock_window = len(foreshocks)\n",
    "    foreshock_distance = np.median(foreshocks['DISTANCE_TO_MAINSHOCK'])\n",
    "\n",
    "    catalogue_start_date = earthquake_catalogue['DATETIME'].iloc[0]\n",
    "    time_since_catalogue_start = (mainshock_DATETIME - catalogue_start_date).total_seconds()/3600/24\n",
    "    cut_off_day = math.floor(time_since_catalogue_start)\n",
    "    if cut_off_day > calibration_period:\n",
    "        cut_off_day = calibration_period\n",
    "        \n",
    "    range_scaler = 100\n",
    "    sliding_window_points = np.array(np.arange((-cut_off_day+foreshock_window)*range_scaler, -foreshock_window*range_scaler+1/range_scaler, 1))/range_scaler*-1\n",
    "\n",
    "    sliding_window_counts = np.array([len(regular_seismicity_period.loc[(regular_seismicity_period['DAYS_TO_MAINSHOCK'] > point) &\\\n",
    "                                                                    (regular_seismicity_period['DAYS_TO_MAINSHOCK'] <= (point + foreshock_window))]) for point in sliding_window_points])\n",
    "    sliding_window_distances = np.array([np.median(regular_seismicity_period.loc[(regular_seismicity_period['DAYS_TO_MAINSHOCK'] > point) &\\\n",
    "                                                                    (regular_seismicity_period['DAYS_TO_MAINSHOCK'] <= (point + foreshock_window)), 'DISTANCE_TO_MAINSHOCK']) for point in sliding_window_points])\n",
    "\n",
    "    try:\n",
    "        distance_probability = len(sliding_window_distances[sliding_window_distances >= foreshock_distance])/len(sliding_window_distances)\n",
    "    except:\n",
    "        distance_probability = float('nan')\n",
    "\n",
    "    try:\n",
    "        max_window = max(sliding_window_counts)\n",
    "    except:\n",
    "        max_window = float('nan')\n",
    "\n",
    "    if n_events_in_foreshock_window > max_window:\n",
    "        max_window_method = 0.0\n",
    "    elif n_events_in_foreshock_window <= max_window:\n",
    "        max_window_method = 1.0\n",
    "    else:\n",
    "        max_window_method = float('nan')\n",
    "\n",
    "    if (len(sliding_window_counts)==0) & (n_events_in_foreshock_window > 0):\n",
    "        sliding_window_probability = 0.00\n",
    "        sliding_window_99CI = float('nan')\n",
    "    elif (len(sliding_window_counts)==0) & (n_events_in_foreshock_window == 0):    \n",
    "        sliding_window_probability = 1.00\n",
    "        sliding_window_99CI = float('nan')\n",
    "    else:\n",
    "        sliding_window_probability = len(sliding_window_counts[sliding_window_counts >= n_events_in_foreshock_window])/len(sliding_window_counts)\n",
    "        sliding_window_99CI = np.percentile(sliding_window_counts,99)\n",
    "\n",
    "    cat_start_date = earthquake_catalogue['DATETIME'].iloc[0]\n",
    "    cat_end_date = earthquake_catalogue['DATETIME'].iloc[-1]\n",
    "\n",
    "    mshock_to_start = (mainshock_DATETIME - cat_start_date).total_seconds()/3600/24\n",
    "    mshock_to_end = (mainshock_DATETIME - cat_end_date).total_seconds()/3600/24\n",
    "\n",
    "    start_point = math.floor(mshock_to_start)-foreshock_window\n",
    "    end_point = math.ceil(mshock_to_end)\n",
    "\n",
    "    window_results = []\n",
    "    point = start_point\n",
    "    print('start_point, end_point', start_point, end_point)\n",
    "    while point>end_point:\n",
    "        print(point)\n",
    "        window = local_cat_all.loc[(local_cat_all['DAYS_TO_MAINSHOCK'] >= point) &\\\n",
    "                                               (local_cat_all['DAYS_TO_MAINSHOCK'] < (point + foreshock_window))].copy()\n",
    "        # if len(window)>0:\n",
    "        #     count = len(window)\n",
    "        #     mainshocks = len(window.loc[window['MAGNITUDE']>=mainshock_mag])\n",
    "        #     type_As = len(window.loc[(window['MAGNITUDE']>=type_A_mag) & (window['MAGNITUDE']<mainshock_mag)])\n",
    "\n",
    "        # else:\n",
    "        #     count, type_As, mainshocks = [0]*3\n",
    "        count = len(window)\n",
    "        mainshocks = len(window.loc[window['MAGNITUDE']>=mainshock_mag])\n",
    "        type_As = len(window.loc[(window['MAGNITUDE']>=type_A_mag) & (window['MAGNITUDE']<mainshock_mag)])\n",
    "        window_results.append({'point':point,\n",
    "                               'count':count,\n",
    "                               'type_As':type_As,\n",
    "                               'mainshocks':mainshocks})\n",
    "\n",
    "        point-=points_per_day\n",
    "    window_results = pd.DataFrame.from_dict(window_results)\n",
    "    print(window_results)\n",
    "    window_results['Actual'] = (window_results['mainshocks']>0)\n",
    "    window_results['type_A_predicted'] = window_results['type_As'] > 0\n",
    "    window_results['type_B_predicted'] = window_results['count'] > sliding_window_99CI\n",
    "\n",
    "    type_dict = {'A':{}, 'B':{}}\n",
    "    for key, item in type_dict.items():\n",
    "        TN, FP, FN, TP  = confusion_matrix(window_results['Actual'], window_results[f'type_{key}_predicted']).ravel()\n",
    "        type_dict[key] = {'TN':TN,\n",
    "                     'FP':FP,\n",
    "                     'FN':FN,\n",
    "                     'TP':TP,\n",
    "                     'TPR': TP/(TP+FN),\n",
    "                     'TNR': TN/(TN+FP),\n",
    "                     'PPV': TP/(TP+FP),\n",
    "                     'NPV': TN/(TN+FN),\n",
    "                     'FPR': FP/(FP+TN),\n",
    "                     'FNR': FN/(TP+FN),\n",
    "                     'FDR': FP/(TP+FP),\n",
    "                     'ACC': (TP+TN)/(TP+FP+FN+TN)\n",
    "                     }\n",
    "\n",
    "    results_dict = {'ID':mainshock_ID,\n",
    "                    'MAGNITUDE':mainshock_MAG,\n",
    "                    'LON':mainshock_LON,\n",
    "                    'LAT':mainshock_LAT,\n",
    "                    'DATETIME':mainshock_DATETIME,\n",
    "                    'DEPTH':mainshock.DEPTH,\n",
    "                    'Mc':mainshock_Mc,\n",
    "                    'time_since_catalogue_start':time_since_catalogue_start,\n",
    "                    'n_regular_seismicity_events':n_regular_seismicity_events,\n",
    "                    'n_events_in_foreshock_window':n_events_in_foreshock_window,\n",
    "                    'max_20day_rate':max_window,\n",
    "                    'ESR':sliding_window_probability,\n",
    "                    'ESR_99CI':sliding_window_99CI,\n",
    "                    'ESD':distance_probability,\n",
    "                    'cut_off_day':cut_off_day,\n",
    "                    'A_fp_rate':type_dict['A']['FPR'],\n",
    "                    'B_fp_rate':type_dict['B']['FPR'],\n",
    "                    'tn_A':type_dict['A']['TN'],\n",
    "                    'fp_A':type_dict['A']['FP'],\n",
    "                    'fn_A':type_dict['A']['FN'],\n",
    "                    'tp_A':type_dict['A']['TP'],\n",
    "                    'tn_B':type_dict['B']['TN'],\n",
    "                    'fp_B':type_dict['B']['FP'],\n",
    "                    'fn_B':type_dict['B']['FN'],\n",
    "                    'tp_B':type_dict['B']['TP']\n",
    "                    }\n",
    "    \n",
    "    file_dict = {'local_catalogue':local_catalogue,\n",
    "                #  'local_catalogue_pre_Mc_cutoff':local_catalogue_pre_Mc_cutoff,\n",
    "                #  'local_catalogue_below_Mc':local_catalogue_below_Mc,\n",
    "                 'foreshocks':foreshocks,\n",
    "                #  'foreshocks_below_Mc':foreshocks_below_Mc,\n",
    "                 'sliding_window_points':sliding_window_points,\n",
    "                 'sliding_window_counts':sliding_window_counts,\n",
    "                 'sliding_window_distances':sliding_window_distances\n",
    "                 }\n",
    "    \n",
    "    return results_dict, file_dict, window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M4s = local_cat.loc[(local_cat['MAGNITUDE']>=4)].copy()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.step(file_dict['sliding_window_points'], file_dict['sliding_window_counts'], color=plot_color_dict['pink'])\n",
    "ax.step(window_results['point'], window_results['count'], color=plot_color_dict['pink'])\n",
    "axtwin = ax.twinx()\n",
    "axtwin.scatter(local_cat['DAYS_TO_MAINSHOCK'], local_cat['MAGNITUDE'], alpha=0.9, ec='white', linewidth=0.25, c=local_cat['DAYS_TO_MAINSHOCK'])\n",
    "axtwin.scatter(M4s['DAYS_TO_MAINSHOCK'], M4s['MAGNITUDE'], alpha=0.9, ec='white', linewidth=0.25, marker='*', s=300)\n",
    "ax.axhline(res_dict['ESR_99CI'], color='brown', linestyle='--', alpha=0.7)\n",
    "axtwin.axhline(3, color='grey', linestyle='--', alpha=0.7)\n",
    "axtwin.axhline(4, color='grey', linestyle='--', alpha=0.7)\n",
    "ax.axvline(20, color='grey', linestyle='--', alpha=0.7)\n",
    "axtwin.invert_xaxis()\n",
    "ax.set_xlabel('Days to mainshock')\n",
    "ax.set_ylabel('Count')\n",
    "axtwin.set_ylabel('Magnitude')\n",
    "# plt.savefig('../outputs/rough/false_positives_full_cat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('Type A')\n",
    "metrics = ['tn_A', 'fp_A', 'fn_A', 'tp_A']\n",
    "type_A_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_A_cm[0, i] = res_dict[m]\n",
    "    else:\n",
    "        type_A_cm[1, i - 2] = res_dict[m]\n",
    "type_A_cm = type_A_cm.astype(int)\n",
    "type_A_cm\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_A_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "    \n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title('Type B')\n",
    "metrics = ['tn_B', 'fp_B', 'fn_B', 'tp_B']\n",
    "type_B_cm = np.zeros((2, 2))\n",
    "for i, m in enumerate(metrics):\n",
    "    if i < 2:\n",
    "        type_B_cm[0, i] = res_dict[m]\n",
    "    else:\n",
    "        type_B_cm[1, i - 2] = res_dict[m]\n",
    "type_B_cm = type_B_cm.astype(int)\n",
    "type_B_cm\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=type_B_cm)\n",
    "disp.plot(ax=ax, values_format='d', colorbar=False)\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for (i, j), label in np.ndenumerate(labels):\n",
    "    ax.text(j-0.3, i-0.3, label, ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    for text in ax.texts:\n",
    "        text.set_fontsize(20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many magnitude 3 earthquakes are there that did not lead to mainshocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_Mc_cut_False = pd.merge(Mc_cut_false_foreshock_file_dict['SCSN'], Mc_cut_false_foreshock_file_dict['QTM_12'],\n",
    "                                how='inner', on='ID', suffixes=('_SCSN', '_QTM_12'))\n",
    "merged_Mc_cut_False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_Mc_cut_False['n_Wetzler_foreshocks_SCSN'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(SCSN.loc[(SCSN['MAGNITUDE']>=3) & (SCSN['DATETIME']>dt.datetime(2009,1,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M3_mainshocks = statseis.select_mainshocks(SCSN, catalogue_name='SCSN', mainshock_magnitude_threshold=3)\n",
    "M3_mainshocks = M3_mainshocks.loc[M3_mainshocks['DATETIME']>dt.datetime(2009,1,1)].copy()\n",
    "M3_mainshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(M3_mainshocks.loc[M3_mainshocks['Selection']=='Both']), len(M3_mainshocks.loc[M3_mainshocks['Selection']=='MDET']), len(M3_mainshocks.loc[M3_mainshocks['Selection']=='FET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDET_M3 = M3_mainshocks.loc[M3_mainshocks['Selection']=='Both'].copy()\n",
    "DDET_M3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many events above Mc do I have in the QTM catalog compared to the SCSN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_Mc_cut_False = pd.merge(Mc_cut_false_foreshock_file_dict['SCSN'], Mc_cut_false_foreshock_file_dict['QTM_12'],\n",
    "                                how='inner', on='ID', suffixes=('_SCSN', '_QTM_12'))\n",
    "\n",
    "merged_Mc_cut_True = pd.merge(Mc_cut_true_foreshock_file_dict['SCSN'], Mc_cut_true_foreshock_file_dict['QTM_12'],\n",
    "                                how='inner', on='ID', suffixes=('_SCSN', '_QTM_12'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "i,j = 4,2\n",
    "\n",
    "ax = fig.add_subplot(i,j,1)\n",
    "ax.set_title(panel_labels[0], loc='left')\n",
    "ax.scatter(good_mainshocks['Mc'], good_mainshocks['Mc_QTM_12'], ec='white', linewidth=0.5)\n",
    "ax.set_xlabel(f'SCSN $M_c$')\n",
    "ax.set_ylabel(f'QTM 12 $M_c$')\n",
    "ax.plot(range(0,3), range(0,3), label='1:1')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(i,j,2)\n",
    "ax.set_title(panel_labels[1], loc='left')\n",
    "good_mainshocks['delta_Mc'] = good_mainshocks['Mc']-good_mainshocks['Mc_QTM_12']\n",
    "x = good_mainshocks['delta_Mc']\n",
    "ax.hist(x, alpha=0.5, bins=utils.get_bins(x, nearest=0.25))\n",
    "ax.set_xlabel(f'$\\delta M_c$')\n",
    "ax.set_ylabel(f'N')\n",
    "\n",
    "ax = fig.add_subplot(i,j,3)\n",
    "ax.set_title(panel_labels[2], loc='left')\n",
    "ax.scatter(good_mainshocks['n_local_cat_1yr'], good_mainshocks['n_local_cat_1yr_QTM_12'], ec='white', linewidth=0.5)\n",
    "ax.set_xlabel(f'SCSN N')\n",
    "ax.set_ylabel(f'QTM 12 N')\n",
    "ax.plot(range(0,1000), range(0,1000), label='1:1')\n",
    "ax.legend()\n",
    "# ax.set_ylim(0,2500)\n",
    "\n",
    "ax = fig.add_subplot(i,j,4)\n",
    "ax.set_title(panel_labels[3], loc='left')\n",
    "good_mainshocks['delta_n_local_cat'] = good_mainshocks['n_local_cat_1yr_QTM_12']-good_mainshocks['n_local_cat_1yr']\n",
    "x = good_mainshocks['delta_n_local_cat']\n",
    "ax.hist(x, alpha=0.5, label='SCSN', bins=utils.get_bins(x, nearest=100))\n",
    "ax.set_xlabel(f'$\\delta$ local catalog n')\n",
    "ax.set_ylabel(f'N')\n",
    "# ax.set_xlim(0,2500)\n",
    "\n",
    "ax = fig.add_subplot(i,j,5)\n",
    "ax.set_title(panel_labels[4], loc='left')\n",
    "ax.scatter(merged_Mc_cut_True['n_regular_seismicity_events_SCSN'], merged_Mc_cut_True['n_regular_seismicity_events_QTM_12'], ec='white', linewidth=0.5)\n",
    "ax.set_xlabel(f'SCSN N>$M_c$')\n",
    "ax.set_ylabel(f'QTM 12 N>$M_c$')\n",
    "ax.plot(range(0,1000), range(0,1000), label='1:1')\n",
    "ax.legend()\n",
    "# ax.set_ylim(0,750)\n",
    "\n",
    "ax = fig.add_subplot(i,j,6)\n",
    "ax.set_title(panel_labels[5], loc='left')\n",
    "merged_Mc_cut_True['delta_n_over_Mc'] = merged_Mc_cut_True['n_regular_seismicity_events_QTM_12']-merged_Mc_cut_True['n_regular_seismicity_events_SCSN']\n",
    "x = merged_Mc_cut_True['delta_n_over_Mc']\n",
    "ax.hist(x, alpha=0.5, label='SCSN', bins=utils.get_bins(x, nearest=50))\n",
    "ax.set_xlabel(f'$\\delta$ n>M_c')\n",
    "ax.set_ylabel(f'N')\n",
    "# ax.set_xlim(0,750)\n",
    "\n",
    "ax = fig.add_subplot(i,j,7)\n",
    "ax.set_title(panel_labels[6], loc='left')\n",
    "good_mainshocks['n_local_cat_inc'] = 100*(good_mainshocks['n_local_cat_1yr_QTM_12']-good_mainshocks['n_local_cat_1yr'])/good_mainshocks['n_local_cat_1yr']\n",
    "x = np.array(good_mainshocks['n_local_cat_inc'])\n",
    "x = x[~np.isinf(x)]\n",
    "ax.hist(x, alpha=0.5, label='SCSN', bins=utils.get_bins(x, nearest=10))\n",
    "ax.set_xlabel(f'n local cat % increase')\n",
    "ax.set_ylabel(f'N')\n",
    "ax.legend()\n",
    "# ax.set_ylim(0,750)\n",
    "\n",
    "ax = fig.add_subplot(i,j,8)\n",
    "ax.set_title(panel_labels[7], loc='left')\n",
    "merged_Mc_cut_True['n_over_Mc_inc'] = 100*(merged_Mc_cut_True['n_regular_seismicity_events_QTM_12']-merged_Mc_cut_True['n_regular_seismicity_events_SCSN'])/merged_Mc_cut_True['n_regular_seismicity_events_SCSN']\n",
    "x = np.array(merged_Mc_cut_True['n_over_Mc_inc'])\n",
    "x = x[~np.isinf(x)]\n",
    "ax.hist(x, alpha=0.5, bins=utils.get_bins(x, nearest=10))\n",
    "ax.set_xlabel(f'n>M_c % increase')\n",
    "ax.set_ylabel(f'N')\n",
    "# ax.set_xlim(0,750)\n",
    "\n",
    "plt.savefig('../outputs/rough/Mc_catalog_comp_v2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "Mc_comp_dict = {'delta_Mc':{}, 'delta_n_local_cat':{}, 'delta_n_over_Mc':{}, 'n_over_Mc_inc':{}}\n",
    "for i, (key, item) in enumerate(Mc_comp_dict.items()):\n",
    "    try:\n",
    "        b = good_mainshocks[key]\n",
    "    except:\n",
    "        b = merged_Mc_cut_True[key]\n",
    "    sorted_data = np.sort(b)\n",
    "    # pdf = b/sum(b)\n",
    "    # cdf = np.cumsum(pdf)\n",
    "    cdf = np.arange(1, len(sorted_data) + 1)/len(b)\n",
    "\n",
    "    ax=fig.add_subplot(4,1,i+1)\n",
    "    ax.step(sorted_data, cdf, label=key)\n",
    "    percentiles = np.percentile(sorted_data, [25, 50, 75])\n",
    "    cdf_percentiles = np.array([0.25, 0.5, 0.75])\n",
    "    for percentile, cdf_percentile in zip(percentiles, cdf_percentiles):\n",
    "\n",
    "        plt.vlines(percentile, ymin=0, ymax=cdf_percentile, color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Annotate the percentiles\n",
    "        plt.text(percentile, cdf_percentile, f'{percentile:.2f}', ha='center', va='top', color='black', fontsize=20)\n",
    "    plt.yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    plt.xlabel(key)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in Mc_comp_dict.items():\n",
    "    try:\n",
    "        b = good_mainshocks[key]\n",
    "    except:\n",
    "        b = merged_Mc_cut_True[key]\n",
    "    b = np.array(b)\n",
    "    b = b[~np.isinf(b)]\n",
    "    item.update({'mean':np.mean(b),\n",
    "                'median':np.median(b),\n",
    "                'std':np.std(b),\n",
    "                'var':np.var(b),\n",
    "                'se':np.std(b)/np.sqrt(len(b)),\n",
    "                '5_per':np.percentile(b, 5),\n",
    "                '25_per':np.percentile(b, 25),\n",
    "                '50_per':np.percentile(b, 50),\n",
    "                '75_per':np.percentile(b, 75),\n",
    "                '95_per':np.percentile(b, 95),\n",
    "                'min':b.min(),\n",
    "                'max':b.max()\n",
    "                })\n",
    "\n",
    "Mc_comp_df = pd.DataFrame.from_dict(Mc_comp_dict)\n",
    "Mc_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(good_mainshocks['delta_Mc'])\n",
    "print(len(x[x<0])/len(x), len(x[x==0])/len(x), len(x[(x>0) & (x<0.25)])/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(good_mainshocks['delta_Mc'])\n",
    "len(x[(x>-0.25) & (x<0.25)])/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(merged_Mc_cut_True['delta_n_over_Mc'])\n",
    "len(x[(x>-30) & (x<30)])/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(merged_Mc_cut_True['n_over_Mc_inc'])\n",
    "# conds = [x<0, (x>=0) & (x<10), (x>=10) & (x<50), (x>=50)]\n",
    "# for cond in conds:\n",
    "    # print(cond, len(x[conds])/len(x))\n",
    "len(x[(x<0)])/len(x), len(x[(x>=0) & (x<10)])/len(x), len(x[(x>=10) & (x<50)])/len(x), len(x[(x>=50)])/len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we see foreshocks before more mainshocks as $M_{c}$ decreases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_mainshocks.loc[abs(good_mainshocks['delta_Mc'])<0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_Mc_mainshocks = good_mainshocks.loc[good_mainshocks['delta_Mc']>=0.25].copy()\n",
    "print(len(lower_Mc_mainshocks))\n",
    "lower_Mc_mainshocks[['n_local_cat_1yr' , 'Mc', 'n_local_cat_1yr_QTM_12' , 'Mc_QTM_12']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_names =['QTM_9_5', 'QTM_12', 'SCSN']\n",
    "results_tables_list = []\n",
    "lower_Mc_mainshocks_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, IDs=lower_Mc_mainshocks['ID'], Mc_cut=True,\n",
    "                                             )\n",
    "    lower_Mc_mainshocks_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "lower_Mc_mainshocks_results = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "lower_Mc_mainshocks_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_names =['QTM_9_5', 'QTM_12', 'SCSN']\n",
    "results_tables_list = []\n",
    "lower_Mc_mainshocks_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, IDs=lower_Mc_mainshocks['ID'], Mc_cut=False,\n",
    "                                             )\n",
    "    lower_Mc_mainshocks_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "lower_Mc_mainshocks_results = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "lower_Mc_mainshocks_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are my BP and G-IET Gamma distributions/fits the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_results = pd.read_csv('../outputs/QTM_12/preferred_results.csv')\n",
    "utils.string_to_datetime_df(preferred_results)\n",
    "preferred_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preferred_results[['ID', 'y_gam_IETs', 'loc_gam_IETs', 'mu_gam_IETs', 'background_rate']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mu'] = df['background_rate']*20\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_law_MLE(t):\n",
    "    \"\"\"\n",
    "    Calculate background seismicity rate based on the interevent time distribution. From CORSSA (originally in MATLAB), changed to Python (by me).\n",
    "    \"\"\"\n",
    "    dt = np.diff(t)\n",
    "    dt = dt[dt>0]\n",
    "    T = sum(dt)\n",
    "    N = len(dt)\n",
    "    S = sum(np.log(dt))\n",
    "    dg = 10**-4\n",
    "    gam = np.arange(dg, 1-dg, dg) # increment from dg to 1-dg with a step of dg (dg:dg:dg-1 in matlab)\n",
    "    ell = N*gam*(1-np.log(N)+np.log(T)-np.log(gam))+N*special.loggamma(gam)-gam*S # scipy gamma funcion\n",
    "    ell_min = np.amin(ell)\n",
    "    i = np.where(ell == ell_min)\n",
    "    gam=gam[i]\n",
    "    mu=N/T*gam\n",
    "    return mu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = [10319593, 14481152, 10148002, 14692972, 14850084, 15017828, 15220393]\n",
    "ID = IDs[2]\n",
    "\n",
    "# catalogue_name = 'QTM_12'\n",
    "catalogue_name = 'SCSN'\n",
    "earthquake_catalogue = catalogue_dict[catalogue_name].copy()\n",
    "mainshock_file = mainshock_dict[catalogue_name].copy()\n",
    "mainshock = mainshock_file.loc[mainshock_file['ID']==ID]\n",
    "RowTuple = namedtuple('RowTuple', mainshock.columns)\n",
    "mainshock = [RowTuple(*row) for row in mainshock.values][0]\n",
    "local_cat = statseis.create_local_catalogue(mainshock, earthquake_catalogue, catalogue_name=catalogue_name)\n",
    "local_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = statseis.datetime_to_decimal_days(local_cat['DATETIME'])\n",
    "IET = np.diff(time_series) ### V&As Gamma IET method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(IET))\n",
    "# plt.hist(IET)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_law_MLE(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.diff(time_series)\n",
    "dt = dt[dt>0]\n",
    "T = sum(dt)\n",
    "N = len(dt)\n",
    "S = sum(np.log(dt))\n",
    "dg = 10**-4\n",
    "gam = np.arange(dg, 1-dg, dg) # increment from dg to 1-dg with a step of dg (dg:dg:dg-1 in matlab)\n",
    "ell = N*gam*(1-np.log(N)+np.log(T)-np.log(gam))+N*special.loggamma(gam)-gam*S # scipy gamma funcion\n",
    "ell_min = np.amin(ell)\n",
    "i = np.where(ell == ell_min)\n",
    "gam=gam[i]\n",
    "print(gam)\n",
    "mu=N/T*gam\n",
    "mu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam = np.arange(dg, 1-dg, dg) # increment from dg to 1-dg with a step of dg (dg:dg:dg-1 in matlab)\n",
    "len(gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell, ell_min, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.log(ell))\n",
    "plt.axvline(np.log(ell_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_, loc_, mu_ = stats.gamma.fit(IET, floc=0)\n",
    "y_, loc_, mu_, 1/mu_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_alt = N/T*y_\n",
    "mu_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreshock_window, iterations = 20, 50000\n",
    "t_day = 3600 * 24.0\n",
    "t_win = foreshock_window * t_day\n",
    "IET = IET[IET>0]\n",
    "try:\n",
    "    y_, loc_, mu_ = stats.gamma.fit(IET, floc=0.0)\n",
    "except:\n",
    "    y_, loc_, mu_ = stats.gamma.fit(IET, loc=0.0)\n",
    "# print(f\"y_ {y_}, loc_ {loc_}, mu_ {mu_}\")\n",
    "\n",
    "if (np.isnan(y_)==False) & (np.isnan(mu_)==False):\n",
    "    N_eq = np.zeros(iterations, dtype=int) # Buffer for the number of earthquakes observed in each random sample\n",
    "    for i in range(0,iterations):\n",
    "        prev_size = 200 # Generate a random IET sample with 200 events\n",
    "        IET2 = stats.gamma.rvs(a=y_, loc=0, scale=mu_, size=prev_size) * t_day # Sample from gamma distribution\n",
    "        t0 = np.random.rand() * IET2[0] # Random shift of timing of first event\n",
    "        t_sum = np.cumsum(IET2) - t0 # Cumulative sum of interevent times\n",
    "        inds = (t_sum > t_win) # Find the events that lie outside t_win\n",
    "        while (inds.sum() == 0):\n",
    "            prev_size *= 2 # If no events lie outside t_win, create a bigger sample and stack with previous sample\n",
    "            IET2 = np.hstack([IET2, stats.gamma.rvs(a=y_, loc=0, scale=mu_, size=prev_size) * t_day])\n",
    "            t_sum = np.cumsum(IET2) # Cumulative sum of event times\n",
    "            inds = (t_sum > t_win) # Find the events that lie outside t_win\n",
    "        N_inside_t_win = (~inds).sum()\n",
    "        if N_inside_t_win == 0: \n",
    "            N_eq[i] = 0 # No events inside t_win, seismicity rate = 0.\n",
    "        else:\n",
    "            N_eq[i] =  N_inside_t_win - 1 # Store the number of events that lie within t_win (excluding shifted event)\n",
    "\n",
    "    try:\n",
    "        y_gam_IETs, loc_gam_IETs, mu_gam_IETs = stats.gamma.fit(N_eq[N_eq > 0], floc=0.0)\n",
    "    except:\n",
    "        y_gam_IETs, loc_gam_IETs, mu_gam_IETs = stats.gamma.fit(N_eq[N_eq > 0], loc=0.0)\n",
    "\n",
    "print(y_, loc_, mu_)\n",
    "print(y_gam_IETs, loc_gam_IETs, mu_gam_IETs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_1 = np.var(IET)/np.mean(IET)\n",
    "gamma_1 = (np.mean(IET)**2)/np.var(IET)\n",
    "C_1 = (mu_1**gamma_1)/gamma(gamma_1)\n",
    "print(mu_1, gamma_1, C_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam = np.arange(0,1.1,0.1)\n",
    "gam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(IET)\n",
    "T = IET.sum()\n",
    "start, step, end = 0, 0.01, 1\n",
    "gamma_1 = np.arange(start+step, end+step, step)\n",
    "l = N*gamma_1*(1-np.log(N*gamma_1/T)) + N*np.log(gamma(gamma_1)) - gamma_1*sum(np.log(IET))\n",
    "# ell = N*gam*(1-np.log(N)+np.log(T)-np.log(gam))+N*special.loggamma(gam)-gam*S # scipy gamma funcion\n",
    "plt.hist(l)\n",
    "l_min = l.min()\n",
    "i = np.where(l == l_min)\n",
    "plt.axvline(l_min)\n",
    "print(l, l_min, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different significance levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_names =['QTM_9_5', 'QTM_12', 'SCSN']\n",
    "results_tables_list = []\n",
    "Mc_cut_false_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, significance_level=0.05)\n",
    "    Mc_cut_false_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "Mc_cut_false_subset_results = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "# Mc_cut_false_subset_results.to_csv(f'../outputs/figures/main_results.csv', index=False)\n",
    "Mc_cut_false_subset_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables_list = []\n",
    "Mc_cut_true_foreshock_file_dict = {}\n",
    "for catalogue_name in catalogue_names:\n",
    "    res_tab, res_file = create_results_table(catalogue_name=catalogue_name, Mc_cut=True, significance_level=0.05)\n",
    "    Mc_cut_true_foreshock_file_dict.update({catalogue_name:res_file})\n",
    "    results_tables_list.append(res_tab)\n",
    "Mc_cut_true_subset_results = reduce(lambda df1, df2: pd.merge(df1, df2, on='Count type'), results_tables_list)\n",
    "# Mc_cut_true_subset_results.to_csv(f'../outputs/figures/main_results_Mc_cut.csv', index=False)\n",
    "Mc_cut_true_subset_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How often can I not fit Gamma distributions to the IETs for the BP and G-IET models, and are my assumptions valid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCSN_f = pd.read_csv('../data/SCSN/foreshocks/default_params_240429.csv')\n",
    "QTM_12_f = pd.read_csv('../data/QTM_12/foreshocks/default_params_240429.csv')\n",
    "QTM_9_5_f = pd.read_csv('../data/QTM_9_5/foreshocks/default_params_240429.csv')\n",
    "df = pd.merge(pd.merge(SCSN_f, QTM_12_f, on='ID', how='inner', suffixes=('', '_QTM_12')), QTM_9_5_f, on='ID', how='inner', suffixes=('', '_QTM_9_5'))\n",
    "# df['mp_inc'] = (df['n_regular_seismicity_events_QTM_12']+1)/(df['n_regular_seismicity_events']+1)\n",
    "# df['fw_inc'] = (df['n_events_in_foreshock_window_QTM_12']+1)/(df['n_events_in_foreshock_window']+1)\n",
    "df['mp_inc'] = df['n_regular_seismicity_events_QTM_12']/df['n_regular_seismicity_events']\n",
    "df['fw_inc'] = df['n_events_in_foreshock_window_QTM_12']/df['n_events_in_foreshock_window']\n",
    "df['fw-mp'] = df['fw_inc'] - df['mp_inc']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['mp_inc']==np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['fw_inc']==np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c = 1,2\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(r,c,1)\n",
    "\n",
    "plot_df = df.loc[(df['mp_inc']!=np.inf) & (df['fw_inc']!=np.inf)].copy()\n",
    "ax.hist(plot_df['mp_inc'], label='mp', alpha=0.6, bins=utils.get_bins(plot_df['mp_inc'], nearest=1))\n",
    "ax.hist(plot_df['fw_inc'], label='fw', alpha=0.6, bins=utils.get_bins(plot_df['fw_inc'], nearest=1))\n",
    "ax.legend()\n",
    "ax.set_ylabel('N')\n",
    "ax.set_xlabel(f'% increase')\n",
    "\n",
    "ks_statistic, p_value = ks_2samp(plot_df['mp_inc'], plot_df['fw_inc'])\n",
    "print(ks_statistic, p_value)\n",
    "\n",
    "ax = fig.add_subplot(r,c,2)\n",
    "\n",
    "ax.hist(plot_df['fw-mp'], label='mp - fw', alpha=0.6, bins=utils.get_bins(plot_df['fw-mp'], nearest=1))\n",
    "ax.set_ylabel('N')\n",
    "ax.set_xlabel(f'fw - mp')\n",
    "plt.savefig('../outputs/rough/QTM_bias.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10 = df.loc[df['n_regular_seismicity_events']<10].copy()\n",
    "print(len(l10), len(l10.loc[l10['n_regular_seismicity_events_QTM_12']<10]))\n",
    "[['n_regular_seismicity_events', 'n_events_in_foreshock_window', 'G-IET',\n",
    "                 'n_regular_seismicity_events_QTM_12', 'n_events_in_foreshock_window_QTM_12', 'G-IET_QTM_12']].sort_values(by='G-IET')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10[['n_regular_seismicity_events', 'n_events_in_foreshock_window', 'BP',\n",
    "                 'n_regular_seismicity_events_QTM_12', 'n_events_in_foreshock_window_QTM_12', 'BP_QTM_12']].sort_values(by='BP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = l10.sort_values(by=['n_regular_seismicity_events', 'n_regular_seismicity_events_QTM_12'])\n",
    "plot_df.reset_index(inplace=True, drop=True)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.scatter(plot_df.index, plot_df['n_regular_seismicity_events'], marker='s')\n",
    "ax.scatter(plot_df.index, plot_df['n_regular_seismicity_events_QTM_12'], marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c = 2, 2\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "i=1\n",
    "for model in ['BP', 'G-IET', 'ESR']:\n",
    "    ax = fig.add_subplot(r,c,i)\n",
    "    plot_df = l10.sort_values(by=[f'{model}', f'{model}_QTM_12'], ascending=False)\n",
    "    plot_df.reset_index(inplace=True, drop=True)\n",
    "    ax.scatter(plot_df.index, plot_df[f'{model}'], ec='white', alpha=0.8)\n",
    "    ax.scatter(plot_df.index, plot_df[f'{model}_QTM_12'], ec='white', alpha=0.8)\n",
    "    i+=1\n",
    "\n",
    "    # ax = fig.add_subplot(r,c,2)\n",
    "    # ax.scatter(l10.index, l10['BP'])\n",
    "    # ax.scatter(l10.index, l10['BP_QTM_12'])\n",
    "    # ax = fig.add_subplot(r,c,3)\n",
    "    # ax.scatter(l10.index, l10['ESR'])\n",
    "    # ax.scatter(l10.index, l10['ESR_QTM_12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_increases = l10.loc[l10['n_regular_seismicity_events_QTM_12']>=10]\n",
    "seven_increases[['n_regular_seismicity_events', 'n_events_in_foreshock_window', 'G-IET',\n",
    "                 'n_regular_seismicity_events_QTM_12', 'n_events_in_foreshock_window_QTM_12', 'G-IET_QTM_12']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10[['n_regular_seismicity_events', 'n_events_in_foreshock_window', 'ESR', 'n_regular_seismicity_events_QTM_12', 'n_events_in_foreshock_window_QTM_12', 'ESR_QTM_12', 'n_regular_seismicity_events_QTM_9_5', 'n_events_in_foreshock_window_QTM_9_5', 'ESR_QTM_9_5']].sort_values(by='ESR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10['n_regular_seismicity_events'].median(), l10['n_regular_seismicity_events_QTM_12'].median(), l10['n_regular_seismicity_events_QTM_9_5'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10[['ESR', 'ESR_QTM_12']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(l10.loc[l10['n_regular_seismicity_events_QTM_12']>l10['n_regular_seismicity_events']]))\n",
    "# l10.loc[l10['n_regular_seismicity_events_QTM_12']>l10['n_regular_seismicity_events']]\n",
    "print(len(l10.loc[l10['ESR_QTM_12']>l10['ESR']]))\n",
    "l10.loc[l10['ESR_QTM_12']>l10['ESR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(l10.loc[l10['n_regular_seismicity_events_QTM_12']==l10['n_regular_seismicity_events']]))\n",
    "# l10.loc[l10['n_regular_seismicity_events_QTM_12']==l10['n_regular_seismicity_events']]\n",
    "print(len(l10.loc[l10['ESR_QTM_12']==l10['ESR']]))\n",
    "l10.loc[l10['ESR_QTM_12']==l10['ESR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(l10.loc[l10['ESR_QTM_12']<l10['ESR']]))\n",
    "l10.loc[l10['ESR_QTM_12']<l10['ESR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[~(df['ID'].isin(l10['ID'])) & (df['background_rate'].isna())]\n",
    "# df.loc[~(df['ID'].isin(l10['ID'])) & (df['y_gam_IETs'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['n_regular_seismicity_events']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_IET_nans = df.loc[df['y_gam_IETs'].isna()].copy()\n",
    "print(len(G_IET_nans))\n",
    "G_IET_nans.sort_values(by='G-IET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_nans = df.loc[df['background_rate'].isna()].copy()\n",
    "print(len(BP_nans))\n",
    "BP_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESR_nans = df.loc[df['ESR_99CI'].isna()].copy()\n",
    "ESR_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma_5 = G_IET_nans.loc[~(G_IET_nans['ID'].isin(BP_nans['ID']))]\n",
    "Gamma_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10.loc[~l10['ID'].isin(list(BP_nans['ID']) + list(G_IET_nans['ID']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how many modelling events in each catalog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCSN_f = pd.read_csv('../data/SCSN/foreshocks/default_params_240429.csv')\n",
    "QTM_12_f = pd.read_csv('../data/QTM_12/foreshocks/default_params_240429.csv')\n",
    "QTM_9_5_f = pd.read_csv('../data/QTM_9_5/foreshocks/default_params_240429.csv')\n",
    "df = pd.merge(pd.merge(SCSN_f, QTM_12_f, on='ID', how='inner', suffixes=('', '_QTM_12')), QTM_9_5_f, on='ID', how='inner', suffixes=('', '_QTM_9_5'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.loc[df['n_regular_seismicity_events_QTM_9_5']<400].copy()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['n_regular_seismicity_events', 'n_regular_seismicity_events_QTM_12', 'n_regular_seismicity_events_QTM_9_5']:\n",
    "    x = df.loc[df[col]<400].copy()\n",
    "    print(col, len(x))\n",
    "    plt.hist(x[col], alpha=0.5, bins=utils.get_bins(x[col], nearest=20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An error in counts\n",
    "Pretty much solved, there is a small differene in how distance is calculated from the mainshock between my select mainshocks and identify foreshocks functions.\n",
    "It causes 1 extra earthquake to be included in the local catalogue of one mainshock, even though it occurs 10.004 km from the mainshock epicentre.\n",
    "Solve later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ten_events = preferred_results.loc[preferred_results['local_catalogue_length']<10].copy()\n",
    "print(len(sub_ten_events))\n",
    "sub_ten_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ten_events[['local_catalogue_length', 'n_regular_seismicity_events', 'n_events_in_foreshock_window']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches = sub_ten_events.loc[sub_ten_events['local_catalogue_length']!=sub_ten_events['n_regular_seismicity_events']+sub_ten_events['n_events_in_foreshock_window']].copy()\n",
    "mismatches[['local_catalogue_length', 'n_regular_seismicity_events', 'n_events_in_foreshock_window']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainshock_file = mismatches.copy()\n",
    "earthquake_catalogue=QTM_12.copy()\n",
    "catalogue_name='QTM_12'\n",
    "iterations=10000\n",
    "local_catalogue_radius = 10 # km \n",
    "search_style='radius'\n",
    "search_radius = 10 # km\n",
    "foreshock_window = 20 # days\n",
    "modelling_time_period=365 # days\n",
    "significance_level = 0.01 # (1%)\n",
    "Mc_cutoff=False\n",
    "local_catalogues = []\n",
    "for mainshock in mismatches.itertuples():\n",
    "    print(mainshock.ID)\n",
    "    # print(f\"{count} / '{len(mainshock_file_min_obs_time)} mainshocks\")\n",
    "    # count +=1\n",
    "            \n",
    "    # print(catalogue_name)\n",
    "    # print(f\"\\r {count} of {len(mainshock_file)}\")\n",
    "    # print(f\"\\r    mainshock ID: {mainshock.ID}\")\n",
    "\n",
    "    try:\n",
    "        local_catalogue = pd.read_csv('../data/' + catalogue_name + '/local_catalogues/' + str(mainshock.ID) + '.csv')\n",
    "        utils.string_to_datetime_df(local_catalogue)\n",
    "        print(f\"len(local_catalogue): {len(local_catalogue)}\")\n",
    "        # print(\"    succesfully loaded in data\")\n",
    "        local_catalogue_2 = local_catalogue.copy()\n",
    "        local_catalogues.append(local_catalogue_2)\n",
    "        print('appended')\n",
    "    except:\n",
    "        # print(\"    data not found - creating\")\n",
    "\n",
    "        box_halfwidth_km = search_radius\n",
    "        min_box_lon, min_box_lat = utils.add_distance_to_position_pyproj(mainshock.LON, mainshock.LAT, -box_halfwidth_km, -box_halfwidth_km)\n",
    "        max_box_lon, max_box_lat = utils.add_distance_to_position_pyproj(mainshock.LON, mainshock.LAT, box_halfwidth_km, box_halfwidth_km)\n",
    "\n",
    "        local_catalogue = earthquake_catalogue.loc[\n",
    "                                        (earthquake_catalogue['LON']>= min_box_lon) &\\\n",
    "                                        (earthquake_catalogue['LON']<= max_box_lon) &\\\n",
    "                                        (earthquake_catalogue['LAT']>= min_box_lat) &\\\n",
    "                                        (earthquake_catalogue['LAT']<= max_box_lat)\n",
    "                                        ].copy()\n",
    "\n",
    "        local_catalogue['DAYS_TO_MAINSHOCK'] = (mainshock.DATETIME - local_catalogue['DATETIME']).apply(lambda d: (d.total_seconds()/(24*3600)))\n",
    "\n",
    "        local_catalogue['DISTANCE_TO_MAINSHOCK'] = statseis.calculate_distance_pyproj_vectorized(mainshock.LON, mainshock.LAT, local_catalogue['LON'], local_catalogue['LAT'])\n",
    "        # local_catalogue['DISTANCE_TO_MAINSHOCK'] = calculate_distances_haversine_vect(mainshock.LON, mainshock.LAT, local_catalogue['LON'],  local_catalogue['LAT'])\n",
    "        print(f\"len(local_catalogue): {len(local_catalogue)}\")\n",
    "            \n",
    "    if search_style=='radius':\n",
    "        local_catalogue = local_catalogue[(local_catalogue['DATETIME'] < mainshock.DATETIME) &\\\n",
    "                                                    (local_catalogue['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                                                    (local_catalogue['DAYS_TO_MAINSHOCK'] > 0)  &\\\n",
    "                                                    (local_catalogue['DISTANCE_TO_MAINSHOCK'] < local_catalogue_radius) &\\\n",
    "                                                    (local_catalogue['ID'] != mainshock.ID)\n",
    "                                                    ].copy()\n",
    "        print(f\"len(local_catalogue): {len(local_catalogue)}\")\n",
    "\n",
    "            \n",
    "    # elif (search_style=='box') & (local_catalogue_radius!=search_radius):\n",
    "    elif (search_style=='box'):\n",
    "        box_halfwidth_km = search_radius\n",
    "        min_box_lon, min_box_lat = utils.add_distance_to_position_pyproj(mainshock.LON, mainshock.LAT, -box_halfwidth_km, -box_halfwidth_km)\n",
    "        max_box_lon, max_box_lat = utils.add_distance_to_position_pyproj(mainshock.LON, mainshock.LAT, box_halfwidth_km, box_halfwidth_km)\n",
    "\n",
    "        local_catalogue = local_catalogue.loc[\n",
    "                                        (local_catalogue['LON']>= min_box_lon) &\\\n",
    "                                        (local_catalogue['LON']<= max_box_lon) &\\\n",
    "                                        (local_catalogue['LAT']>= min_box_lat) &\\\n",
    "                                        (local_catalogue['LAT']<= max_box_lat) &\\\n",
    "                                        (local_catalogue['DATETIME'] < mainshock.DATETIME) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                                            (local_catalogue['DAYS_TO_MAINSHOCK'] > 0)  &\\\n",
    "                                        (local_catalogue['ID'] != mainshock.ID)\n",
    "                                        ].copy()\n",
    "    print(f\"len(local_catalogue_1yr): {len(local_catalogue)}\")\n",
    "    \n",
    "    # print(f\"    len(local_catalogue) {len(local_catalogue)}\")\n",
    "        \n",
    "    local_catalogue_pre_Mc_cutoff = local_catalogue.copy()\n",
    "    \n",
    "    try:\n",
    "        Mc = round(Mc_by_maximum_curvature(local_catalogue['MAGNITUDE']),2)\n",
    "    except:\n",
    "        Mc = float('nan')\n",
    "    # print(f\"    Mc {Mc}\")\n",
    "    \n",
    "    local_catalogue_below_Mc = local_catalogue.loc[local_catalogue['MAGNITUDE']<Mc].copy()\n",
    "    foreshocks_below_Mc = local_catalogue_below_Mc.loc[local_catalogue_below_Mc['DAYS_TO_MAINSHOCK']<foreshock_window]\n",
    "\n",
    "    # local_catalogue_below_Mc = local_catalogue_below_Mc.loc[(local_catalogue_below_Mc['DAYS_TO_MAINSHOCK']) < modelling_time_period].copy()\n",
    "    \n",
    "    if Mc_cutoff==True:\n",
    "        local_catalogue = local_catalogue.loc[local_catalogue['MAGNITUDE']>=Mc].copy()\n",
    "    else:\n",
    "        local_catalogue = local_catalogue_pre_Mc_cutoff.copy()\n",
    "    \n",
    "    regular_seismicity_period = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] >= foreshock_window)]\n",
    "    foreshocks = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] < foreshock_window)]\n",
    "    \n",
    "    n_local_catalogue_pre_Mc_cutoff = len(local_catalogue_pre_Mc_cutoff)\n",
    "    n_local_catalogue = len(local_catalogue)\n",
    "    n_local_catalogue_below_Mc = len(local_catalogue_below_Mc)\n",
    "    n_regular_seismicity_events = len(regular_seismicity_period)\n",
    "    n_events_in_foreshock_window = len(foreshocks)\n",
    "    print(f\"len(local_catalogue_1yr): {n_regular_seismicity_events+n_events_in_foreshock_window} ({n_regular_seismicity_events}, {n_events_in_foreshock_window})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_catalogue=QTM_12.copy()\n",
    "catalogue_name='QTM_12'\n",
    "search_style='radius'\n",
    "search_distance_km=10\n",
    "mainshock_magnitude_threshold = 4\n",
    "minimum_exclusion_distance = 20\n",
    "scaling_exclusion_distance = 5\n",
    "minimum_exclusion_time = 50\n",
    "scaling_exclusion_time = 25\n",
    "foreshock_window = 20 # days\n",
    "modelling_time_period=365 \n",
    "for mainshock in mismatches.itertuples():\n",
    "        print(mainshock.ID)\n",
    "        min_box_lon, min_box_lat = utils.add_distance_to_position_pyproj(mainshock.LON, mainshock.LAT, -search_distance_km, -search_distance_km)\n",
    "        max_box_lon, max_box_lat = utils.add_distance_to_position_pyproj(mainshock.LON, mainshock.LAT, search_distance_km, search_distance_km)\n",
    "\n",
    "        local_catalogue = earthquake_catalogue.loc[\n",
    "                                        (earthquake_catalogue['LON']>= min_box_lon) &\\\n",
    "                                        (earthquake_catalogue['LON']<= max_box_lon) &\\\n",
    "                                        (earthquake_catalogue['LAT']>= min_box_lat) &\\\n",
    "                                        (earthquake_catalogue['LAT']<= max_box_lat)\n",
    "                                        ].copy()\n",
    "        \n",
    "        if search_style=='radius':\n",
    "            local_catalogue['DISTANCE_TO_MAINSHOCK'] = statseis.calculate_distance_pyproj_vectorized(mainshock.LON, mainshock.LAT, local_catalogue['LON'],  local_catalogue['LAT'])\n",
    "            local_catalogue = local_catalogue[(local_catalogue['DISTANCE_TO_MAINSHOCK'] < search_distance_km)].copy()    \n",
    "            print(f\"len(local_catalogue): {len(local_catalogue)}\")\n",
    "            local_catalogue_1 = local_catalogue.copy()\n",
    "            local_catalogues.append(local_catalogue_1)\n",
    "            print('appended')\n",
    "\n",
    "        elif search_style=='box':\n",
    "            print(f\"A box has been chosen, even though a box allows a distance of 14 km between mainshock epicentre and box corner.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Invalid search style - we are going to craaaaash\")\n",
    "\n",
    "        local_catalogue_1yr = local_catalogue[(local_catalogue.DATETIME < mainshock.DATETIME) &\\\n",
    "                                        ((mainshock.DATETIME - local_catalogue.DATETIME) < dt.timedelta(days=modelling_time_period+foreshock_window)) &\\\n",
    "                                        (local_catalogue['ID'] != mainshock.ID)\n",
    "                                        ].copy()\n",
    "\n",
    "        len_local_catalogue = len(local_catalogue_1yr)\n",
    "        print(f\"len(local_catalogue_1yr): {len_local_catalogue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_catalogues_1 = local_catalogues[2].copy()\n",
    "local_catalogues_1.reset_index(inplace=True, drop=True)\n",
    "local_catalogues_1_IDs = local_catalogues_1['ID'].values\n",
    "local_catalogues_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_catalogues_2 = local_catalogues[0].copy()\n",
    "local_catalogues_2.reset_index(inplace=True, drop=True)\n",
    "local_catalogues_2_IDs = local_catalogues_2['ID'].values\n",
    "local_catalogues_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_catalogues_2.loc[np.where(~np.in1d(local_catalogues_2_IDs, local_catalogues_1_IDs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESR vs ETAS Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moutote_mainshock_ETAS_params = pd.read_csv('../data/Moutote/Moutote_ETAS_params.txt')\n",
    "Moutote_mainshock_ETAS_params.rename(columns={'#mainshockID':'ID'}, inplace=True)\n",
    "Moutote_mainshock_ETAS_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESR_results_10e = pd.read_csv('../outputs/QTM_12/ESR_results_10e.csv')\n",
    "utils.string_to_datetime_df(ESR_results_10e)\n",
    "print(len(ESR_results_10e.loc[ESR_results_10e['ESR']<0.01]))\n",
    "ESR_results_10e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FET_only = pd.read_csv(f'../data/{catalogue_name}/foreshocks/FET_only_all_cats.csv')\n",
    "utils.string_to_datetime_df(FET_only)\n",
    "FET_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_Moutote = pd.concat([ESR_results_10e, FET_only])\n",
    "My_Moutote.reset_index(inplace=True, drop=True)\n",
    "My_Moutote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moutote_mainshock_IDs = np.array(Moutote_mainshock_ETAS_params['ID'])\n",
    "# recreation_foreshock_results = pd.merge(ESR_results_10e, Moutote_mainshock_ETAS_params, on='ID')\n",
    "recreation_foreshock_results = pd.merge(My_Moutote, Moutote_mainshock_ETAS_params, on='ID')\n",
    "recreation_foreshock_results = recreation_foreshock_results[['ID', 'n_regular_seismicity_events', 'n_events_in_foreshock_window', 'ESR', 'p_ETASexpect_2', 'p_ETASexpect_f']].copy()\n",
    "print(len(recreation_foreshock_results))\n",
    "recreation_foreshock_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESR_foreshocks = recreation_foreshock_results.loc[(recreation_foreshock_results['ESR']<0.01)].copy()\n",
    "len(ESR_foreshocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moutote_foreshocks = recreation_foreshock_results.loc[(recreation_foreshock_results['p_ETASexpect_f']<0.01) & (recreation_foreshock_results['p_ETASexpect_2']<0.01)].copy()\n",
    "Moutote_foreshocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_both = ESR_foreshocks.loc[ESR_foreshocks['ID'].isin(Moutote_foreshocks['ID'])].copy()\n",
    "in_mine_not_Mouts = ESR_foreshocks.loc[~ESR_foreshocks['ID'].isin(Moutote_foreshocks['ID'])].copy()\n",
    "in_Mouts_not_mine = Moutote_foreshocks.loc[~Moutote_foreshocks['ID'].isin(ESR_foreshocks['ID'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(in_both), len(in_mine_not_Mouts), len(in_Mouts_not_mine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_mine_not_Mouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_Mouts_not_mine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Moutote's p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_value in ['p_ETASexpect_2', 'p_ETASexpect_f', 'p_declust_2', 'p_declust_f']:\n",
    "    print(f\"{p_value}: {len(Moutote_mainshock_ETAS_params.loc[Moutote_mainshock_ETAS_params[p_value]<0.01])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "i=1\n",
    "for p_value in ['p_ETASexpect_2', 'p_ETASexpect_f', 'p_declust_2', 'p_declust_f']:\n",
    "    ax = fig.add_subplot(2,2,i)\n",
    "    ax.set_title(p_value)\n",
    "    print(f\"{p_value}: {len(Moutote_mainshock_ETAS_params.loc[Moutote_mainshock_ETAS_params[p_value]<0.01])}\")\n",
    "    ax.hist(Moutote_mainshock_ETAS_params[p_value])\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moutote_mainshock_ETAS_params.loc[Moutote_mainshock_ETAS_params['p_ETASexpect_f']<0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreation_foreshock_results.loc[(recreation_foreshock_results['p_ETASexpect_f']<0.01) & ~(recreation_foreshock_results['p_ETASexpect_2']<0.01)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreation_foreshock_results.loc[~(recreation_foreshock_results['p_ETASexpect_f']<0.01) & (recreation_foreshock_results['p_ETASexpect_2']<0.01)].copy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does the R-IET model give non-zero probabilitites for negative seismicity rates?\n",
    "I don't know, but V&As data does it too, so it's not my problem to solve.\n",
    "\n",
    "Think this section must have some code from V&S's that is being stored over. Null for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=50000\n",
    "local_catalogue_radius = 10 # km \n",
    "search_style='radius'\n",
    "search_radius = 10 # km\n",
    "foreshock_window = 20 # days\n",
    "# modelling_time_period=365 # days\n",
    "modelling_time_period=360 # days\n",
    "significance_level = 0.01 # (1%)\n",
    "observation_time_scalar=4\n",
    "Wetzler_cutoff=3\n",
    "t_day = 3600 * 24.0 \n",
    "t_win = foreshock_window * t_day    \n",
    "count = 1\n",
    "detailed_sampling_dict ={}\n",
    "IDs_with_neg_seis_rates = []\n",
    "\n",
    "mainshock_ID = 15481673\n",
    "\n",
    "earthquake_catalogue = pd.read_csv(f\"../../catalogues/reformatted/QTM_9_5_reformat.csv\")\n",
    "utils.string_to_datetime_df(earthquake_catalogue)\n",
    "mainshock = earthquake_catalogue.loc[earthquake_catalogue['ID']==mainshock_ID].iloc[0]\n",
    "local_catalogue = pd.read_csv(f\"../data/QTM_9_5/local_catalogues/{mainshock_ID}.csv\")\n",
    "utils.string_to_datetime_df(local_catalogue)\n",
    "local_catalogue = local_catalogue.loc[(local_catalogue['DATETIME'] < mainshock.DATETIME) &\\\n",
    "                                      (local_catalogue['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] > 0)  &\\\n",
    "                                            (local_catalogue['ID'] != mainshock.ID)\n",
    "                                            ].copy()\n",
    "print(f\"local_catalogue: {len(local_catalogue)}\")\n",
    "\n",
    "foreshocks = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] < foreshock_window)].copy()\n",
    "n_events_in_foreshock_window = len(foreshocks)\n",
    "\n",
    "regular_seismicity_period = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] >= foreshock_window)].copy()\n",
    "print(f\"regular_seismicity_period: {len(regular_seismicity_period)}\")\n",
    "time_series = np.array(regular_seismicity_period['DATETIME'].apply(lambda d: (d-regular_seismicity_period['DATETIME'].iloc[0]).total_seconds()/3600/24))\n",
    "IET = np.diff(time_series) # Note: no need for /t_day as I already do it above (/3600/24)\n",
    "IET = IET[IET>0]\n",
    "print(f\"len IETs: {len(IET)}\")\n",
    "# IET = IET/t_day # a step I was missing... how has this been working without this?\n",
    "y_, loc_, mu_ = stats.gamma.fit(IET, floc=0.0)\n",
    "\n",
    "# try:\n",
    "#     y_, loc_, mu_ = stats.gamma.fit(IET, floc=0.0)\n",
    "# except:\n",
    "#     y_, loc_, mu_ = stats.gamma.fit(IET, loc=0.0)\n",
    "print(f\"y_ {y_}, loc_ {loc_}, mu_ {mu_}\")\n",
    "\n",
    "catalogue_start_date = earthquake_catalogue['DATETIME'].iloc[0]\n",
    "time_since_catalogue_start = (mainshock.DATETIME - catalogue_start_date).total_seconds()/3600/24\n",
    "cut_off_day = math.floor(time_since_catalogue_start)\n",
    "if cut_off_day > 365:\n",
    "    cut_off_day = 365\n",
    "range_scaler = 100    \n",
    "\n",
    "event_counts_in_x_days = []\n",
    "# upper_time_limit = math.ceil(max(time_series)) - foreshock_window # wrong, went from 1st earthquake, not from 1 year prior to mainshock or start of catalogue (if less than 1 year) \n",
    "upper_time_limit = cut_off_day - foreshock_window \n",
    "\n",
    "print(\"     Creating ESR & G-IET models.\")\n",
    "N_eq = np.zeros(iterations, dtype=int) # Buffer for the number of earthquakes observed in each random sample\n",
    "loop_dict = []\n",
    "for i in range(0,iterations):\n",
    "    \n",
    "    random_point = dt.timedelta(days=random.random()*upper_time_limit)\n",
    "    \n",
    "    random_sample = regular_seismicity_period.loc[((mainshock.DATETIME - regular_seismicity_period['DATETIME']) < (random_point + dt.timedelta(days=foreshock_window))) &\\\n",
    "                            ((mainshock.DATETIME - regular_seismicity_period['DATETIME']) > random_point)]\n",
    "    \n",
    "    event_counts_in_x_days.append(len(random_sample))\n",
    "        \n",
    "    ## V&A IET method\n",
    "    prev_size = 200 # Generate a random IET sample with 200 events\n",
    "    IET2 = stats.gamma.rvs(a=y_, loc=0, scale=mu_, size=prev_size) * t_day # Sample from gamma distribution\n",
    "    random_number = np.random.rand()\n",
    "    t0 = random_number * IET2[0]\n",
    "    # t0 = np.random.rand() * IET2[0] # Random shift of timing of first event\n",
    "    t_sum = np.cumsum(IET2) - t0 # Cumulative sum of interevent times\n",
    "    inds = (t_sum > t_win) # Find the events that lie outside t_win\n",
    "    while (inds.sum() == 0):\n",
    "        prev_size *= 2 # If no events lie outside t_win, create a bigger sample and stack with previous sample\n",
    "        IET2 = np.hstack([IET2, stats.gamma.rvs(a=y_, loc=0, scale=mu_, size=prev_size) * t_day])\n",
    "        t_sum = np.cumsum(IET2) # Cumulative sum of event times\n",
    "        inds = (t_sum > t_win) # Find the events that lie outside t_win\n",
    "    # N_eq[i] = (~inds).sum() - 1 # Store the number of events that lie within t_win (excluding shifted event)\n",
    "    seismicity_rate = (~inds).sum() - 1\n",
    "    N_eq[i] =  seismicity_rate # this is the problem line, if there are zero events, then the count becomes -1\n",
    "    # currently the model does not predict a seismicity rate of 0, this is wrong, this is the fix\n",
    "\n",
    "    if N_eq[i] <0:\n",
    "        print(f\"{i}: negative seismicity rate, changing to 0\")\n",
    "        IDs_with_neg_seis_rates.append(mainshock_ID)\n",
    "        # N_eq[i] = 0\n",
    "        # raise KeyboardInterrupt\n",
    "    loop_dict.append({\"random_number\":random_number,\n",
    "                      \"IET2_0\":IET2[0]/t_day,\n",
    "                      \"t0\":t0/t_day,\n",
    "                      \"n_outside_t_win\":(t_sum > t_win).sum(),\n",
    "                      \"seismicity_rate\":seismicity_rate#,\n",
    "                #  \"while_counter\":while_counter,\n",
    "                #  \"prev_size\":prev_size,\n",
    "                #  \"inds.sum()\":inds.sum()\n",
    "                    })\n",
    "\n",
    "loop_df = pd.DataFrame.from_dict(loop_dict)\n",
    "\n",
    "try:\n",
    "    y_gam_IETs, loc_gam_IETs, mu_gam_IETs = gamma.fit(N_eq[N_eq > 0], floc=0.0)\n",
    "except:\n",
    "    y_gam_IETs, loc_gam_IETs, mu_gam_IETs = gamma.fit(N_eq[N_eq > 0], loc=0.0)\n",
    "\n",
    "print(f\"y_gam_IETs {y_gam_IETs}, loc_gam_IETs {loc_gam_IETs}, mu_gam_IETs {mu_gam_IETs}\")\n",
    "VA_gamma_probability = gamma.sf(n_events_in_foreshock_window, y_gam_IETs, loc_gam_IETs, mu_gam_IETs)\n",
    "VA_gamma_99CI = gamma.ppf(0.99, a=y_gam_IETs, loc=loc_gam_IETs, scale=mu_gam_IETs)\n",
    "VA_IETs_probability = len(N_eq[N_eq>=n_events_in_foreshock_window])/iterations\n",
    "VA_IETs_99CI = np.percentile(N_eq,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 2, 2\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "\n",
    "i=1\n",
    "ax = fig.add_subplot(rows, cols, i)\n",
    "ax.hist(IET, density=True, alpha=0.5, label='IETs')\n",
    "ax.hist(IET2/t_day, density=True, \n",
    "        alpha=0.5, label='IET2: 200 samples')\n",
    "x_gam_IETs = np.arange(gamma.ppf(0.001, a=y_, loc=loc_, scale=mu_),\n",
    "                       gamma.ppf(0.999, a=y_, loc=loc_, scale=mu_))\n",
    "gamma_pdf = gamma.pdf(x_gam_IETs, a=y_, loc=loc_, scale=mu_)\n",
    "ax.plot(x_gam_IETs, gamma_pdf, label=f'$\\mu$ = {round(1/mu_,2)}', color='green')\n",
    "# p_tail = [float('0.9999' + '9' * i) for i in range(4)]\n",
    "# x_tail = [gamma.ppf(p_tail, a=y_, loc=loc_, scale=mu_) for x in p_tail]\n",
    "# y_tail = [gamma.pdf(x, a=y_, loc=loc_, scale=mu_) for x in x_tail]\n",
    "# ax.plot(np.append(x_gam_IETs, x_tail), np.append(gamma_pdf, y_tail), label=f'$\\mu$ = {round(1/mu_,2)}', color='green')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('PDF')\n",
    "ax.set_xlabel('IET (days)')\n",
    "# ax.set_ylim(10**-2, 10**0)\n",
    "ax.legend()\n",
    "\n",
    "i+=1\n",
    "ax = fig.add_subplot(rows, cols, i)\n",
    "ax.hist(t_sum/t_day,  \n",
    "        alpha=0.5, label=f't_sum: cumsum(IET2)')\n",
    "# ax.set_yscale('log')\n",
    "ax.axvline(x=t0/t_day, color='grey', label=f't0: {round(random_number,2)} * IET2[0] = {round(t0/t_day,1)} (days)', alpha=0.9)\n",
    "ax.axvline(x=t_win/t_day, color='red', label=f't_win: {t_win/t_day} (days)', alpha=0.5, linestyle='--')\n",
    "ax.axvline(x=IET2[0]/t_day, label=f\"IET2[0]: {round(IET2[0]/t_day,1)} (days)\", color='green')\n",
    "ax.axhline(y=seismicity_rate, label=f'(events <= t_win) - 1 = {seismicity_rate}')\n",
    "ax.set_ylabel('N')\n",
    "ax.set_xlabel('cumsum(IET2) (days)')\n",
    "ax.legend()\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# i+=1\n",
    "# ax = fig.add_subplot(rows, cols, i)\n",
    "# ax.hist(IET, density=True, alpha=0.5, label='IETs')\n",
    "# ax.hist(IET2/t_day, density=True, \n",
    "#         alpha=0.5, label='IET2: 200 samples')\n",
    "# x_gam_IETs = np.arange(gamma.ppf(0.001, a=y_, loc=loc_, scale=mu_),\n",
    "#                        gamma.ppf(0.999, a=y_, loc=loc_, scale=mu_))\n",
    "# gamma_pdf = gamma.pdf(x_gam_IETs, a=y_, loc=loc_, scale=mu_)\n",
    "# # ax.plot(x_gam_IETs, gamma_pdf, label=f'$\\mu$ = {round(1/mu_,2)}', color='green')\n",
    "# p_tail = [float('0.9999' + '9' * i) for i in range(4)]\n",
    "# x_tail = [gamma.ppf(p_tail, a=y_, loc=loc_, scale=mu_) for x in p_tail]\n",
    "# y_tail = [gamma.pdf(x, a=y_, loc=loc_, scale=mu_) for x in x_tail]\n",
    "# ax.plot(np.append(x_gam_IETs, x_tail), np.append(gamma_pdf, y_tail), label=f'$\\mu$ = {round(1/mu_,2)}', color='green')\n",
    "\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_ylabel('PDF')\n",
    "# ax.set_xlabel('IET (days)')\n",
    "# # ax.set_ylim(10**-2, 10**0)\n",
    "# ax.legend()\n",
    "\n",
    "plt.savefig(f\"../outputs/rough/good_gamma.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_IET2 = IET2\n",
    "# bad_tsum = t_sum\n",
    "# bad_t0 = t0\n",
    "# bad_rand = random_number\n",
    "# bad_seismicity_rate = seismicity_rate\n",
    "bad_seismicity_rate = -1\n",
    "\n",
    "rows, cols = 2, 2 \n",
    "fig = plt.figure(figsize=(16,12))\n",
    "\n",
    "i=1\n",
    "ax = fig.add_subplot(rows, cols ,i)\n",
    "ax.hist(IET, density=True, alpha=0.5, label='IETs') #don't need /tday for IET as I do it further up\n",
    "ax.hist(bad_IET2/t_day, density=True, \n",
    "        alpha=0.5, label='IET2: 200 samples')\n",
    "x_gam_IETs = np.arange(gamma.ppf(0.0001, a=y_, loc=loc_, scale=mu_),\n",
    "                       gamma.ppf(0.9999, a=y_, loc=loc_, scale=mu_))\n",
    "gamma_pdf = gamma.pdf(x_gam_IETs, a=y_, loc=loc_, scale=mu_)\n",
    "ax.plot(x_gam_IETs, gamma_pdf, label=f'$\\mu$ = {round(1/mu_,2)}', color='green')\n",
    "# p_tail = [float('0.999' + '9' * i) for i in range(4)]\n",
    "# x_tail = [gamma.ppf(p_tail, a=y_, loc=loc_, scale=mu_) for x in p_tail]\n",
    "# y_tail = [gamma.pdf(x, a=y_, loc=loc_, scale=mu_) for x in x_tail]\n",
    "# ax.plot(np.append(x_gam_IETs, x_tail), np.append(gamma_pdf, y_tail), label=f'$\\mu$ = {round(1/mu_,2)}', color='green')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('PDF')\n",
    "ax.set_xlabel('IET (days)')\n",
    "# ax.set_ylim(10**-2, 10**0)\n",
    "ax.legend()\n",
    "\n",
    "i+=1\n",
    "ax = fig.add_subplot(rows, cols ,i)\n",
    "ax.hist(bad_tsum/t_day,  \n",
    "        alpha=0.5, label=f't_sum: cumsum(IET2)')\n",
    "# ax.set_yscale('log')\n",
    "ax.axvline(x=bad_t0/t_day, color='grey', label=f't0: {round(bad_rand,2)} * IET2[0] = {round(bad_t0/t_day,1)} (days)', alpha=0.9)\n",
    "ax.axvline(x=t_win/t_day, color='red', label=f't_win: {t_win/t_day} (days)', alpha=0.5, linestyle='--')\n",
    "ax.axvline(x=bad_IET2[0]/t_day, label=f\"IET2[0]: {round(bad_IET2[0]/t_day,1)} (days)\", color='green')\n",
    "ax.axhline(y=bad_seismicity_rate, label=f'(events <= t_win) - 1 = {bad_seismicity_rate}')\n",
    "ax.set_ylabel('N')\n",
    "ax.set_xlabel('cumsum(IET2) (days)')\n",
    "ax.set_xscale('log')\n",
    "ax.legend()\n",
    "\n",
    "# i+=1\n",
    "# ax = fig.add_subplot(rows, cols ,i)\n",
    "# ax.hist(IET, density=True, alpha=0.5, label='IETs') #don't need /tday for IET as I do it further up\n",
    "# ax.hist(bad_IET2/t_day, density=True, \n",
    "#         alpha=0.5, label='IET2: 200 samples')\n",
    "# x_gam_IETs = np.arange(gamma.ppf(0.0001, a=y_, loc=loc_, scale=mu_),\n",
    "#                        gamma.ppf(0.9999, a=y_, loc=loc_, scale=mu_))\n",
    "# gamma_pdf = gamma.pdf(x_gam_IETs, a=y_, loc=loc_, scale=mu_)\n",
    "# # ax.plot(x_gam_IETs, gamma_pdf, label=f'$\\mu$ = {round(1/mu_,2)}', color='green')\n",
    "# p_tail = [float('0.9999' + '9' * i) for i in range(4)]\n",
    "# x_tail = [gamma.ppf(p_tail, a=y_, loc=loc_, scale=mu_) for x in p_tail]\n",
    "# y_tail = [gamma.pdf(x, a=y_, loc=loc_, scale=mu_) for x in x_tail]\n",
    "# ax.plot(np.append(x_gam_IETs, x_tail), np.append(gamma_pdf, y_tail), label=f'$\\mu$ = {round(1/mu_,2)}', color='green')\n",
    "\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_ylabel('PDF')\n",
    "# ax.set_xlabel('IET (days)')\n",
    "# # ax.set_ylim(10**-2, 10**0)\n",
    "# ax.legend()\n",
    "\n",
    "plt.savefig(f\"../outputs/rough/bad_gamma.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neggy_rates = loop_df.loc[loop_df['seismicity_rate']<0].copy()\n",
    "neggy_rates\n",
    "\n",
    "pozzy_rates = loop_df.loc[loop_df['seismicity_rate']>=0].copy()\n",
    "\n",
    "rows, cols = 3,3\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "\n",
    "i=1\n",
    "for column in loop_df.columns:\n",
    "    ax = fig.add_subplot(rows, cols ,i)\n",
    "    # ax.hist(loop_df[column], alpha=0.3, density=True, label='All rates')\n",
    "    ax.hist(pozzy_rates[column], alpha=0.5, density=True, label='+ve rates')\n",
    "    ax.hist(neggy_rates[column], alpha=0.5, density=True, label='-ve rates')\n",
    "    if column=='t_sum_0':\n",
    "        ax.axvline(x=3600 * 24.0 * 20, color='red')\n",
    "    if column in ['n_outside_t_win', 'seismicity_rate']:\n",
    "        ax.set_yscale('log')\n",
    "    # if column in ['IET2_0', 't0']:\n",
    "    #     ax.set_xscale('log')\n",
    "    ax.set_title(column, loc='right')\n",
    "    ax.legend()\n",
    "    i+=1\n",
    "\n",
    "ax = fig.add_subplot(rows, cols ,i)\n",
    "ax.hist(IET, density=True, alpha=0.5, label='IETs') #don't need /tday for IET as I do it further up\n",
    "ax.hist(bad_IET2/t_day, density=True, \n",
    "        alpha=0.5, label='IET2: 200 samples')\n",
    "x_gam_IETs = np.arange(gamma.ppf(0.0001, a=y_, loc=loc_, scale=mu_),\n",
    "                       gamma.ppf(0.9999, a=y_, loc=loc_, scale=mu_))\n",
    "gamma_pdf = gamma.pdf(x_gam_IETs, a=y_, loc=loc_, scale=mu_)\n",
    "# ax.plot(x_gam_IETs, gamma_pdf, label=f'$\\mu$ = {round(1/mu_,2)}', color='green')\n",
    "p_tail = [float('0.9999' + '9' * i) for i in range(4)]\n",
    "x_tail = [gamma.ppf(p_tail, a=y_, loc=loc_, scale=mu_) for x in p_tail]\n",
    "y_tail = [gamma.pdf(x, a=y_, loc=loc_, scale=mu_) for x in x_tail]\n",
    "ax.plot(np.append(x_gam_IETs, x_tail), np.append(gamma_pdf, y_tail), label=f'$\\mu$ = {round(1/mu_,2)}', color='green')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('PDF')\n",
    "ax.set_xlabel('IET (days)')\n",
    "# ax.set_ylim(10**-2, 10**0)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(f'../outputs/rough/good_vs_bad_gamma.png')\n",
    "\n",
    "old_loop_df = loop_df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution to negative seismicity rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=50000\n",
    "local_catalogue_radius = 10 # km \n",
    "search_style='radius'\n",
    "search_radius = 10 # km\n",
    "foreshock_window = 20 # days\n",
    "# modelling_time_period=365 # days\n",
    "modelling_time_period=360 # days\n",
    "significance_level = 0.01 # (1%)\n",
    "observation_time_scalar=4\n",
    "Wetzler_cutoff=3\n",
    "t_day = 3600 * 24.0 \n",
    "t_win = foreshock_window * t_day    \n",
    "count = 1\n",
    "detailed_sampling_dict ={}\n",
    "IDs_with_neg_seis_rates = []\n",
    "\n",
    "mainshock_ID = 15481673\n",
    "\n",
    "earthquake_catalogue = pd.read_csv(f\"../../catalogues/reformatted/QTM_9_5_reformat.csv\")\n",
    "utils.string_to_datetime_df(earthquake_catalogue)\n",
    "mainshock = earthquake_catalogue.loc[earthquake_catalogue['ID']==mainshock_ID].iloc[0]\n",
    "local_catalogue = pd.read_csv(f\"../data/QTM_9_5/local_catalogues/{mainshock_ID}.csv\")\n",
    "utils.string_to_datetime_df(local_catalogue)\n",
    "local_catalogue = local_catalogue.loc[(local_catalogue['DATETIME'] < mainshock.DATETIME) &\\\n",
    "                                      (local_catalogue['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] > 0)  &\\\n",
    "                                            (local_catalogue['ID'] != mainshock.ID)\n",
    "                                            ].copy()\n",
    "print(f\"local_catalogue: {len(local_catalogue)}\")\n",
    "\n",
    "foreshocks = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] < foreshock_window)].copy()\n",
    "n_events_in_foreshock_window = len(foreshocks)\n",
    "\n",
    "regular_seismicity_period = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] >= foreshock_window)].copy()\n",
    "print(f\"regular_seismicity_period: {len(regular_seismicity_period)}\")\n",
    "time_series = np.array(regular_seismicity_period['DATETIME'].apply(lambda d: (d-regular_seismicity_period['DATETIME'].iloc[0]).total_seconds()/3600/24))\n",
    "IET = np.diff(time_series) # Note: no need for /t_day as I already do it above (/3600/24)\n",
    "IET = IET[IET>0]\n",
    "print(f\"len IETs: {len(IET)}\")\n",
    "# IET = IET/t_day # a step I was missing... how has this been working without this?\n",
    "y_, loc_, mu_ = gamma.fit(IET, floc=0.0)\n",
    "\n",
    "# try:\n",
    "#     y_, loc_, mu_ = gamma.fit(IET, floc=0.0)\n",
    "# except:\n",
    "#     y_, loc_, mu_ = gamma.fit(IET, loc=0.0)\n",
    "print(f\"y_ {y_}, loc_ {loc_}, mu_ {mu_}\")\n",
    "\n",
    "catalogue_start_date = earthquake_catalogue['DATETIME'].iloc[0]\n",
    "time_since_catalogue_start = (mainshock.DATETIME - catalogue_start_date).total_seconds()/3600/24\n",
    "cut_off_day = math.floor(time_since_catalogue_start)\n",
    "if cut_off_day > 365:\n",
    "    cut_off_day = 365\n",
    "range_scaler = 100    \n",
    "\n",
    "event_counts_in_x_days = []\n",
    "# upper_time_limit = math.ceil(max(time_series)) - foreshock_window # wrong, went from 1st earthquake, not from 1 year prior to mainshock or start of catalogue (if less than 1 year) \n",
    "upper_time_limit = cut_off_day - foreshock_window \n",
    "\n",
    "print(\"     Creating ESR & G-IET models.\")\n",
    "N_eq = np.zeros(iterations, dtype=int) # Buffer for the number of earthquakes observed in each random sample\n",
    "loop_dict = []\n",
    "for i in range(0,iterations):\n",
    "    \n",
    "    random_point = dt.timedelta(days=random.random()*upper_time_limit)\n",
    "    \n",
    "    random_sample = regular_seismicity_period.loc[((mainshock.DATETIME - regular_seismicity_period['DATETIME']) < (random_point + dt.timedelta(days=foreshock_window))) &\\\n",
    "                            ((mainshock.DATETIME - regular_seismicity_period['DATETIME']) > random_point)]\n",
    "    \n",
    "    event_counts_in_x_days.append(len(random_sample))\n",
    "        \n",
    "    ## V&A IET method\n",
    "    prev_size = 200 # Generate a random IET sample with 200 events\n",
    "    IET2 = stats.gamma.rvs(a=y_, loc=0, scale=mu_, size=prev_size) * t_day # Sample from gamma distribution\n",
    "    random_number = np.random.rand()\n",
    "    t0 = random_number * IET2[0]\n",
    "    # t0 = np.random.rand() * IET2[0] # Random shift of timing of first event\n",
    "    t_sum = np.cumsum(IET2) - t0 # Cumulative sum of interevent times\n",
    "    inds = (t_sum > t_win) # Find the events that lie outside t_win\n",
    "    while (inds.sum() == 0):\n",
    "        prev_size *= 2 # If no events lie outside t_win, create a bigger sample and stack with previous sample\n",
    "        IET2 = np.hstack([IET2, stats.gamma.rvs(a=y_, loc=0, scale=mu_, size=prev_size) * t_day])\n",
    "        t_sum = np.cumsum(IET2) # Cumulative sum of event times\n",
    "        inds = (t_sum > t_win) # Find the events that lie outside t_win\n",
    "    # N_eq[i] = (~inds).sum() - 1 # Store the number of events that lie within t_win (excluding shifted event)\n",
    "    N_inside_t_win = (~inds).sum()\n",
    "    if N_inside_t_win == 0:\n",
    "        N_eq[i] = 0\n",
    "    else:\n",
    "        N_eq[i] =  N_inside_t_win - 1\n",
    "\n",
    "    loop_dict.append({\"random_number\":random_number,\n",
    "                      \"IET2_0\":IET2[0]/t_day,\n",
    "                      \"t0\":t0/t_day,\n",
    "                      \"n_outside_t_win\":(t_sum > t_win).sum(),\n",
    "                      \"seismicity_rate\":N_eq[i]#,\n",
    "                #  \"while_counter\":while_counter,\n",
    "                #  \"prev_size\":prev_size,\n",
    "                #  \"inds.sum()\":inds.sum()\n",
    "                    })\n",
    "\n",
    "loop_df = pd.DataFrame.from_dict(loop_dict)\n",
    "\n",
    "try:\n",
    "    y_gam_IETs, loc_gam_IETs, mu_gam_IETs = gamma.fit(N_eq[N_eq > 0], floc=0.0)\n",
    "except:\n",
    "    y_gam_IETs, loc_gam_IETs, mu_gam_IETs = gamma.fit(N_eq[N_eq > 0], loc=0.0)\n",
    "\n",
    "print(f\"y_gam_IETs {y_gam_IETs}, loc_gam_IETs {loc_gam_IETs}, mu_gam_IETs {mu_gam_IETs}\")\n",
    "VA_gamma_probability = gamma.sf(n_events_in_foreshock_window, y_gam_IETs, loc_gam_IETs, mu_gam_IETs)\n",
    "VA_gamma_99CI = gamma.ppf(0.99, a=y_gam_IETs, loc=loc_gam_IETs, scale=mu_gam_IETs)\n",
    "VA_IETs_probability = len(N_eq[N_eq>=n_events_in_foreshock_window])/iterations\n",
    "VA_IETs_99CI = np.percentile(N_eq,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loop_df['seismicity_rate'], bins=utils.get_bins(loop_df['seismicity_rate'], nearest=1), alpha=0.5, label='New')\n",
    "plt.hist(old_loop_df['seismicity_rate'], bins=utils.get_bins(old_loop_df['seismicity_rate'], nearest=1), alpha=0.5, label='Old')\n",
    "plt.legend()\n",
    "plt.savefig(f\"../outputs/rough/old_vs_new_Gamma.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old work showing that non-zero probabilities of negative seismicity rates are present in V&A's original work, not just when I use their code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.hist(N_eq, bins=range(min(N_eq)-1, max(N_eq)+1), density=True, rwidth=1.0, alpha=0.3, label='This Study', align='mid')\n",
    "ax1.hist(VA_N_eq, bins=np.arange(N_eq.min()-1, N_eq.max()+1,1), density=True, rwidth=1.0, alpha=0.3, label='V&A (2020)', align='mid')\n",
    "ax1.set_xlabel('Seismicity Rate (N per 20 days)')\n",
    "ax1.set_ylabel('PDF')\n",
    "ax1.set_title(f\"a)\", loc='left', fontsize=20)\n",
    "ax1.axhline(y=0.01, color='grey', alpha=0.5, label=f\"p=0.01\")\n",
    "ax1.axvline(x=np.percentile(N_eq,99), color='red', alpha=0.5, label=f\"99th percentile ({np.percentile(N_eq,99)})\")\n",
    "plt.legend()\n",
    "# ax1.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.hist(N_eq, bins=np.arange(N_eq.min(), N_eq.max()), density=True, rwidth=1.0, alpha=0.3, label='This Study')\n",
    "ax2.hist(VA_N_eq, bins=np.arange(N_eq.min(), N_eq.max()), density=True, rwidth=1.0, alpha=0.3, label='V&A (2020)')\n",
    "ax2.set_xlabel('N_eq')\n",
    "ax2.set_xlabel('Seismicity Rate (N per 20 days)')\n",
    "# ax2.set_ylabel(f\"{r'$log_{10}$'}(PDF)\")\n",
    "ax2.set_ylabel(f\"PDF (log scale)\")\n",
    "ax2.set_title(f\"b)\", loc='left', fontsize=20)\n",
    "ax2.axhline(y=0.01, color='grey', alpha=0.5, label=f\"p=0.01\")\n",
    "ax2.axvline(x=np.percentile(N_eq,99), color='red', alpha=0.5, label=f\"99th percentile ({np.percentile(N_eq,99)})\")\n",
    "ax2.set_yscale(\"log\")\n",
    "# ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(f\"../outputs/paper_1/this_study_vs_VA.png\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(N_eq[N_eq<0]), len(VA_N_eq[VA_N_eq<0])\n",
    "# plt.hist(np.sort(N_eq) - np.sort(VA_N_eq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_catalogue = pd.read_csv(f\"../catalogues/reformatted_catalogues/QTM_9_5_reformat.csv\")\n",
    "utils.string_to_datetime_df(earthquake_catalogue)\n",
    "mainshock = earthquake_catalogue.loc[earthquake_catalogue['ID']==mainshock_ID].iloc[0]\n",
    "local_catalogue = pd.read_csv(f\"../outputs/QTM_12/local_catalogues/{mainshock_ID}.csv\")\n",
    "utils.string_to_datetime_df(local_catalogue)\n",
    "local_catalogue = local_catalogue.loc[(local_catalogue['DATETIME'] <= mainshock.DATETIME) &\\\n",
    "                                      (local_catalogue['DAYS_TO_MAINSHOCK'] < modelling_time_period+foreshock_window) &\\\n",
    "                                        (local_catalogue['DAYS_TO_MAINSHOCK'] > 0)  &\\\n",
    "                                            (local_catalogue['ID'] != mainshock.ID)\n",
    "                                            ].copy()\n",
    "\n",
    "time_series = local_catalogue['DATETIME'].copy()\n",
    "regular_seismicity_period = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] >= foreshock_window)].copy()\n",
    "\n",
    "foreshocks = local_catalogue[(local_catalogue['DAYS_TO_MAINSHOCK'] < foreshock_window)].copy()\n",
    "n_events_in_foreshock_window = len(foreshocks)\n",
    "IET = np.diff(time_series) ### V&As Gamma IET method\n",
    "# IET = IET[IET>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax = sns.histplot(IET)\n",
    "ax.set_xlabel(f\"IET\")\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax = sns.histplot(np.log10(IET))\n",
    "ax.set_xlabel(f\"log10(IET)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r VA_local_catalogue\n",
    "VA_local_catalogue\n",
    "t_main_sec = 1.9632774e+08\n",
    "t_sec = VA_local_catalogue['t_sec'].values\n",
    "inds_prior = (t_sec < t_main_sec - 20 * t_day) & (t_sec > t_main_sec - 380 * t_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VA_local_catalogue = VA_local_catalogue.loc[(VA_local_catalogue['t_sec'] < t_main_sec - 20 * t_day) & (VA_local_catalogue['t_sec'] > t_main_sec - 380 * t_day)].copy()\n",
    "VA_local_catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_catalogue.loc[local_catalogue['ID'].isin(VA_local_catalogue.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VA_IET = (t_sec[inds_prior][1:] - t_sec[inds_prior][:-1]).astype(int)/t_day\n",
    "VA_IET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(regular_seismicity_period),\n",
    "      len(t_sec[inds_prior]),\n",
    "      len(IET),\n",
    "      len(VA_IET),\n",
    "      len(IET[IET>0]),\n",
    "      len(VA_IET[VA_IET>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IET[IET<=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VA_IET[VA_IET<=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE\n",
    "time_series = np.array(regular_seismicity_period['DATETIME'].apply(lambda d: (d-regular_seismicity_period['DATETIME'].iloc[0]).total_seconds()/3600/24))\n",
    "IET = np.diff(time_series) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure: Original mainshock catalogue overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'Moutote_53'\n",
    "mag_thresh = 4\n",
    "iterations = 10000\n",
    "Mc_cut = 'False'\n",
    "radius = 10\n",
    "window = 20\n",
    "period = 365\n",
    "name = 'QTM_9_5'\n",
    "\n",
    "Moutote_53_QTM_9_5 = pd.read_csv(f\"../outputs/{name}/{method}/foreshocks/Mw_{mag_thresh}_iter{iterations}_Mc_cut_{Mc_cut}_{radius}km_{window}day_{period}mtp.csv\")\n",
    "\n",
    "method = 'TR_46'\n",
    "mag_thresh = 4\n",
    "iterations = 10000\n",
    "Mc_cut = 'False'\n",
    "radius = 10\n",
    "window = 20\n",
    "period = 365\n",
    "name = 'QTM_9_5'\n",
    "\n",
    "TR_46_QTM_9_5 = pd.read_csv(f\"../outputs/{name}/{method}/foreshocks/Mw_{mag_thresh}_iter{iterations}_Mc_cut_{Mc_cut}_{radius}km_{window}day_{period}mtp.csv\")\n",
    "TR_46_QTM_9_5.sort_values(by='ESR_2')\n",
    "\n",
    "TR_only = TR_46_QTM_9_5.loc[~TR_46_QTM_9_5['ID'].isin(Moutote_53_QTM_9_5['ID'])].copy()\n",
    "Moutote_only = Moutote_53_QTM_9_5.loc[~Moutote_53_QTM_9_5['ID'].isin(TR_46_QTM_9_5['ID'])].copy()\n",
    "\n",
    "TR_both = TR_46_QTM_9_5.loc[TR_46_QTM_9_5['ID'].isin(Moutote_53_QTM_9_5['ID'])].copy()\n",
    "Moutote_both = Moutote_53_QTM_9_5.loc[Moutote_53_QTM_9_5['ID'].isin(TR_46_QTM_9_5['ID'])].copy()\n",
    "\n",
    "permutations = {'TR_all':TR_46_QTM_9_5,\n",
    "                'Moutote_all':Moutote_53_QTM_9_5,\n",
    "                'Both':TR_both,\n",
    "                # 'Moutote_both':Moutote_both,\n",
    "                'TR_only':TR_only,\n",
    "                'Moutote_only':Moutote_only\n",
    "                }\n",
    "\n",
    "perm_results = []\n",
    "for perm, df in permutations.items():\n",
    "    N = len(df)\n",
    "    N_w_foreshocks = len(df.loc[df['ESR_2']<0.01])\n",
    "    perm_results.append({'Mainshock_category':perm,\n",
    "                         'Count':N,\n",
    "                         'N_w_foreshocks':N_w_foreshocks,\n",
    "                         'percent':100*N_w_foreshocks/N})\n",
    "    \n",
    "perm_results = pd.DataFrame.from_dict(perm_results)\n",
    "perm_results\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=perm_results['Mainshock_category'], y=perm_results['percent'], palette=sns.)\n",
    "rects = ax.patches\n",
    "\n",
    "labels = []\n",
    "for row in perm_results.itertuples():\n",
    "    labels.append(f\"{row.N_w_foreshocks}/{row.Count}\")\n",
    "labels\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width() / 2, height, label, ha=\"center\", va=\"bottom\", fontsize=20\n",
    "    )\n",
    "\n",
    "plt.ylabel('% w/foreshocks')\n",
    "plt.savefig('../outputs/paper_1/paper_figures/TR_Moutote_original_overlap.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foreshocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
